---
title: Vertex AI + SHAP integration
description: Right to Explanation for high-risk clinical AI inferences
---

## Overview

iLuminara-Core integrates Vertex AI AutoML with SHAP (SHapley Additive exPlanations) to provide transparent, auditable AI decisions that comply with the EU AI Act §6 and GDPR Art. 22.

<Card
  title="Philosophy"
  icon="brain-circuit"
>
  "Every high-risk clinical inference requires explainability."
</Card>

## Compliance requirements

<CardGroup cols={2}>
  <Card title="EU AI Act §6" icon="scale-balanced">
    High-risk AI systems must provide explanations
  </Card>
  <Card title="GDPR Art. 22" icon="user-shield">
    Right to explanation for automated decisions
  </Card>
  <Card title="HIPAA §164.312(b)" icon="file-contract">
    Audit controls for clinical decisions
  </Card>
  <Card title="ISO 27001 A.18.1.4" icon="shield-check">
    Privacy and protection of PII
  </Card>
</CardGroup>

## Risk levels

| Level | Confidence Range | Explanation Required | Frameworks |
|-------|------------------|---------------------|------------|
| **LOW** | < 0.5 | No | - |
| **MEDIUM** | 0.5 - 0.7 | No | - |
| **HIGH** | 0.7 - 0.9 | Yes | EU AI Act §6, GDPR Art. 22 |
| **CRITICAL** | > 0.9 | Yes | EU AI Act §6, GDPR Art. 22, HIPAA §164.312(b), ISO 27001 |

## Basic usage

```python
from cloud_oracle.vertex_ai_shap import VertexAIExplainer

# Initialize explainer
explainer = VertexAIExplainer(
    project_id="iluminara-core",
    location="us-central1",
    high_risk_threshold=0.7,
    enable_audit=True
)

# Patient symptoms
features = {
    "fever": 0.9,
    "cough": 0.3,
    "diarrhea": 0.8,
    "vomiting": 0.7,
    "fatigue": 0.6,
    "headache": 0.4,
    "body_aches": 0.5
}

# Context for audit trail
context = {
    "patient_id": "PAT_12345",
    "location": "Dadaab",
    "jurisdiction": "KDPA_KE",
    "data_type": "PHI"
}

# Get prediction with explanation
result = explainer.predict_with_explanation(features, context)
```

## Response structure

```json
{
  "prediction": 0.85,
  "confidence": 0.92,
  "risk_level": "critical",
  "explanation": {
    "shap_values": [0.27, 0.09, 0.32, 0.21, 0.06, 0.12, 0.08],
    "feature_importance": {
      "diarrhea": 0.32,
      "fever": 0.27,
      "vomiting": 0.21,
      "headache": 0.12,
      "cough": 0.09,
      "body_aches": 0.08,
      "fatigue": 0.06
    },
    "decision_rationale": "High-risk prediction based on: severe diarrhea (contribution: 32.0%), severe fever (contribution: 27.0%), moderate vomiting (contribution: 21.0%). Clinical review recommended.",
    "evidence_chain": [
      "diarrhea: 0.80 (importance: 0.320)",
      "fever: 0.90 (importance: 0.270)",
      "vomiting: 0.70 (importance: 0.210)"
    ],
    "top_features": ["diarrhea", "fever", "vomiting"]
  },
  "compliance": {
    "requires_explanation": true,
    "frameworks": [
      "EU AI Act §6 (High-Risk AI Systems)",
      "GDPR Art. 22 (Right to Explanation)",
      "HIPAA §164.312(b) (Audit Controls)",
      "ISO 27001 A.18.1.4 (Privacy and Protection of PII)"
    ],
    "high_risk_threshold": 0.7
  },
  "timestamp": "2025-12-23T19:00:00.000Z",
  "context": {
    "patient_id": "PAT_12345",
    "location": "Dadaab",
    "jurisdiction": "KDPA_KE",
    "data_type": "PHI"
  }
}
```

## SHAP explanation

SHAP (SHapley Additive exPlanations) provides:

<Steps>
  <Step title="Feature importance">
    Quantifies each feature's contribution to the prediction
  </Step>
  <Step title="SHAP values">
    Individual feature impact on the model output
  </Step>
  <Step title="Decision rationale">
    Human-readable explanation of the decision
  </Step>
  <Step title="Evidence chain">
    Ordered list of contributing factors
  </Step>
</Steps>

### SHAP values interpretation

```python
# Positive SHAP value = increases prediction
# Negative SHAP value = decreases prediction
# Magnitude = strength of contribution

shap_values = result['explanation']['shap_values']
features = list(features.keys())

for feature, shap_value in zip(features, shap_values):
    direction = "increases" if shap_value > 0 else "decreases"
    print(f"{feature}: {shap_value:.3f} ({direction} risk)")
```

## Batch predictions

Process multiple patients efficiently:

```python
# Multiple patients
features_list = [
    {"fever": 0.9, "cough": 0.3, "diarrhea": 0.8},
    {"fever": 0.4, "cough": 0.7, "diarrhea": 0.2},
    {"fever": 0.8, "cough": 0.6, "diarrhea": 0.9}
]

contexts = [
    {"patient_id": "PAT_001", "location": "Dadaab"},
    {"patient_id": "PAT_002", "location": "Nairobi"},
    {"patient_id": "PAT_003", "location": "Garissa"}
]

# Batch prediction
results = explainer.batch_predict_with_explanation(
    features_list=features_list,
    contexts=contexts
)

# Filter high-risk cases
high_risk = [r for r in results if r['risk_level'] in ['high', 'critical']]
print(f"High-risk cases: {len(high_risk)}/{len(results)}")
```

## Integration with SovereignGuardrail

All high-risk inferences are validated against sovereignty constraints:

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# Validate high-risk inference
if result['compliance']['requires_explanation']:
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'actor': 'vertex_ai_model',
            'resource': 'patient_diagnosis',
            'explanation': result['explanation']['decision_rationale'],
            'confidence_score': result['confidence'],
            'evidence_chain': result['explanation']['evidence_chain'],
            'consent_token': context.get('consent_token'),
            'consent_scope': 'diagnosis'
        },
        jurisdiction='KDPA_KE'
    )
```

## Audit trail

All high-risk inferences are automatically logged to BigQuery:

```sql
-- Query audit trail
SELECT
  timestamp,
  model_name,
  prediction,
  confidence,
  risk_level,
  JSON_EXTRACT(explanation, '$.decision_rationale') as rationale,
  JSON_EXTRACT(context, '$.patient_id') as patient_id,
  JSON_EXTRACT(context, '$.location') as location
FROM `iluminara_audit.ai_explanations`
WHERE risk_level IN ('high', 'critical')
  AND timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
ORDER BY timestamp DESC
LIMIT 100;
```

## Vertex AI deployment

### Deploy model to Vertex AI

```bash
# Upload model
gcloud ai models upload \
  --region=us-central1 \
  --display-name=outbreak-forecaster \
  --container-image-uri=gcr.io/iluminara-core/forecaster:latest

# Create endpoint
gcloud ai endpoints create \
  --region=us-central1 \
  --display-name=outbreak-forecaster-endpoint

# Deploy model to endpoint
gcloud ai endpoints deploy-model ENDPOINT_ID \
  --region=us-central1 \
  --model=MODEL_ID \
  --display-name=outbreak-forecaster-v1 \
  --machine-type=n1-standard-4 \
  --min-replica-count=1 \
  --max-replica-count=10
```

### Update explainer to use endpoint

```python
from google.cloud import aiplatform

class VertexAIExplainer:
    def __init__(self, endpoint_name: str, **kwargs):
        self.endpoint = aiplatform.Endpoint(endpoint_name=endpoint_name)
        # ... rest of initialization
    
    def _get_vertex_prediction(self, feature_df):
        # Call actual Vertex AI endpoint
        instances = feature_df.to_dict('records')
        predictions = self.endpoint.predict(instances=instances)
        
        prediction = predictions.predictions[0]['value']
        confidence = predictions.predictions[0]['confidence']
        
        return prediction, confidence
```

## SHAP explainer types

Choose the appropriate SHAP explainer based on your model:

<AccordionGroup>
  <Accordion title="TreeExplainer">
    For tree-based models (XGBoost, LightGBM, Random Forest)
    ```python
    explainer = shap.TreeExplainer(model)
    ```
  </Accordion>
  <Accordion title="DeepExplainer">
    For deep learning models (TensorFlow, PyTorch)
    ```python
    explainer = shap.DeepExplainer(model, background_data)
    ```
  </Accordion>
  <Accordion title="LinearExplainer">
    For linear models (Logistic Regression, Linear SVM)
    ```python
    explainer = shap.LinearExplainer(model, background_data)
    ```
  </Accordion>
  <Accordion title="KernelExplainer">
    Model-agnostic (works with any model, slower)
    ```python
    explainer = shap.KernelExplainer(model.predict, background_data)
    ```
  </Accordion>
</AccordionGroup>

## Visualization

Generate SHAP visualizations for clinical review:

```python
import shap
import matplotlib.pyplot as plt

# Force plot (single prediction)
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    feature_df.iloc[0],
    matplotlib=True
)
plt.savefig('shap_force_plot.png')

# Summary plot (multiple predictions)
shap.summary_plot(
    shap_values,
    feature_df,
    plot_type="bar"
)
plt.savefig('shap_summary.png')

# Waterfall plot (single prediction)
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values[0],
        base_values=explainer.expected_value,
        data=feature_df.iloc[0]
    )
)
plt.savefig('shap_waterfall.png')
```

## Performance considerations

- **Batch predictions**: Use `batch_predict_with_explanation()` for efficiency
- **SHAP caching**: Explainer is initialized once and reused
- **Async processing**: For real-time systems, compute SHAP asynchronously
- **Sampling**: For large datasets, use background data sampling

## Next steps

<CardGroup cols={2}>
  <Card
    title="Deploy to Vertex AI"
    icon="google"
    href="/deployment/gcp"
  >
    Deploy models to Google Cloud Platform
  </Card>
  <Card
    title="SovereignGuardrail"
    icon="shield-check"
    href="/governance/overview"
  >
    Integrate with compliance enforcement
  </Card>
  <Card
    title="Audit trail"
    icon="file-contract"
    href="/governance/audit"
  >
    Configure tamper-proof logging
  </Card>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Use explainable AI in autonomous agents
  </Card>
</CardGroup>
