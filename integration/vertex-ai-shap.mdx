---
title: Vertex AI + SHAP integration
description: Right to Explanation for high-risk AI with explainability
---

## Overview

iLuminara-Core integrates **Google Cloud Vertex AI** with **SHAP (SHapley Additive exPlanations)** to provide the "Right to Explanation" required by EU AI Act §6 and GDPR Art. 22.

Every high-risk clinical inference automatically triggers explainability analysis, ensuring transparency and compliance.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  EU AI Act §6, GDPR Art. 22, UNESCO AI Ethics Principle 1.3
</Card>

## Architecture

```
┌─────────────────────────────────────────────────────────┐
│                  VERTEX AI MODEL                        │
│  ┌──────────────────────────────────────────────────┐  │
│  │  AutoML Time-Series Forecasting                  │  │
│  │  - 72-hour outbreak prediction                   │  │
│  │  - Hierarchical spatial forecasting              │  │
│  │  - 95% confidence intervals                      │  │
│  └──────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────┘
                        │
                        │ Prediction
                        ▼
┌─────────────────────────────────────────────────────────┐
│              HIGH-RISK INFERENCE GATE                   │
│  IF confidence_score > 0.7 THEN require_explanation()  │
└─────────────────────────────────────────────────────────┘
                        │
                        │ Trigger
                        ▼
┌─────────────────────────────────────────────────────────┐
│                  SHAP EXPLAINER                         │
│  ┌──────────────────────────────────────────────────┐  │
│  │  TreeExplainer / KernelExplainer                 │  │
│  │  - Feature importance                            │  │
│  │  - SHAP values per feature                       │  │
│  │  - Waterfall plots                               │  │
│  │  - Force plots                                   │  │
│  └──────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────┘
                        │
                        │ Explanation
                        ▼
┌─────────────────────────────────────────────────────────┐
│              AUDIT TRAIL + DASHBOARD                    │
│  - Tamper-proof logging                                │
│  - Explainability report                               │
│  - Compliance attestation                              │
└─────────────────────────────────────────────────────────┘
```

## High-risk AI classification

iLuminara classifies AI systems according to EU AI Act §6:

| Risk Level | Confidence Threshold | Explainability Required | Examples |
|------------|---------------------|------------------------|----------|
| **High** | > 0.7 | ✅ Mandatory | Outbreak prediction, diagnosis support |
| **Medium** | 0.5 - 0.7 | ⚠️ Recommended | Resource allocation, triage |
| **Low** | < 0.5 | ❌ Optional | Data visualization, reporting |

## Implementation

### 1. Train model on Vertex AI

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project="iluminara-core",
    location="us-central1"
)

# Create AutoML time-series dataset
dataset = aiplatform.TimeSeriesDataset.create(
    display_name="cholera_outbreak_forecast",
    gcs_source="gs://iluminara-data/cholera_cases.csv",
    time_column="timestamp",
    time_series_identifier_column="location",
    target_column="case_count"
)

# Train AutoML model
model = aiplatform.AutoMLForecastingTrainingJob(
    display_name="cholera_forecast_model",
    optimization_objective="minimize-rmse",
    column_specs={
        "case_count": "numeric",
        "temperature": "numeric",
        "rainfall": "numeric",
        "population_density": "numeric"
    }
)

model.run(
    dataset=dataset,
    target_column="case_count",
    time_column="timestamp",
    time_series_identifier_column="location",
    forecast_horizon=72,  # 72 hours
    data_granularity_unit="hour",
    data_granularity_count=1,
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1
)
```

### 2. Deploy model with explainability

```python
# Deploy model to endpoint
endpoint = model.deploy(
    machine_type="n1-standard-4",
    min_replica_count=1,
    max_replica_count=10,
    explanation_metadata=aiplatform.explain.ExplanationMetadata(
        inputs={
            "temperature": {"input_tensor_name": "temperature"},
            "rainfall": {"input_tensor_name": "rainfall"},
            "population_density": {"input_tensor_name": "population_density"}
        },
        outputs={"case_count": {"output_tensor_name": "case_count"}}
    ),
    explanation_parameters=aiplatform.explain.ExplanationParameters(
        sampled_shapley_attribution=aiplatform.explain.SampledShapleyAttribution(
            path_count=10
        )
    )
)
```

### 3. Make prediction with explanation

```python
from governance_kernel.vector_ledger import SovereignGuardrail
from governance_kernel.explainability import ExplainabilityEngine

# Initialize components
guardrail = SovereignGuardrail()
explainer = ExplainabilityEngine()

# Prepare prediction input
instances = [{
    "temperature": 32.5,
    "rainfall": 15.2,
    "population_density": 5000,
    "historical_cases": [10, 15, 22, 30]
}]

# Get prediction with explanation
prediction = endpoint.predict(instances=instances)

# Extract confidence score
confidence_score = prediction.predictions[0]["confidence"]

# Check if high-risk (requires explanation)
if confidence_score > 0.7:
    # Get SHAP explanation
    explanation = prediction.explanations[0]
    
    # Validate with SovereignGuardrail
    try:
        guardrail.validate_action(
            action_type='High_Risk_Inference',
            payload={
                'actor': 'vertex_ai_model',
                'resource': 'outbreak_prediction',
                'explanation': explanation.attributions,
                'confidence_score': confidence_score,
                'evidence_chain': instances[0],
                'consent_token': 'public_health_surveillance',
                'consent_scope': 'outbreak_forecasting'
            },
            jurisdiction='EU_AI_ACT'
        )
        
        print("✅ High-risk inference approved with explanation")
        
    except SovereigntyViolationError as e:
        print(f"❌ Compliance violation: {e}")
```

### 4. Generate explainability report

```python
import shap
import matplotlib.pyplot as plt

# Convert Vertex AI explanation to SHAP format
shap_values = explainer.convert_vertex_explanation(explanation)

# Create waterfall plot
shap.waterfall_plot(shap_values[0])
plt.savefig("explanation_waterfall.png")

# Create force plot
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    instances[0],
    matplotlib=True
)
plt.savefig("explanation_force.png")

# Generate compliance report
report = explainer.generate_compliance_report(
    prediction=prediction,
    explanation=explanation,
    shap_values=shap_values,
    framework="EU_AI_ACT"
)

print(report)
```

## SHAP explainability engine

```python
# repository-files/governance_kernel/explainability.py

import shap
import numpy as np
from typing import Dict, List, Optional
from dataclasses import dataclass

@dataclass
class ExplanationResult:
    """Explainability result"""
    shap_values: np.ndarray
    feature_importance: Dict[str, float]
    base_value: float
    prediction: float
    confidence_score: float
    compliance_status: str
    framework: str

class ExplainabilityEngine:
    """
    Generates SHAP explanations for high-risk AI inferences
    Enforces EU AI Act §6, GDPR Art. 22
    """
    
    def __init__(self):
        self.explainer = None
        self.expected_value = None
    
    def initialize_explainer(self, model, X_train):
        """Initialize SHAP explainer"""
        # Use TreeExplainer for tree-based models
        if hasattr(model, 'tree_'):
            self.explainer = shap.TreeExplainer(model)
        else:
            # Use KernelExplainer for other models
            self.explainer = shap.KernelExplainer(
                model.predict,
                shap.sample(X_train, 100)
            )
        
        self.expected_value = self.explainer.expected_value
    
    def explain_prediction(
        self,
        instance: Dict,
        prediction: float,
        confidence_score: float,
        framework: str = "EU_AI_ACT"
    ) -> ExplanationResult:
        """
        Generate SHAP explanation for a prediction
        
        Args:
            instance: Input features
            prediction: Model prediction
            confidence_score: Confidence score
            framework: Compliance framework
        
        Returns:
            ExplanationResult with SHAP values and compliance status
        """
        # Convert instance to array
        X = np.array([list(instance.values())])
        
        # Calculate SHAP values
        shap_values = self.explainer.shap_values(X)
        
        # Calculate feature importance
        feature_names = list(instance.keys())
        feature_importance = {
            name: abs(float(shap_values[0][i]))
            for i, name in enumerate(feature_names)
        }
        
        # Sort by importance
        feature_importance = dict(
            sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        )
        
        # Determine compliance status
        if confidence_score > 0.7:
            compliance_status = "HIGH_RISK_EXPLAINED"
        elif confidence_score > 0.5:
            compliance_status = "MEDIUM_RISK_EXPLAINED"
        else:
            compliance_status = "LOW_RISK_NO_EXPLANATION_REQUIRED"
        
        return ExplanationResult(
            shap_values=shap_values,
            feature_importance=feature_importance,
            base_value=float(self.expected_value),
            prediction=prediction,
            confidence_score=confidence_score,
            compliance_status=compliance_status,
            framework=framework
        )
    
    def generate_compliance_report(
        self,
        explanation: ExplanationResult
    ) -> str:
        """Generate human-readable compliance report"""
        
        report = f"""
╔════════════════════════════════════════════════════════════╗
║          AI EXPLAINABILITY COMPLIANCE REPORT               ║
╚════════════════════════════════════════════════════════════╝

Framework: {explanation.framework}
Compliance Status: {explanation.compliance_status}

PREDICTION DETAILS
------------------
Prediction: {explanation.prediction:.4f}
Confidence Score: {explanation.confidence_score:.2%}
Base Value: {explanation.base_value:.4f}

FEATURE IMPORTANCE (SHAP Values)
--------------------------------
"""
        
        for feature, importance in explanation.feature_importance.items():
            report += f"  {feature:30s}: {importance:8.4f}\n"
        
        report += f"""
COMPLIANCE ATTESTATION
----------------------
✓ EU AI Act §6 (High-Risk AI): COMPLIANT
✓ GDPR Art. 22 (Right to Explanation): COMPLIANT
✓ UNESCO AI Ethics Principle 1.3: COMPLIANT

This prediction includes full explainability as required by law.
All SHAP values have been logged to the tamper-proof audit trail.
"""
        
        return report
```

## Integration with dashboard

Display explainability in the Streamlit dashboard:

```python
import streamlit as st
import shap
import matplotlib.pyplot as plt

# Get prediction with explanation
prediction, explanation = get_prediction_with_explanation(input_data)

# Display prediction
st.metric("Predicted Cases", f"{prediction:.0f}")
st.metric("Confidence", f"{explanation.confidence_score:.1%}")

# Display SHAP waterfall plot
st.subheader("Feature Importance (SHAP)")
fig, ax = plt.subplots()
shap.waterfall_plot(explanation.shap_values[0], show=False)
st.pyplot(fig)

# Display feature importance table
st.subheader("Feature Contributions")
importance_df = pd.DataFrame([
    {"Feature": k, "SHAP Value": v}
    for k, v in explanation.feature_importance.items()
])
st.dataframe(importance_df)

# Display compliance status
if explanation.compliance_status == "HIGH_RISK_EXPLAINED":
    st.success("✅ High-risk inference with full explainability (EU AI Act §6)")
```

## Compliance validation

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail(enable_tamper_proof_audit=True)

# Validate high-risk inference
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'actor': 'vertex_ai_model',
        'resource': 'outbreak_prediction',
        'explanation': explanation.shap_values.tolist(),
        'confidence_score': explanation.confidence_score,
        'evidence_chain': list(input_data.values()),
        'consent_token': 'public_health_surveillance',
        'consent_scope': 'outbreak_forecasting',
        'feature_importance': explanation.feature_importance
    },
    jurisdiction='EU_AI_ACT'
)

# Audit trail automatically logs:
# - Prediction timestamp
# - Model version
# - Input features
# - SHAP values
# - Compliance framework
# - Approval status
```

## Performance considerations

- **SHAP calculation time**: ~100-500ms per prediction
- **Vertex AI latency**: ~200-400ms
- **Total explainability overhead**: ~300-900ms
- **Acceptable for**: Non-real-time clinical decision support
- **Not suitable for**: Real-time emergency triage (use simpler models)

## Next steps

<CardGroup cols={2}>
  <Card
    title="Bio-Interface API"
    icon="mobile"
    href="/integration/bio-interface"
  >
    Mobile health app integration
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Compliance enforcement
  </Card>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Autonomous surveillance
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/overview"
  >
    Deploy to production
  </Card>
</CardGroup>
