---
title: Vertex AI + SHAP integration
description: Right to Explanation with SHAP explainability for high-risk AI inferences
---

## Overview

iLuminara-Core integrates Vertex AI with SHAP (SHapley Additive exPlanations) to provide "Right to Explanation" for every high-risk clinical inference, ensuring compliance with EU AI Act §6 and GDPR Art. 22.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  EU AI Act §6 (High-Risk AI), GDPR Art. 22 (Right to Explanation)
</Card>

## Architecture

```
┌─────────────────────────────────────┐
│         VERTEX AI MODEL             │
│  - AutoML Time-series               │
│  - Custom trained models            │
│  - 72-hour outbreak forecasting     │
└─────────────────────────────────────┘
              │
              │ Prediction
              ▼
┌─────────────────────────────────────┐
│      SHAP EXPLAINABILITY            │
│  - Feature importance               │
│  - SHAP values                      │
│  - Evidence chain                   │
└─────────────────────────────────────┘
              │
              │ Validation
              ▼
┌─────────────────────────────────────┐
│      SOVEREIGNGUARDRAIL             │
│  - High-risk threshold check        │
│  - Explainability validation        │
│  - Audit logging                    │
└─────────────────────────────────────┘
```

## High-risk inference detection

The SovereignGuardrail automatically detects high-risk inferences based on confidence score:

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# High-risk threshold: 0.7 (70% confidence)
HIGH_RISK_THRESHOLD = 0.7

if prediction_confidence >= HIGH_RISK_THRESHOLD:
    # Requires SHAP explanation
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'cholera_outbreak',
            'confidence_score': prediction_confidence,
            'explanation': shap_values,
            'evidence_chain': feature_contributions
        },
        jurisdiction='EU_AI_ACT'
    )
```

## SHAP integration

### Installation

```bash
pip install shap google-cloud-aiplatform
```

### Basic usage

```python
import shap
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(project='your-project-id', location='us-central1')

# Load model
model = aiplatform.Model('projects/123/locations/us-central1/models/456')

# Get prediction
prediction = model.predict(instances=[input_data])

# Generate SHAP explanation
explainer = shap.Explainer(model.predict, training_data)
shap_values = explainer(input_data)

# Visualize
shap.plots.waterfall(shap_values[0])
```

## Complete integration example

```python
from google.cloud import aiplatform
import shap
import numpy as np
from governance_kernel.vector_ledger import SovereignGuardrail, SovereigntyViolationError

class ExplainableOutbreakPredictor:
    """
    Vertex AI outbreak predictor with mandatory SHAP explainability.
    """
    
    def __init__(
        self,
        project_id: str,
        model_id: str,
        location: str = 'us-central1',
        high_risk_threshold: float = 0.7
    ):
        # Initialize Vertex AI
        aiplatform.init(project=project_id, location=location)
        
        # Load model
        self.model = aiplatform.Model(
            f'projects/{project_id}/locations/{location}/models/{model_id}'
        )
        
        # Initialize guardrail
        self.guardrail = SovereignGuardrail()
        self.high_risk_threshold = high_risk_threshold
        
        # SHAP explainer (initialized with training data)
        self.explainer = None
    
    def initialize_explainer(self, training_data: np.ndarray):
        """Initialize SHAP explainer with training data"""
        self.explainer = shap.Explainer(
            self.model.predict,
            training_data
        )
    
    def predict_with_explanation(
        self,
        input_data: dict,
        feature_names: list
    ) -> dict:
        """
        Make prediction with mandatory SHAP explanation for high-risk inferences.
        
        Args:
            input_data: Input features
            feature_names: Names of features
        
        Returns:
            Prediction with explanation and compliance validation
        """
        # Get prediction
        prediction = self.model.predict(instances=[input_data])
        confidence_score = prediction.predictions[0]['confidence']
        predicted_class = prediction.predictions[0]['class']
        
        # Check if high-risk
        is_high_risk = confidence_score >= self.high_risk_threshold
        
        result = {
            'prediction': predicted_class,
            'confidence_score': confidence_score,
            'is_high_risk': is_high_risk,
            'timestamp': prediction.predictions[0]['timestamp']
        }
        
        # Generate SHAP explanation for high-risk inferences
        if is_high_risk:
            if self.explainer is None:
                raise ValueError("SHAP explainer not initialized. Call initialize_explainer() first.")
            
            # Calculate SHAP values
            shap_values = self.explainer(np.array([list(input_data.values())]))
            
            # Extract feature contributions
            feature_contributions = {
                feature_names[i]: float(shap_values.values[0][i])
                for i in range(len(feature_names))
            }
            
            # Sort by absolute contribution
            sorted_features = sorted(
                feature_contributions.items(),
                key=lambda x: abs(x[1]),
                reverse=True
            )
            
            # Build evidence chain
            evidence_chain = [
                f"{feature}: {contribution:.4f}"
                for feature, contribution in sorted_features[:5]
            ]
            
            result['explanation'] = {
                'shap_values': shap_values.values[0].tolist(),
                'feature_contributions': feature_contributions,
                'evidence_chain': evidence_chain,
                'base_value': float(shap_values.base_values[0])
            }
            
            # Validate with SovereignGuardrail
            try:
                self.guardrail.validate_action(
                    action_type='High_Risk_Inference',
                    payload={
                        'inference': predicted_class,
                        'confidence_score': confidence_score,
                        'explanation': result['explanation'],
                        'evidence_chain': evidence_chain,
                        'consent_token': 'VALID_TOKEN',  # From user consent
                        'consent_scope': 'diagnosis'
                    },
                    jurisdiction='EU_AI_ACT'
                )
                result['compliance_status'] = 'APPROVED'
            
            except SovereigntyViolationError as e:
                result['compliance_status'] = 'REJECTED'
                result['compliance_error'] = str(e)
        
        else:
            result['explanation'] = None
            result['compliance_status'] = 'NOT_REQUIRED'
        
        return result


# Example usage
predictor = ExplainableOutbreakPredictor(
    project_id='iluminara-health',
    model_id='cholera-forecaster-v1',
    high_risk_threshold=0.7
)

# Initialize explainer with training data
predictor.initialize_explainer(training_data)

# Make prediction
input_features = {
    'case_count': 45,
    'attack_rate': 0.04,
    'r_effective': 2.8,
    'population_density': 15000,
    'water_quality_index': 0.3
}

feature_names = list(input_features.keys())

result = predictor.predict_with_explanation(
    input_data=input_features,
    feature_names=feature_names
)

print(f"Prediction: {result['prediction']}")
print(f"Confidence: {result['confidence_score']:.2%}")
print(f"High-risk: {result['is_high_risk']}")
print(f"Compliance: {result['compliance_status']}")

if result['explanation']:
    print("\nTop contributing features:")
    for evidence in result['explanation']['evidence_chain']:
        print(f"  - {evidence}")
```

## Vertex AI AutoML integration

```python
from google.cloud import aiplatform

# Create AutoML time-series forecasting model
dataset = aiplatform.TimeSeriesDataset.create(
    display_name='cholera-outbreak-data',
    gcs_source='gs://iluminara/data/cholera_cases.csv',
    time_column='date',
    time_series_identifier_column='location',
    target_column='case_count'
)

# Train model
job = aiplatform.AutoMLForecastingTrainingJob(
    display_name='cholera-forecaster',
    optimization_objective='minimize-rmse',
    column_transformations=[
        {'numeric': {'column_name': 'case_count'}},
        {'numeric': {'column_name': 'attack_rate'}},
        {'numeric': {'column_name': 'r_effective'}},
    ]
)

model = job.run(
    dataset=dataset,
    target_column='case_count',
    time_column='date',
    time_series_identifier_column='location',
    forecast_horizon=72,  # 72 hours
    data_granularity_unit='hour',
    data_granularity_count=1,
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
)

# Deploy model
endpoint = model.deploy(
    machine_type='n1-standard-4',
    min_replica_count=1,
    max_replica_count=10,
    accelerator_type='NVIDIA_TESLA_T4',
    accelerator_count=1
)
```

## Explainability requirements

Per SovereignGuardrail configuration, high-risk inferences must include:

<Steps>
  <Step title="Confidence score">
    Numerical confidence value (0.0 to 1.0)
  </Step>
  <Step title="Evidence chain">
    Top 5 contributing features with SHAP values
  </Step>
  <Step title="Feature contributions">
    Complete feature importance breakdown
  </Step>
  <Step title="Decision rationale">
    Human-readable explanation of the prediction
  </Step>
</Steps>

## Visualization

### SHAP waterfall plot

```python
import shap
import matplotlib.pyplot as plt

# Generate SHAP values
shap_values = explainer(input_data)

# Waterfall plot (shows feature contributions)
shap.plots.waterfall(shap_values[0])
plt.savefig('shap_waterfall.png')
```

### SHAP summary plot

```python
# Summary plot (shows feature importance across all predictions)
shap.summary_plot(shap_values, input_data, feature_names=feature_names)
plt.savefig('shap_summary.png')
```

### SHAP force plot

```python
# Force plot (interactive visualization)
shap.force_plot(
    shap_values.base_values[0],
    shap_values.values[0],
    input_data.iloc[0],
    feature_names=feature_names
)
```

## Compliance validation

The SovereignGuardrail validates explainability requirements:

```python
# Validation rules from config/sovereign_guardrail.yaml
explainability:
  enabled: true
  high_risk_threshold: 0.7
  methods:
    - "SHAP"
    - "LIME"
    - "Feature_Importance"
  requirements:
    - "confidence_score"
    - "evidence_chain"
    - "feature_contributions"
    - "decision_rationale"
```

## Performance considerations

- **SHAP computation time:** ~100-500ms per prediction
- **Model inference time:** ~50-200ms per prediction
- **Total latency:** ~150-700ms for high-risk inferences
- **Caching:** SHAP explainer can be cached for repeated predictions

## Next steps

<CardGroup cols={2}>
  <Card
    title="Bio-Interface API"
    icon="mobile"
    href="/integration/bio-interface"
  >
    Mobile health app integration
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Understand compliance enforcement
  </Card>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy autonomous surveillance
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/overview"
  >
    Deploy to production
  </Card>
</CardGroup>
