---
title: Vertex AI + SHAP integration
description: Right to Explanation for high-risk clinical AI with explainability
---

## Overview

iLuminara-Core integrates Google Cloud Vertex AI with SHAP (SHapley Additive exPlanations) to provide the "Right to Explanation" required by EU AI Act §6 and GDPR Art. 22.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk clinical inference requires explainability (SHAP values, feature importance, evidence chain).
</Card>

## Architecture

```
┌─────────────────────────────────────────────────────────┐
│                  VERTEX AI MODEL                         │
│  - AutoML Time-Series Forecasting                       │
│  - Custom Training (TensorFlow, PyTorch)                │
│  - Batch/Online Prediction                              │
└────────────────┬────────────────────────────────────────┘
                 │
                 │ Prediction
                 ▼
┌─────────────────────────────────────────────────────────┐
│              SHAP EXPLAINER                              │
│  - TreeExplainer (for tree-based models)                │
│  - DeepExplainer (for neural networks)                  │
│  - KernelExplainer (model-agnostic)                     │
└────────────────┬────────────────────────────────────────┘
                 │
                 │ SHAP values + Feature importance
                 ▼
┌─────────────────────────────────────────────────────────┐
│           SOVEREIGNGUARDRAIL                             │
│  - Validates explainability requirements                │
│  - Enforces EU AI Act §6, GDPR Art. 22                  │
│  - Logs to tamper-proof audit trail                     │
└─────────────────────────────────────────────────────────┘
```

## High-risk AI systems

According to EU AI Act §6, these systems require explainability:

- **Clinical diagnosis** - Disease prediction, treatment recommendations
- **Outbreak forecasting** - Epidemic trajectory prediction
- **Resource allocation** - Hospital bed assignment, vaccine distribution
- **Triage decisions** - Patient prioritization

## Setup

### Install dependencies

```bash
pip install google-cloud-aiplatform shap tensorflow scikit-learn
```

### Configure Vertex AI

```python
from google.cloud import aiplatform

aiplatform.init(
    project="your-project-id",
    location="us-central1",
    staging_bucket="gs://your-bucket"
)
```

## Training with explainability

### AutoML time-series forecasting

```python
from google.cloud import aiplatform

# Create dataset
dataset = aiplatform.TimeSeriesDataset.create(
    display_name="cholera-outbreak-forecast",
    gcs_source="gs://your-bucket/outbreak-data.csv",
    time_column="timestamp",
    time_series_identifier_column="location",
    target_column="case_count"
)

# Train model with explainability
job = aiplatform.AutoMLForecastingTrainingJob(
    display_name="cholera-forecast-model",
    optimization_objective="minimize-rmse",
    column_transformations=[
        {"numeric": {"column_name": "temperature"}},
        {"numeric": {"column_name": "rainfall"}},
        {"numeric": {"column_name": "population_density"}}
    ]
)

model = job.run(
    dataset=dataset,
    target_column="case_count",
    time_column="timestamp",
    time_series_identifier_column="location",
    forecast_horizon=72,  # 72 hours
    data_granularity_unit="hour",
    data_granularity_count=1,
    export_evaluated_data_items=True,  # Enable explainability
    model_display_name="cholera-forecast-v1"
)
```

### Custom training with SHAP

```python
import shap
import numpy as np
from sklearn.ensemble import RandomForestClassifier

# Train model
X_train, y_train = load_training_data()
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Create SHAP explainer
explainer = shap.TreeExplainer(model)

# Generate SHAP values for predictions
X_test = load_test_data()
shap_values = explainer.shap_values(X_test)

# Feature importance
feature_importance = np.abs(shap_values).mean(axis=0)
```

## Making explainable predictions

### With SovereignGuardrail validation

```python
from governance_kernel.vector_ledger import SovereignGuardrail
from google.cloud import aiplatform
import shap

guardrail = SovereignGuardrail()

def make_explainable_prediction(patient_data, model_endpoint):
    """
    Make a high-risk clinical prediction with explainability.
    """
    # Get prediction from Vertex AI
    endpoint = aiplatform.Endpoint(model_endpoint)
    prediction = endpoint.predict(instances=[patient_data])
    
    # Generate SHAP explanation
    explainer = shap.KernelExplainer(
        model=lambda x: endpoint.predict(instances=x.tolist()).predictions,
        data=shap.sample(X_train, 100)
    )
    shap_values = explainer.shap_values(patient_data)
    
    # Extract explanation components
    confidence_score = prediction.predictions[0]['confidence']
    feature_contributions = dict(zip(
        patient_data.keys(),
        shap_values[0]
    ))
    
    # Sort features by absolute contribution
    top_features = sorted(
        feature_contributions.items(),
        key=lambda x: abs(x[1]),
        reverse=True
    )[:5]
    
    # Build evidence chain
    evidence_chain = [
        f"{feature}: {contribution:.3f}"
        for feature, contribution in top_features
    ]
    
    # Validate with SovereignGuardrail
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'clinical_diagnosis',
            'explanation': f"SHAP values: {feature_contributions}",
            'confidence_score': confidence_score,
            'evidence_chain': evidence_chain,
            'consent_token': 'VALID_TOKEN',
            'consent_scope': 'diagnosis'
        },
        jurisdiction='EU_AI_ACT'
    )
    
    return {
        'prediction': prediction.predictions[0],
        'confidence': confidence_score,
        'explanation': {
            'method': 'SHAP',
            'feature_contributions': feature_contributions,
            'top_features': top_features,
            'evidence_chain': evidence_chain
        }
    }

# Example usage
patient_data = {
    'age': 35,
    'temperature': 38.5,
    'symptoms': ['fever', 'cough', 'fatigue'],
    'location': 'Dadaab',
    'exposure_risk': 0.7
}

result = make_explainable_prediction(
    patient_data=patient_data,
    model_endpoint="projects/123/locations/us-central1/endpoints/456"
)

print(f"Prediction: {result['prediction']}")
print(f"Confidence: {result['confidence']:.2%}")
print(f"Top contributing features:")
for feature, contribution in result['explanation']['top_features']:
    print(f"  - {feature}: {contribution:.3f}")
```

## Visualization

### SHAP waterfall plot

```python
import shap
import matplotlib.pyplot as plt

# Generate SHAP values
shap_values = explainer.shap_values(patient_data)

# Create waterfall plot
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values[0],
        base_values=explainer.expected_value[0],
        data=patient_data,
        feature_names=list(patient_data.keys())
    )
)

plt.title("Clinical Diagnosis Explanation")
plt.tight_layout()
plt.savefig("shap_explanation.png")
```

### SHAP summary plot

```python
# Summary plot for multiple predictions
shap.summary_plot(
    shap_values,
    X_test,
    feature_names=feature_names,
    plot_type="bar"
)

plt.title("Feature Importance Across All Predictions")
plt.tight_layout()
plt.savefig("shap_summary.png")
```

## Compliance validation

### EU AI Act §6 requirements

```python
def validate_eu_ai_act_compliance(prediction_result):
    """
    Validate that prediction meets EU AI Act §6 requirements.
    """
    required_fields = [
        'explanation',
        'confidence_score',
        'evidence_chain'
    ]
    
    for field in required_fields:
        if field not in prediction_result:
            raise ValueError(f"Missing required field: {field}")
    
    # Confidence threshold for high-risk systems
    if prediction_result['confidence_score'] < 0.7:
        raise ValueError("Confidence too low for high-risk inference")
    
    # Minimum evidence chain length
    if len(prediction_result['evidence_chain']) < 3:
        raise ValueError("Insufficient evidence chain")
    
    print("✅ EU AI Act §6 compliance validated")
    return True
```

### GDPR Art. 22 requirements

```python
def validate_gdpr_art22_compliance(prediction_result):
    """
    Validate Right to Explanation (GDPR Art. 22).
    """
    # Must have human-readable explanation
    if 'explanation' not in prediction_result:
        raise ValueError("Missing explanation (GDPR Art. 22)")
    
    # Must identify decision logic
    if 'evidence_chain' not in prediction_result:
        raise ValueError("Missing evidence chain (GDPR Art. 22)")
    
    # Must provide significance and consequences
    if 'confidence_score' not in prediction_result:
        raise ValueError("Missing confidence score (GDPR Art. 22)")
    
    print("✅ GDPR Art. 22 compliance validated")
    return True
```

## Integration with outbreak forecasting

```python
from edge_node.ai_agents import EpidemiologicalForecastingAgent
import shap

class ExplainableForecastingAgent(EpidemiologicalForecastingAgent):
    """
    Forecasting agent with built-in explainability.
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.explainer = None
    
    def train_with_explainability(self, training_data):
        """Train model and create SHAP explainer."""
        # Train base model
        self.train(training_data)
        
        # Create explainer
        self.explainer = shap.TreeExplainer(self.model)
        
        print("✅ Model trained with explainability")
    
    def forecast_with_explanation(self, input_data):
        """Generate forecast with SHAP explanation."""
        # Get forecast
        forecast = self.forecast(input_data)
        
        # Generate SHAP values
        shap_values = self.explainer.shap_values(input_data)
        
        # Build explanation
        explanation = {
            'method': 'SHAP',
            'feature_contributions': dict(zip(
                input_data.keys(),
                shap_values[0]
            )),
            'base_value': self.explainer.expected_value
        }
        
        return {
            'forecast': forecast,
            'explanation': explanation
        }

# Usage
agent = ExplainableForecastingAgent(location="Dadaab")
agent.train_with_explainability(historical_data)

result = agent.forecast_with_explanation({
    'temperature': 32.5,
    'rainfall': 15.2,
    'population_density': 1200,
    'previous_cases': 45
})

print(f"Forecast: {result['forecast']}")
print(f"Explanation: {result['explanation']}")
```

## Performance considerations

- **SHAP computation**: ~100ms per prediction (KernelExplainer)
- **TreeExplainer**: ~10ms per prediction (faster for tree-based models)
- **Batch processing**: Use `shap.Explainer` for multiple predictions
- **Caching**: Cache explainers for repeated use

## Best practices

<AccordionGroup>
  <Accordion title="Choose the right explainer">
    - **TreeExplainer**: For tree-based models (XGBoost, Random Forest)
    - **DeepExplainer**: For neural networks (TensorFlow, PyTorch)
    - **KernelExplainer**: Model-agnostic (slower but universal)
  </Accordion>
  <Accordion title="Validate explanations">
    Always validate that explanations meet regulatory requirements before deployment
  </Accordion>
  <Accordion title="Store explanations">
    Log all explanations to tamper-proof audit trail for compliance
  </Accordion>
  <Accordion title="Human review">
    High-risk decisions should include human review of AI explanations
  </Accordion>
</AccordionGroup>

## Next steps

<CardGroup cols={2}>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy explainable AI agents
  </Card>
  <Card
    title="Governance"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="Vertex AI"
    icon="google"
    href="/deployment/gcp"
  >
    Deploy to Google Cloud Platform
  </Card>
  <Card
    title="Audit trail"
    icon="file-contract"
    href="/governance/audit"
  >
    Log explanations for compliance
  </Card>
</CardGroup>
