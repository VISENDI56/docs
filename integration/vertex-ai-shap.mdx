---
title: Vertex AI + SHAP integration
description: Right to Explanation for high-risk clinical AI with explainability
---

## Overview

iLuminara-Core integrates Google Cloud Vertex AI with SHAP (SHapley Additive exPlanations) to provide **Right to Explanation** for every high-risk clinical inference, ensuring compliance with EU AI Act §6 and GDPR Art. 22.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk AI decision requires explainability with SHAP values, feature importance, and evidence chain.
</Card>

## Architecture

```
┌─────────────────────────────────────┐
│      VERTEX AI MODEL                │
│  - AutoML Time-Series               │
│  - Custom Training                  │
│  - Batch/Online Prediction          │
└─────────────────────────────────────┘
              │
              │ Prediction
              ▼
┌─────────────────────────────────────┐
│   EXPLAINABILITY ENGINE             │
│  - SHAP Analysis                    │
│  - Feature Importance               │
│  - Evidence Chain                   │
└─────────────────────────────────────┘
              │
              │ Validation
              ▼
┌─────────────────────────────────────┐
│   SOVEREIGN GUARDRAIL               │
│  - High-Risk Threshold Check        │
│  - Explanation Validation           │
│  - Audit Logging                    │
└─────────────────────────────────────┘
```

## High-risk inference detection

The SovereignGuardrail automatically detects high-risk clinical inferences:

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# High-risk threshold: confidence > 0.7
if prediction_confidence > 0.7:
    # Requires explainability
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'diagnosis',
            'confidence_score': prediction_confidence,
            'explanation': shap_values,
            'evidence_chain': feature_contributions
        },
        jurisdiction='EU_AI_ACT'
    )
```

## SHAP integration

### Install dependencies

```bash
pip install shap google-cloud-aiplatform
```

### Basic usage

```python
import shap
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(project='your-project-id', location='us-central1')

# Load model
model = aiplatform.Model('projects/.../models/...')

# Get prediction
prediction = model.predict(instances=[patient_features])

# Generate SHAP explanation
explainer = shap.Explainer(model.predict, patient_features)
shap_values = explainer(patient_features)

# Visualize
shap.plots.waterfall(shap_values[0])
```

## Complete integration example

```python
from google.cloud import aiplatform
import shap
import numpy as np
from governance_kernel.vector_ledger import SovereignGuardrail, SovereigntyViolationError

class ExplainableHealthAI:
    """
    Vertex AI model with mandatory explainability for high-risk inferences.
    """
    
    def __init__(
        self,
        project_id: str,
        model_id: str,
        location: str = 'us-central1',
        high_risk_threshold: float = 0.7
    ):
        # Initialize Vertex AI
        aiplatform.init(project=project_id, location=location)
        
        # Load model
        self.model = aiplatform.Model(model_id)
        
        # Initialize guardrail
        self.guardrail = SovereignGuardrail()
        
        # High-risk threshold
        self.high_risk_threshold = high_risk_threshold
    
    def predict_with_explanation(
        self,
        patient_features: dict,
        patient_id: str,
        jurisdiction: str = 'EU_AI_ACT'
    ) -> dict:
        """
        Make prediction with mandatory explainability for high-risk cases.
        
        Args:
            patient_features: Patient data for prediction
            patient_id: Patient identifier
            jurisdiction: Legal jurisdiction
        
        Returns:
            Prediction with explanation and compliance validation
        """
        # Convert features to array
        features_array = np.array([list(patient_features.values())])
        
        # Get prediction
        prediction = self.model.predict(instances=[features_array.tolist()])
        
        # Extract confidence score
        confidence_score = float(prediction.predictions[0][0])
        predicted_class = int(np.argmax(prediction.predictions[0]))
        
        # Check if high-risk
        is_high_risk = confidence_score > self.high_risk_threshold
        
        explanation = None
        if is_high_risk:
            # Generate SHAP explanation
            explainer = shap.Explainer(
                lambda x: self.model.predict(instances=x.tolist()).predictions,
                features_array
            )
            shap_values = explainer(features_array)
            
            # Extract feature contributions
            feature_names = list(patient_features.keys())
            feature_contributions = {
                feature_names[i]: float(shap_values.values[0][i])
                for i in range(len(feature_names))
            }
            
            # Build evidence chain
            evidence_chain = sorted(
                feature_contributions.items(),
                key=lambda x: abs(x[1]),
                reverse=True
            )[:5]  # Top 5 features
            
            explanation = {
                'shap_values': shap_values.values[0].tolist(),
                'feature_contributions': feature_contributions,
                'evidence_chain': [f"{k}: {v:.3f}" for k, v in evidence_chain],
                'base_value': float(shap_values.base_values[0])
            }
            
            # Validate with SovereignGuardrail
            try:
                self.guardrail.validate_action(
                    action_type='High_Risk_Inference',
                    payload={
                        'actor': 'vertex_ai_model',
                        'resource': f'patient_{patient_id}',
                        'inference': 'diagnosis',
                        'confidence_score': confidence_score,
                        'explanation': explanation,
                        'evidence_chain': [k for k, v in evidence_chain],
                        'consent_token': 'VALID_TOKEN',  # Should be real token
                        'consent_scope': 'diagnosis'
                    },
                    jurisdiction=jurisdiction
                )
            except SovereigntyViolationError as e:
                raise ValueError(f"High-risk inference blocked: {e}")
        
        return {
            'patient_id': patient_id,
            'prediction': predicted_class,
            'confidence_score': confidence_score,
            'is_high_risk': is_high_risk,
            'explanation': explanation,
            'compliance_validated': is_high_risk,
            'jurisdiction': jurisdiction
        }


# Example usage
if __name__ == "__main__":
    # Initialize explainable AI
    ai = ExplainableHealthAI(
        project_id='iluminara-health',
        model_id='projects/iluminara-health/locations/us-central1/models/cholera-predictor',
        high_risk_threshold=0.7
    )
    
    # Patient features
    patient_features = {
        'age': 35,
        'temperature': 38.5,
        'diarrhea_severity': 8,
        'vomiting': 1,
        'dehydration_score': 7,
        'location_risk': 0.9
    }
    
    # Get prediction with explanation
    result = ai.predict_with_explanation(
        patient_features=patient_features,
        patient_id='PAT_12345',
        jurisdiction='EU_AI_ACT'
    )
    
    print(f"Prediction: {result['prediction']}")
    print(f"Confidence: {result['confidence_score']:.2%}")
    print(f"High-Risk: {result['is_high_risk']}")
    
    if result['explanation']:
        print("\nExplanation:")
        print(f"Evidence Chain: {result['explanation']['evidence_chain']}")
        print(f"Feature Contributions: {result['explanation']['feature_contributions']}")
```

## Vertex AI AutoML integration

For AutoML models, use the built-in explainability:

```python
from google.cloud import aiplatform

# Deploy model with explainability
model = aiplatform.Model('projects/.../models/...')

endpoint = model.deploy(
    deployed_model_display_name='cholera-predictor',
    machine_type='n1-standard-4',
    explanation_metadata=aiplatform.explain.ExplanationMetadata(
        inputs={
            'age': {'input_tensor_name': 'age'},
            'temperature': {'input_tensor_name': 'temperature'},
            'diarrhea_severity': {'input_tensor_name': 'diarrhea_severity'}
        },
        outputs={'prediction': {'output_tensor_name': 'prediction'}}
    ),
    explanation_parameters=aiplatform.explain.ExplanationParameters(
        sampled_shapley_attribution=aiplatform.explain.SampledShapleyAttribution(
            path_count=10
        )
    )
)

# Get prediction with explanation
response = endpoint.explain(instances=[patient_features])

print(f"Prediction: {response.predictions}")
print(f"Explanations: {response.explanations}")
```

## Compliance validation

The SovereignGuardrail enforces explanation requirements:

<AccordionGroup>
  <Accordion title="EU AI Act §6 (High-Risk AI)">
    High-risk AI systems must provide explanations for decisions affecting individuals.
  </Accordion>
  <Accordion title="GDPR Art. 22 (Right to Explanation)">
    Individuals have the right to obtain an explanation of automated decisions.
  </Accordion>
  <Accordion title="ISO 27001 A.18.1.4 (Privacy)">
    Privacy impact assessments must include explainability for automated processing.
  </Accordion>
</AccordionGroup>

## Visualization

Generate SHAP visualizations for clinical review:

```python
import shap
import matplotlib.pyplot as plt

# Waterfall plot (single prediction)
shap.plots.waterfall(shap_values[0])
plt.savefig('shap_waterfall.png')

# Force plot (interactive)
shap.plots.force(shap_values[0])

# Summary plot (multiple predictions)
shap.plots.summary(shap_values, patient_features)
plt.savefig('shap_summary.png')

# Dependence plot (feature interaction)
shap.plots.dependence('temperature', shap_values, patient_features)
plt.savefig('shap_dependence.png')
```

## Performance considerations

- **SHAP computation**: ~100-500ms per prediction
- **Caching**: Cache SHAP explainers for repeated predictions
- **Batch processing**: Use batch prediction for non-urgent cases
- **Sampling**: Use sampled Shapley for faster approximations

## Next steps

<CardGroup cols={2}>
  <Card
    title="Bio-Interface API"
    icon="mobile"
    href="/integration/bio-interface"
  >
    Mobile health app integration
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Understand compliance enforcement
  </Card>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy autonomous surveillance
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/overview"
  >
    Deploy to production
  </Card>
</CardGroup>
