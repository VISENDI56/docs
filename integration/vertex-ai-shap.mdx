---
title: Vertex AI + SHAP integration
description: Right to Explanation compliance with explainable AI
---

## Overview

iLuminara integrates Google Cloud Vertex AI with SHAP (SHapley Additive exPlanations) to provide explainable AI that complies with the EU AI Act §6 and GDPR Article 22 (Right to Explanation).

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk clinical inference requires explainability with SHAP values, feature importance, and decision rationale.
</Card>

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    VERTEX AI MODEL                          │
│  - AutoML Time-Series Forecasting                          │
│  - Custom ML Models                                         │
│  - 72-hour Outbreak Prediction                             │
└─────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│                    SHAP EXPLAINER                           │
│  - TreeExplainer (for tree-based models)                   │
│  - KernelExplainer (for any model)                         │
│  - Feature Importance Calculation                          │
└─────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│                SOVEREIGNGUARDRAIL                           │
│  - Validates explanation completeness                       │
│  - Enforces EU AI Act §6                                   │
│  - Logs to tamper-proof audit trail                        │
└─────────────────────────────────────────────────────────────┘
```

## Implementation

### Step 1: Train model on Vertex AI

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project="your-project-id",
    location="us-central1"
)

# Create AutoML time-series dataset
dataset = aiplatform.TimeSeriesDataset.create(
    display_name="cholera-outbreak-data",
    gcs_source="gs://your-bucket/outbreak-data.csv",
    target_column="case_count"
)

# Train AutoML model
model = aiplatform.AutoMLForecastingTrainingJob(
    display_name="outbreak-forecaster",
    optimization_objective="minimize-rmse",
    column_specs={
        "case_count": "numeric",
        "location": "categorical",
        "temperature": "numeric",
        "rainfall": "numeric"
    }
)

model.run(
    dataset=dataset,
    target_column="case_count",
    time_column="date",
    time_series_identifier_column="location",
    forecast_horizon=72,  # 72 hours
    budget_milli_node_hours=1000
)
```

### Step 2: Generate SHAP explanations

```python
import shap
import numpy as np
from google.cloud import aiplatform

# Load trained model
endpoint = aiplatform.Endpoint("projects/.../endpoints/...")

# Prepare data for explanation
X_test = np.array([
    [0.4221, 40.2255, 28.5, 15.2],  # [lat, lng, temp, rainfall]
    # ... more samples
])

# Create SHAP explainer
explainer = shap.KernelExplainer(
    model=lambda x: endpoint.predict(instances=x.tolist()).predictions,
    data=shap.sample(X_test, 100)  # Background dataset
)

# Calculate SHAP values
shap_values = explainer.shap_values(X_test)

# Visualize
shap.summary_plot(shap_values, X_test, feature_names=[
    "latitude", "longitude", "temperature", "rainfall"
])
```

### Step 3: Integrate with SovereignGuardrail

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# Validate high-risk inference
prediction = endpoint.predict(instances=[[0.4221, 40.2255, 28.5, 15.2]])

# Generate explanation
explanation = {
    "shap_values": shap_values[0].tolist(),
    "feature_names": ["latitude", "longitude", "temperature", "rainfall"],
    "feature_values": [0.4221, 40.2255, 28.5, 15.2],
    "base_value": explainer.expected_value,
    "prediction": prediction.predictions[0]
}

# Validate with SovereignGuardrail
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'outbreak_prediction',
        'explanation': explanation,
        'confidence_score': 0.92,
        'evidence_chain': ['fever_cases', 'water_contamination', 'population_density'],
        'consent_token': 'VALID_TOKEN',
        'consent_scope': 'public_health_surveillance'
    },
    jurisdiction='EU_AI_ACT'
)
```

## Explanation requirements

### EU AI Act §6 compliance

<AccordionGroup>
  <Accordion title="Confidence score">
    Every prediction must include a confidence score (0.0-1.0)
  </Accordion>
  <Accordion title="Evidence chain">
    List of features that contributed to the decision
  </Accordion>
  <Accordion title="SHAP values">
    Quantitative contribution of each feature
  </Accordion>
  <Accordion title="Decision rationale">
    Human-readable explanation of the prediction
  </Accordion>
</AccordionGroup>

### Example explanation output

```json
{
  "prediction": {
    "case_count": 127,
    "confidence": 0.92,
    "forecast_horizon_hours": 72
  },
  "explanation": {
    "shap_values": {
      "temperature": 0.35,
      "rainfall": 0.28,
      "population_density": 0.18,
      "water_quality": 0.12,
      "previous_cases": 0.07
    },
    "feature_importance": [
      "High temperature (28.5°C) increases cholera risk by 35%",
      "Recent rainfall (15.2mm) increases contamination risk by 28%",
      "Dense population (5000/km²) accelerates transmission by 18%"
    ],
    "decision_rationale": "High-risk outbreak predicted due to elevated temperature and recent rainfall in densely populated area with compromised water infrastructure.",
    "base_value": 45.2,
    "evidence_chain": [
      "temperature_anomaly",
      "rainfall_event",
      "water_contamination_alert",
      "population_density_high"
    ]
  },
  "compliance": {
    "eu_ai_act_s6": "COMPLIANT",
    "gdpr_art22": "COMPLIANT",
    "explanation_completeness": 1.0
  }
}
```

## Visualization

### SHAP waterfall plot

```python
import shap
import matplotlib.pyplot as plt

# Generate waterfall plot for single prediction
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values[0],
        base_values=explainer.expected_value,
        data=X_test[0],
        feature_names=["latitude", "longitude", "temperature", "rainfall"]
    )
)
plt.savefig("shap_waterfall.png")
```

### SHAP force plot

```python
# Interactive force plot
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    X_test[0],
    feature_names=["latitude", "longitude", "temperature", "rainfall"]
)
```

## Model registry

Store models with explanations in Vertex AI Model Registry:

```python
# Register model with metadata
model.upload(
    display_name="outbreak-forecaster-v1",
    description="72-hour cholera outbreak forecaster with SHAP explanations",
    labels={
        "compliance": "eu-ai-act",
        "explainability": "shap",
        "risk_level": "high"
    }
)

# Add explanation metadata
model.update(
    explanation_metadata={
        "inputs": {
            "latitude": {"input_tensor_name": "latitude"},
            "longitude": {"input_tensor_name": "longitude"},
            "temperature": {"input_tensor_name": "temperature"},
            "rainfall": {"input_tensor_name": "rainfall"}
        },
        "outputs": {
            "case_count": {"output_tensor_name": "predictions"}
        },
        "explanation_method": "shap"
    }
)
```

## Audit trail

All explanations are logged to the tamper-proof audit trail:

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail(enable_tamper_proof_audit=True)

# Explanation is automatically logged
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'outbreak_prediction',
        'explanation': explanation,
        'model_id': 'outbreak-forecaster-v1',
        'model_version': '1.0.0',
        'timestamp': datetime.utcnow().isoformat()
    },
    jurisdiction='EU_AI_ACT'
)

# Retrieve audit history
history = guardrail.get_tamper_proof_audit_history(limit=100)
```

## Performance considerations

- **SHAP calculation time**: ~100ms per prediction (KernelExplainer)
- **TreeExplainer**: ~10ms per prediction (for tree-based models)
- **Caching**: Cache SHAP values for repeated predictions
- **Batch processing**: Calculate SHAP values in batches for efficiency

## Next steps

<CardGroup cols={2}>
  <Card
    title="Bio-Interface API"
    icon="mobile"
    href="/integration/bio-interface"
  >
    Mobile health app integration
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Compliance enforcement
  </Card>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Autonomous surveillance
  </Card>
  <Card
    title="Deploy to GCP"
    icon="cloud"
    href="/deployment/overview"
  >
    Production deployment
  </Card>
</CardGroup>
