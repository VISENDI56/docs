---
title: Vertex AI + SHAP integration
description: Right to Explanation with explainable AI for high-risk clinical inferences
---

## Overview

iLuminara integrates with Google Cloud Vertex AI to provide explainable AI for high-risk clinical inferences, ensuring compliance with EU AI Act §6 and GDPR Art. 22 (Right to Explanation).

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk clinical inference requires SHAP explainability
</Card>

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    VERTEX AI MODEL                          │
│  - AutoML Time-Series Forecasting                          │
│  - Custom Training (TensorFlow, PyTorch)                   │
│  - Model Deployment & Serving                              │
└─────────────────────────────────────────────────────────────┘
                            │
                            │ Prediction Request
                            ▼
┌─────────────────────────────────────────────────────────────┐
│              EXPLAINABILITY ENGINE                          │
│  - SHAP (SHapley Additive exPlanations)                    │
│  - LIME (Local Interpretable Model-agnostic)               │
│  - Feature Importance                                       │
└─────────────────────────────────────────────────────────────┘
                            │
                            │ Explanation + Prediction
                            ▼
┌─────────────────────────────────────────────────────────────┐
│              SOVEREIGN GUARDRAIL                            │
│  - Validates explainability requirements                   │
│  - Enforces EU AI Act §6                                   │
│  - Logs to Chrono-Audit (IP-09)                           │
└─────────────────────────────────────────────────────────────┘
```

## Setup

### 1. Enable Vertex AI

```bash
# Enable Vertex AI API
gcloud services enable aiplatform.googleapis.com

# Set project and region
export PROJECT_ID=your-project-id
export REGION=us-central1
```

### 2. Train Model with Explainability

```python
from google.cloud import aiplatform
from google.cloud.aiplatform import explain

# Initialize Vertex AI
aiplatform.init(project=PROJECT_ID, location=REGION)

# Configure explainability
explanation_metadata = explain.ExplanationMetadata(
    inputs={
        "features": {
            "input_tensor_name": "features",
            "encoding": "BAG_OF_FEATURES",
            "modality": "numeric",
            "index_feature_mapping": [
                "fever",
                "cough",
                "diarrhea",
                "vomiting",
                "dehydration"
            ]
        }
    },
    outputs={
        "prediction": {
            "output_tensor_name": "prediction"
        }
    }
)

explanation_parameters = explain.ExplanationParameters(
    sampled_shapley_attribution=explain.SampledShapleyAttribution(
        path_count=10
    )
)

# Create model with explainability
model = aiplatform.Model.upload(
    display_name="cholera-prediction-model",
    artifact_uri="gs://your-bucket/model",
    serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-8:latest",
    explanation_metadata=explanation_metadata,
    explanation_parameters=explanation_parameters
)
```

### 3. Deploy Model

```python
# Deploy to endpoint
endpoint = model.deploy(
    machine_type="n1-standard-4",
    min_replica_count=1,
    max_replica_count=10,
    traffic_percentage=100
)

print(f"Model deployed to: {endpoint.resource_name}")
```

## Making Predictions with Explanations

### Basic Prediction

```python
from google.cloud import aiplatform

# Initialize endpoint
endpoint = aiplatform.Endpoint(endpoint_name="projects/.../endpoints/...")

# Prepare input
instances = [
    {
        "features": [1, 1, 1, 1, 0]  # fever, cough, diarrhea, vomiting, dehydration
    }
]

# Get prediction with explanation
response = endpoint.predict(instances=instances)

# Extract prediction and explanation
prediction = response.predictions[0]
explanations = response.explanations[0]

print(f"Prediction: {prediction}")
print(f"SHAP values: {explanations.attributions}")
```

### Integration with SovereignGuardrail

```python
from governance_kernel.vector_ledger import SovereignGuardrail, SovereigntyViolationError
from google.cloud import aiplatform

guardrail = SovereignGuardrail(enable_tamper_proof_audit=True)

def predict_with_compliance(endpoint, features, patient_id):
    """
    Make prediction with compliance validation
    """
    # Get prediction with explanation
    response = endpoint.predict(instances=[{"features": features}])
    
    prediction = response.predictions[0]
    explanations = response.explanations[0]
    
    # Extract SHAP values
    shap_values = [attr.feature_attributions for attr in explanations.attributions]
    confidence_score = prediction["confidence"]
    
    # Validate with SovereignGuardrail
    try:
        guardrail.validate_action(
            action_type='High_Risk_Inference',
            payload={
                'actor': 'vertex_ai_model',
                'resource': f'patient_{patient_id}',
                'explanation': f'SHAP values: {shap_values}',
                'confidence_score': confidence_score,
                'evidence_chain': features,
                'consent_token': 'valid_token',
                'consent_scope': 'diagnosis'
            },
            jurisdiction='EU_AI_ACT'
        )
        
        return {
            "prediction": prediction,
            "explanation": shap_values,
            "compliant": True
        }
    
    except SovereigntyViolationError as e:
        return {
            "prediction": None,
            "explanation": None,
            "compliant": False,
            "error": str(e)
        }
```

## SHAP Explainability

### Feature Importance

```python
import shap
import numpy as np

def explain_prediction(model, features, feature_names):
    """
    Generate SHAP explanation for prediction
    """
    # Create explainer
    explainer = shap.KernelExplainer(model.predict, features)
    
    # Calculate SHAP values
    shap_values = explainer.shap_values(features)
    
    # Create explanation
    explanation = {
        "feature_importance": dict(zip(feature_names, shap_values[0])),
        "base_value": explainer.expected_value,
        "prediction": model.predict(features)[0]
    }
    
    return explanation

# Example usage
features = np.array([[1, 1, 1, 1, 0]])  # fever, cough, diarrhea, vomiting, dehydration
feature_names = ["fever", "cough", "diarrhea", "vomiting", "dehydration"]

explanation = explain_prediction(model, features, feature_names)

print("Feature Importance:")
for feature, importance in explanation["feature_importance"].items():
    print(f"  {feature}: {importance:.3f}")
```

### Visualization

```python
import shap
import matplotlib.pyplot as plt

# Create SHAP explainer
explainer = shap.Explainer(model)
shap_values = explainer(features)

# Force plot (single prediction)
shap.plots.force(shap_values[0])

# Waterfall plot
shap.plots.waterfall(shap_values[0])

# Summary plot (multiple predictions)
shap.plots.summary(shap_values)
```

## Compliance Requirements

### EU AI Act §6 (High-Risk AI)

High-risk AI systems must provide:

1. **Explainability** - SHAP values for each prediction
2. **Confidence Score** - Probability of prediction
3. **Evidence Chain** - Input features used
4. **Human Oversight** - Ability to override AI decision

```python
def validate_high_risk_inference(prediction, explanation, confidence_score):
    """
    Validate high-risk inference meets EU AI Act requirements
    """
    requirements = {
        "explainability": explanation is not None,
        "confidence_score": confidence_score is not None,
        "evidence_chain": len(explanation.get("feature_importance", {})) > 0,
        "human_oversight": True  # Requires manual review
    }
    
    if not all(requirements.values()):
        raise ValueError("High-risk inference does not meet EU AI Act §6 requirements")
    
    return requirements
```

### GDPR Art. 22 (Right to Explanation)

Users have the right to:

1. **Obtain explanation** - Understand how decision was made
2. **Contest decision** - Challenge automated decision
3. **Request human intervention** - Escalate to human review

```python
def generate_user_explanation(shap_values, feature_names, prediction):
    """
    Generate human-readable explanation for user
    """
    # Sort features by importance
    importance = sorted(
        zip(feature_names, shap_values),
        key=lambda x: abs(x[1]),
        reverse=True
    )
    
    explanation = f"Prediction: {prediction['diagnosis']} (Confidence: {prediction['confidence']:.1%})\\n\\n"
    explanation += "This prediction was based on the following factors:\\n"
    
    for feature, value in importance[:5]:  # Top 5 features
        direction = "increased" if value > 0 else "decreased"
        explanation += f"  • {feature}: {direction} likelihood by {abs(value):.1%}\\n"
    
    explanation += "\\nYou have the right to:\\n"
    explanation += "  1. Request human review of this decision\\n"
    explanation += "  2. Contest this automated decision\\n"
    explanation += "  3. Receive additional explanation\\n"
    
    return explanation
```

## Monitoring & Audit

### Log Predictions to Chrono-Audit

```python
from governance_kernel.chrono_audit import ChronoAudit, ChronoEventType

chrono = ChronoAudit(enable_tsa=False)

def log_prediction(patient_id, prediction, explanation, confidence_score):
    """
    Log prediction to Chrono-Audit for compliance
    """
    event = chrono.record_event(
        event_type=ChronoEventType.HIGH_RISK_INFERENCE,
        actor="vertex_ai_model",
        resource=f"patient_{patient_id}",
        action="predict_diagnosis",
        jurisdiction="EU_AI_ACT",
        metadata={
            "prediction": prediction,
            "confidence_score": confidence_score,
            "shap_values": explanation,
            "model_version": "v1.2.3"
        },
        compliance_frameworks=["EU_AI_ACT", "GDPR"],
        retention_days=1825  # 5 years
    )
    
    return event
```

### Model Performance Monitoring

```python
from google.cloud import aiplatform_v1

def monitor_model_performance(endpoint_name):
    """
    Monitor model performance and drift
    """
    client = aiplatform_v1.EndpointServiceClient()
    
    # Get endpoint metrics
    endpoint = client.get_endpoint(name=endpoint_name)
    
    # Monitor prediction latency
    latency = endpoint.deployed_models[0].automatic_resources.max_replica_count
    
    # Monitor prediction accuracy (requires ground truth)
    # This would be implemented with your specific metrics
    
    return {
        "endpoint": endpoint_name,
        "latency_ms": latency,
        "status": "healthy"
    }
```

## Best Practices

<AccordionGroup>
  <Accordion title="Always provide explanations">
    Every high-risk inference must include SHAP values and feature importance
  </Accordion>
  <Accordion title="Log all predictions">
    Use Chrono-Audit to create tamper-proof audit trail
  </Accordion>
  <Accordion title="Enable human oversight">
    Provide mechanism for human review and override
  </Accordion>
  <Accordion title="Monitor model drift">
    Track prediction accuracy and retrain when performance degrades
  </Accordion>
  <Accordion title="Respect data sovereignty">
    Ensure model training data stays within sovereign territory
  </Accordion>
</AccordionGroup>

## Next steps

<CardGroup cols={2}>
  <Card
    title="Bio-Interface"
    icon="mobile"
    href="/integration/bio-interface"
  >
    Mobile health app integration
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    29-framework compliance
  </Card>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Autonomous surveillance
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/overview"
  >
    Deploy to production
  </Card>
</CardGroup>
