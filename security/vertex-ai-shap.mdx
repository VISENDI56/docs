---
title: Vertex AI + SHAP integration
description: Right to Explanation for high-risk AI inferences with explainability
---

## Overview

iLuminara-Core integrates Vertex AI with SHAP (SHapley Additive exPlanations) to provide the **Right to Explanation** for every high-risk clinical inference, as required by EU AI Act §6 and GDPR Art. 22.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk inference requires explainability with SHAP values, feature importance, and evidence chain.
</Card>

## Architecture

```
┌─────────────────────────────────────────────────────────┐
│                    VERTEX AI MODEL                      │
│  - AutoML Time-Series Forecasting                      │
│  - Custom Training (TensorFlow/PyTorch)                │
│  - Batch/Online Prediction                             │
└─────────────────────────────────────────────────────────┘
                         ▼
┌─────────────────────────────────────────────────────────┐
│                  SHAP EXPLAINER                         │
│  - TreeExplainer (for tree-based models)               │
│  - DeepExplainer (for neural networks)                 │
│  - KernelExplainer (model-agnostic)                    │
└─────────────────────────────────────────────────────────┘
                         ▼
┌─────────────────────────────────────────────────────────┐
│               SOVEREIGNGUARDRAIL                        │
│  - Validates explanation completeness                   │
│  - Enforces confidence thresholds                       │
│  - Logs to tamper-proof audit trail                    │
└─────────────────────────────────────────────────────────┘
```

## High-risk AI systems

According to EU AI Act §6, these systems require explainability:

<CardGroup cols={2}>
  <Card title="Clinical diagnosis" icon="stethoscope">
    AI-assisted disease diagnosis and treatment recommendations
  </Card>
  <Card title="Outbreak prediction" icon="chart-line">
    Forecasting disease outbreaks and resource allocation
  </Card>
  <Card title="Risk stratification" icon="ranking-star">
    Patient risk scoring and triage decisions
  </Card>
  <Card title="Resource allocation" icon="hospital">
    Automated allocation of medical resources
  </Card>
</CardGroup>

## Implementation

### Basic usage

```python
from google.cloud import aiplatform
from governance_kernel.vector_ledger import SovereignGuardrail
import shap
import numpy as np

# Initialize Vertex AI
aiplatform.init(project="your-project-id", location="us-central1")

# Load model
model = aiplatform.Model("projects/.../models/...")

# Prepare data
patient_features = {
    "age": 45,
    "temperature": 38.5,
    "symptoms": ["fever", "cough", "fatigue"],
    "location": "Dadaab"
}

# Make prediction
prediction = model.predict(instances=[patient_features])
confidence_score = prediction.predictions[0]["confidence"]

# Generate SHAP explanation
explainer = shap.KernelExplainer(model.predict, background_data)
shap_values = explainer.shap_values(patient_features)

# Validate with SovereignGuardrail
guardrail = SovereignGuardrail()

try:
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'cholera_diagnosis',
            'confidence_score': confidence_score,
            'explanation': {
                'shap_values': shap_values.tolist(),
                'feature_importance': dict(zip(features, shap_values)),
                'base_value': explainer.expected_value
            },
            'evidence_chain': [
                'fever (SHAP: 0.45)',
                'diarrhea (SHAP: 0.38)',
                'location_risk (SHAP: 0.12)'
            ],
            'consent_token': 'VALID_TOKEN',
            'consent_scope': 'diagnosis'
        },
        jurisdiction='EU_AI_ACT'
    )
    
    print("✅ Inference approved with explanation")
    
except SovereigntyViolationError as e:
    print(f"❌ Inference blocked: {e}")
```

### Vertex AI AutoML integration

```python
from google.cloud import aiplatform
from google.cloud.aiplatform import gapic

# Create AutoML time-series forecasting model
dataset = aiplatform.TimeSeriesDataset.create(
    display_name="cholera-outbreak-forecast",
    gcs_source="gs://your-bucket/outbreak-data.csv",
    time_column="timestamp",
    time_series_identifier_column="location",
    target_column="case_count"
)

# Train model with explainability
training_job = aiplatform.AutoMLForecastingTrainingJob(
    display_name="cholera-forecast-model",
    optimization_objective="minimize-rmse",
    column_specs={
        "temperature": "numeric",
        "rainfall": "numeric",
        "population_density": "numeric"
    }
)

model = training_job.run(
    dataset=dataset,
    target_column="case_count",
    time_column="timestamp",
    time_series_identifier_column="location",
    forecast_horizon=72,  # 72 hours
    data_granularity_unit="hour",
    data_granularity_count=1,
    enable_probabilistic_inference=True,
    export_evaluated_data_items=True
)

# Get feature attributions (built-in explainability)
attributions = model.explain(instances=[test_instance])

print(f"Feature attributions: {attributions}")
```

### Custom model with SHAP

```python
import tensorflow as tf
import shap

# Load custom TensorFlow model
model = tf.keras.models.load_model("models/outbreak_predictor")

# Create SHAP explainer
background = training_data.sample(100)
explainer = shap.DeepExplainer(model, background)

# Explain prediction
shap_values = explainer.shap_values(test_instance)

# Visualize
shap.force_plot(
    explainer.expected_value[0],
    shap_values[0],
    test_instance,
    matplotlib=True
)

# Extract top features
feature_importance = dict(zip(
    feature_names,
    np.abs(shap_values[0]).mean(axis=0)
))

sorted_features = sorted(
    feature_importance.items(),
    key=lambda x: x[1],
    reverse=True
)

print("Top 5 features:")
for feature, importance in sorted_features[:5]:
    print(f"  {feature}: {importance:.4f}")
```

## Explanation requirements

The SovereignGuardrail enforces these explanation requirements:

<Steps>
  <Step title="Confidence score">
    Prediction confidence (0.0 - 1.0)
  </Step>
  <Step title="SHAP values">
    Feature contribution to prediction
  </Step>
  <Step title="Feature importance">
    Ranked list of influential features
  </Step>
  <Step title="Evidence chain">
    Human-readable explanation of decision
  </Step>
  <Step title="Base value">
    Expected value without features
  </Step>
</Steps>

### Example explanation payload

```json
{
  "inference": "cholera_outbreak_predicted",
  "confidence_score": 0.92,
  "explanation": {
    "shap_values": [0.45, 0.38, 0.12, -0.05],
    "feature_importance": {
      "fever_cases": 0.45,
      "diarrhea_cases": 0.38,
      "location_risk": 0.12,
      "rainfall": -0.05
    },
    "base_value": 0.15
  },
  "evidence_chain": [
    "High fever case count (SHAP: 0.45)",
    "Elevated diarrhea reports (SHAP: 0.38)",
    "High-risk location (Dadaab) (SHAP: 0.12)",
    "Recent rainfall reduces risk (SHAP: -0.05)"
  ],
  "decision_rationale": "Cholera outbreak predicted with 92% confidence based on elevated fever and diarrhea cases in high-risk location."
}
```

## Explainability methods

### TreeExplainer (for tree-based models)

```python
import shap
from sklearn.ensemble import RandomForestClassifier

# Train model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Create explainer
explainer = shap.TreeExplainer(model)

# Explain predictions
shap_values = explainer.shap_values(X_test)

# Visualize
shap.summary_plot(shap_values, X_test)
```

### DeepExplainer (for neural networks)

```python
import shap
import tensorflow as tf

# Load model
model = tf.keras.models.load_model("model.h5")

# Create explainer
explainer = shap.DeepExplainer(model, background_data)

# Explain
shap_values = explainer.shap_values(test_data)

# Visualize
shap.image_plot(shap_values, test_data)
```

### KernelExplainer (model-agnostic)

```python
import shap

# Create explainer for any model
explainer = shap.KernelExplainer(
    model.predict,
    background_data
)

# Explain
shap_values = explainer.shap_values(test_instance)

# Visualize
shap.force_plot(
    explainer.expected_value,
    shap_values,
    test_instance
)
```

## Compliance validation

### EU AI Act §6 compliance

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# High-risk AI system must provide explanation
result = guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'diagnosis',
        'confidence_score': 0.95,
        'explanation': shap_explanation,
        'evidence_chain': evidence,
        'consent_token': 'VALID',
        'consent_scope': 'diagnosis'
    },
    jurisdiction='EU_AI_ACT'
)

# Result includes compliance status
print(f"Compliant: {result['compliant']}")
print(f"Frameworks: {result['frameworks_satisfied']}")
```

### GDPR Art. 22 compliance

```python
# Right to explanation for automated decision-making
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'treatment_recommendation',
        'confidence_score': 0.88,
        'explanation': {
            'method': 'SHAP',
            'shap_values': shap_values,
            'feature_importance': feature_importance
        },
        'evidence_chain': [
            'Patient age: 45 (SHAP: 0.12)',
            'Symptom severity: High (SHAP: 0.56)',
            'Medical history: Relevant (SHAP: 0.20)'
        ],
        'human_review_available': True,
        'consent_token': 'VALID'
    },
    jurisdiction='GDPR_EU'
)
```

## Monitoring explainability

### Track explanation quality

```python
from prometheus_client import Counter, Histogram

# Metrics
explanations_generated = Counter(
    'explanations_generated_total',
    'Total explanations generated'
)

explanation_quality = Histogram(
    'explanation_quality_score',
    'Quality score of explanations'
)

# Track
explanations_generated.inc()
explanation_quality.observe(quality_score)
```

### Audit trail

All explanations are logged to the tamper-proof audit trail:

```python
{
  "timestamp": "2025-12-23T15:00:00Z",
  "action": "High_Risk_Inference",
  "inference": "cholera_diagnosis",
  "confidence_score": 0.92,
  "explanation_method": "SHAP",
  "shap_values": [...],
  "evidence_chain": [...],
  "jurisdiction": "EU_AI_ACT",
  "compliant": true,
  "audit_hash": "sha256:..."
}
```

## Best practices

<AccordionGroup>
  <Accordion title="Use appropriate explainer">
    TreeExplainer for tree models, DeepExplainer for neural networks, KernelExplainer for black-box models
  </Accordion>
  <Accordion title="Validate explanation completeness">
    Always include confidence score, SHAP values, feature importance, and evidence chain
  </Accordion>
  <Accordion title="Human-readable explanations">
    Convert SHAP values to natural language for clinical staff
  </Accordion>
  <Accordion title="Monitor explanation drift">
    Track changes in feature importance over time
  </Accordion>
  <Accordion title="Enable human review">
    Provide mechanism for clinicians to override AI decisions
  </Accordion>
</AccordionGroup>

## Next steps

<CardGroup cols={2}>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy autonomous surveillance
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/gcp"
  >
    Deploy to Vertex AI
  </Card>
  <Card
    title="Security"
    icon="shield-halved"
    href="/security/overview"
  >
    Security stack overview
  </Card>
</CardGroup>
