---
title: Vertex AI + SHAP explainability
description: Right to Explanation for high-risk clinical AI (EU AI Act §6, GDPR Art. 22)
---

## Overview

Every high-risk clinical inference in iLuminara-Core requires explainability using SHAP (SHapley Additive exPlanations) integrated with Google Cloud Vertex AI.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  EU AI Act §6 (High-Risk AI), GDPR Art. 22 (Right to Explanation), ISO 27001 A.18.1.4
</Card>

## Why SHAP?

SHAP provides model-agnostic explanations that satisfy regulatory requirements:

- **Mathematically rigorous** - Based on game theory (Shapley values)
- **Model-agnostic** - Works with any ML model
- **Locally accurate** - Explains individual predictions
- **Consistent** - Same feature importance across models

## Architecture

```
┌─────────────────────────────────────────┐
│         VERTEX AI MODEL                 │
│  - Outbreak risk prediction             │
│  - Disease classification               │
│  - Resource allocation                  │
└─────────────────────────────────────────┘
                  │
                  │ Prediction
                  ▼
┌─────────────────────────────────────────┐
│      SHAP EXPLAINABILITY ENGINE         │
│  - Calculate SHAP values                │
│  - Generate feature importance          │
│  - Build evidence chain                 │
└─────────────────────────────────────────┘
                  │
                  │ Explanation
                  ▼
┌─────────────────────────────────────────┐
│      SOVEREIGNGUARDRAIL                 │
│  - Validate explanation quality         │
│  - Enforce compliance                   │
│  - Audit trail                          │
└─────────────────────────────────────────┘
```

## Basic usage

```python
from cloud_oracle.vertex_ai_explainability import OutbreakRiskExplainer

# Initialize explainer
explainer = OutbreakRiskExplainer(
    project_id="iluminara-health",
    location="us-central1",
    high_risk_threshold=0.7
)

# Explain outbreak risk prediction
explanation = explainer.explain_outbreak_risk(
    model_endpoint="projects/123/locations/us-central1/endpoints/456",
    location={"lat": 0.4221, "lng": 40.2255},
    symptoms=["diarrhea", "vomiting", "dehydration"],
    environmental_factors={
        "temperature": 32.5,
        "rainfall": 15.2,
        "population_density": 850
    }
)

# Access explanation
print(f"Prediction: {explanation.prediction:.1%}")
print(f"Compliance: {explanation.compliance_status}")
print(f"Evidence Chain:")
for evidence in explanation.evidence_chain:
    print(f"  - {evidence}")
```

## Explanation result

```python
@dataclass
class ExplanationResult:
    prediction: float                    # Model prediction (0-1)
    confidence_score: float              # Confidence level
    shap_values: Dict[str, float]        # SHAP values per feature
    feature_importance: Dict[str, float] # Absolute importance
    evidence_chain: List[str]            # Top contributing factors
    decision_rationale: str              # Human-readable explanation
    compliance_status: str               # Compliance check result
    timestamp: str                       # ISO 8601 timestamp
```

## High-risk threshold

Predictions above the high-risk threshold **require** explanation:

| Threshold | Risk Level | Explanation Required |
|-----------|------------|---------------------|
| ≥ 0.7 | HIGH | ✅ Yes (EU AI Act §6) |
| 0.4 - 0.7 | MODERATE | ⚠️ Recommended |
| < 0.4 | LOW | ❌ Optional |

## SHAP values interpretation

SHAP values indicate how much each feature contributes to the prediction:

- **Positive SHAP value** - Feature increases risk
- **Negative SHAP value** - Feature decreases risk
- **Magnitude** - Strength of contribution

### Example

```python
shap_values = {
    "temperature": 0.15,      # Hot weather increases risk
    "rainfall": 0.12,         # Heavy rain increases risk
    "population_density": 0.08,
    "sanitation_score": -0.05  # Good sanitation decreases risk
}
```

## Evidence chain

The evidence chain lists the top contributing factors in human-readable format:

```python
evidence_chain = [
    "temperature=32.5 increases risk by 0.150",
    "rainfall=15.2 increases risk by 0.120",
    "population_density=850 increases risk by 0.080",
    "sanitation_score=0.7 decreases risk by 0.050"
]
```

## Decision rationale

Automatically generated human-readable explanation:

```
Risk assessment: HIGH (85% confidence). 
Primary factor: temperature (SHAP: 0.150). 
Decision based on 8 clinical features with explainable AI (SHAP) analysis.
Symptoms reported: diarrhea, vomiting, dehydration. 
Location: (0.4221, 40.2255).
```

## Compliance validation

The explainer automatically validates compliance:

```python
def _check_compliance(
    self,
    confidence_score: float,
    has_explanation: bool,
    evidence_chain: List[str]
) -> str:
    is_high_risk = confidence_score >= self.high_risk_threshold
    
    if is_high_risk:
        if has_explanation and len(evidence_chain) > 0:
            return "COMPLIANT (EU AI Act §6, GDPR Art. 22)"
        else:
            return "NON-COMPLIANT (Missing explanation)"
    else:
        return "COMPLIANT (Low-risk inference)"
```

## Integration with SovereignGuardrail

All high-risk inferences are validated by the governance kernel:

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# Validate high-risk inference
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'explanation': explanation.shap_values,
        'confidence_score': explanation.confidence_score,
        'evidence_chain': explanation.evidence_chain,
        'consent_token': 'VALID_TOKEN',
        'consent_scope': 'diagnosis'
    },
    jurisdiction='EU_AI_ACT'
)
```

## Explanation quality validation

Ensure explanations meet quality standards:

```python
is_valid = explainer.validate_explanation_quality(
    explanation=explanation,
    min_evidence_items=3
)

if not is_valid:
    raise ValueError("Explanation does not meet quality standards")
```

**Quality requirements:**
- ✅ Confidence score present
- ✅ SHAP values calculated
- ✅ Evidence chain with ≥3 items
- ✅ Decision rationale provided
- ✅ Compliance status = COMPLIANT

## Vertex AI integration

### Deploy model with explanation

```python
from google.cloud import aiplatform

# Deploy model with explanation metadata
model = aiplatform.Model.upload(
    display_name="outbreak-risk-model",
    artifact_uri="gs://bucket/model",
    serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/...",
    explanation_metadata=aiplatform.explain.ExplanationMetadata(
        inputs={
            "temperature": {"input_tensor_name": "temperature"},
            "rainfall": {"input_tensor_name": "rainfall"},
            "population_density": {"input_tensor_name": "population_density"}
        },
        outputs={"risk_score": {"output_tensor_name": "risk_score"}}
    ),
    explanation_parameters=aiplatform.explain.ExplanationParameters(
        {"sampled_shapley_attribution": {"path_count": 10}}
    )
)

# Deploy to endpoint
endpoint = model.deploy(
    machine_type="n1-standard-4",
    min_replica_count=1,
    max_replica_count=10
)
```

## Background data for SHAP

SHAP requires background data to calculate baseline expectations:

```python
import numpy as np

# Historical outbreak data
background_data = np.array([
    [28.0, 10.5, 600],  # temperature, rainfall, population_density
    [30.5, 12.0, 750],
    [29.0, 8.5, 680],
    # ... more historical records
])

# Use background data for SHAP
explanation = explainer.explain_prediction(
    model_endpoint=endpoint_name,
    input_features=current_features,
    feature_names=["temperature", "rainfall", "population_density"],
    background_data=background_data
)
```

## Performance considerations

- **SHAP calculation time**: ~2-5 seconds per prediction
- **Background data size**: 50-100 samples recommended
- **Caching**: Cache SHAP explainer for repeated predictions
- **Batch processing**: Process multiple predictions in parallel

## Audit trail

All explanations are logged to the tamper-proof audit trail:

```python
{
    "timestamp": "2025-12-23T15:00:00Z",
    "action": "High_Risk_Inference",
    "prediction": 0.85,
    "shap_values": {...},
    "evidence_chain": [...],
    "compliance_status": "COMPLIANT",
    "jurisdiction": "EU_AI_ACT",
    "model_endpoint": "projects/.../endpoints/456"
}
```

## Testing

```python
# Test explainability
def test_high_risk_explanation():
    explainer = OutbreakRiskExplainer(
        project_id="test-project",
        high_risk_threshold=0.7
    )
    
    explanation = explainer.explain_outbreak_risk(
        model_endpoint="test-endpoint",
        location={"lat": 0.0, "lng": 0.0},
        symptoms=["fever"],
        environmental_factors={"temperature": 35.0}
    )
    
    # Validate explanation
    assert explanation.confidence_score >= 0.7
    assert len(explanation.shap_values) > 0
    assert len(explanation.evidence_chain) >= 3
    assert "COMPLIANT" in explanation.compliance_status
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="Deploy to Vertex AI"
    icon="google"
    href="/deployment/vertex-ai"
  >
    Deploy models with explanation
  </Card>
  <Card
    title="SovereignGuardrail"
    icon="shield-check"
    href="/governance/overview"
  >
    Integrate with compliance enforcement
  </Card>
  <Card
    title="API integration"
    icon="terminal"
    href="/api-reference/bio-interface"
  >
    Use explainability in Bio-Interface API
  </Card>
  <Card
    title="Audit trail"
    icon="file-contract"
    href="/governance/audit"
  >
    Configure tamper-proof logging
  </Card>
</CardGroup>
