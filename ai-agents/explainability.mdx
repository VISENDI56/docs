---
title: AI explainability
description: Vertex AI + SHAP integration for Right to Explanation compliance
---

## Overview

iLuminara-Core implements **Right to Explanation** for all high-risk AI inferences using SHAP (SHapley Additive exPlanations) integrated with Google Cloud Vertex AI.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  EU AI Act §6, GDPR Art. 22, ISO 27001 A.18.1.4
</Card>

## Why explainability matters

Every high-risk clinical inference requires explainability to:

- **Build trust** - Clinicians understand AI reasoning
- **Enable oversight** - Detect bias and errors
- **Ensure compliance** - Meet regulatory requirements
- **Support appeals** - Patients can challenge decisions

## Architecture

```
┌─────────────────────────────────────────┐
│         VERTEX AI MODEL                 │
│  - AutoML Time-Series Forecasting      │
│  - Custom TensorFlow Models             │
│  - Outbreak Prediction                  │
└─────────────────────────────────────────┘
                  │
                  │ Prediction
                  ▼
┌─────────────────────────────────────────┐
│         SHAP EXPLAINER                  │
│  - TreeExplainer (XGBoost, RF)         │
│  - DeepExplainer (Neural Networks)     │
│  - KernelExplainer (Black Box)         │
└─────────────────────────────────────────┘
                  │
                  │ Explanation
                  ▼
┌─────────────────────────────────────────┐
│      SOVEREIGNGUARDRAIL                 │
│  - Validates explanation completeness   │
│  - Enforces confidence thresholds       │
│  - Logs to tamper-proof audit trail    │
└─────────────────────────────────────────┘
```

## Basic usage

### Train model with explainability

```python
from google.cloud import aiplatform
import shap
import numpy as np

# Initialize Vertex AI
aiplatform.init(project="your-project-id", location="us-central1")

# Train model
model = aiplatform.AutoMLTabularTrainingJob(
    display_name="cholera-outbreak-predictor",
    optimization_prediction_type="regression",
)

dataset = aiplatform.TabularDataset("projects/.../datasets/...")

model.run(
    dataset=dataset,
    target_column="outbreak_cases",
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
)

# Deploy model
endpoint = model.deploy(
    machine_type="n1-standard-4",
    min_replica_count=1,
    max_replica_count=5,
)
```

### Generate SHAP explanations

```python
import shap
from governance_kernel.vector_ledger import SovereignGuardrail

# Load model
model = endpoint.predict

# Create SHAP explainer
explainer = shap.KernelExplainer(model, X_train)

# Generate explanation for prediction
shap_values = explainer.shap_values(X_test[0])

# Validate with SovereignGuardrail
guardrail = SovereignGuardrail()

guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'outbreak_prediction',
        'explanation': shap_values.tolist(),
        'confidence_score': 0.92,
        'evidence_chain': [
            'fever_cases_increased_30%',
            'water_quality_degraded',
            'population_density_high'
        ],
        'feature_importance': {
            'fever_cases': 0.45,
            'water_quality': 0.30,
            'population_density': 0.15,
            'rainfall': 0.10
        }
    },
    jurisdiction='EU_AI_ACT'
)
```

## Explainability methods

### TreeExplainer (XGBoost, Random Forest)

Fast and exact for tree-based models.

```python
import xgboost as xgb
import shap

# Train XGBoost model
model = xgb.XGBRegressor()
model.fit(X_train, y_train)

# Create explainer
explainer = shap.TreeExplainer(model)

# Generate explanations
shap_values = explainer.shap_values(X_test)

# Visualize
shap.summary_plot(shap_values, X_test)
```

### DeepExplainer (Neural Networks)

For TensorFlow/Keras models.

```python
import tensorflow as tf
import shap

# Load model
model = tf.keras.models.load_model('outbreak_predictor.h5')

# Create explainer
explainer = shap.DeepExplainer(model, X_train[:100])

# Generate explanations
shap_values = explainer.shap_values(X_test)

# Visualize
shap.force_plot(explainer.expected_value, shap_values[0], X_test[0])
```

### KernelExplainer (Black Box)

Model-agnostic explainer for any model.

```python
import shap

# Define prediction function
def predict_fn(X):
    return endpoint.predict(instances=X.tolist()).predictions

# Create explainer
explainer = shap.KernelExplainer(predict_fn, X_train)

# Generate explanations
shap_values = explainer.shap_values(X_test[0])
```

## Integration with iLuminara

### Outbreak prediction with explanation

```python
from edge_node.ai_agents import EpidemiologicalForecastingAgent
from governance_kernel.vector_ledger import SovereignGuardrail
import shap

# Initialize agent
agent = EpidemiologicalForecastingAgent(
    location="Dadaab",
    population_size=200000
)

# Train model with historical data
agent.train_model(historical_cases)

# Generate forecast with explanation
forecast = agent.forecast_outbreak(
    disease="cholera",
    forecast_horizon_days=14,
    explain=True  # Enable SHAP explanations
)

# Validate with SovereignGuardrail
guardrail = SovereignGuardrail()

guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'outbreak_forecast',
        'explanation': forecast.shap_values,
        'confidence_score': forecast.confidence,
        'evidence_chain': forecast.evidence,
        'feature_importance': forecast.feature_importance
    },
    jurisdiction='KDPA_KE'
)

print(f"Forecast: {forecast.peak_cases} cases")
print(f"Confidence: {forecast.confidence:.2%}")
print(f"Top features: {forecast.top_features}")
```

### Voice processing with explanation

```python
from edge_node.frenasa_engine.voice_processor import VoiceProcessor
from governance_kernel.vector_ledger import SovereignGuardrail

# Process voice alert
processor = VoiceProcessor()
result = processor.process_voice(
    audio_data=audio,
    explain=True  # Enable explanations
)

# Validate with SovereignGuardrail
guardrail = SovereignGuardrail()

guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'symptom_classification',
        'explanation': result.explanation,
        'confidence_score': result.confidence,
        'evidence_chain': result.symptoms,
        'feature_importance': result.feature_importance
    },
    jurisdiction='GDPR_EU'
)
```

## Explanation requirements

The SovereignGuardrail enforces minimum explanation requirements:

<AccordionGroup>
  <Accordion title="Confidence score">
    Probability or confidence level (0.0 - 1.0)
  </Accordion>
  <Accordion title="Evidence chain">
    List of factors that contributed to the decision
  </Accordion>
  <Accordion title="Feature importance">
    Quantitative contribution of each input feature
  </Accordion>
  <Accordion title="Decision rationale">
    Human-readable explanation of the reasoning
  </Accordion>
</AccordionGroup>

### Example explanation payload

```json
{
  "inference": "cholera_outbreak_prediction",
  "confidence_score": 0.92,
  "evidence_chain": [
    "fever_cases_increased_30%",
    "water_quality_degraded",
    "population_density_high",
    "rainfall_above_average"
  ],
  "feature_importance": {
    "fever_cases": 0.45,
    "water_quality": 0.30,
    "population_density": 0.15,
    "rainfall": 0.10
  },
  "shap_values": [0.23, 0.15, 0.08, 0.05],
  "decision_rationale": "High risk of cholera outbreak due to increased fever cases (30% above baseline) combined with degraded water quality in high-density population area."
}
```

## Visualization

### SHAP summary plot

```python
import shap
import matplotlib.pyplot as plt

# Generate SHAP values
shap_values = explainer.shap_values(X_test)

# Create summary plot
shap.summary_plot(shap_values, X_test, plot_type="bar")
plt.savefig("shap_summary.png")
```

### SHAP force plot

```python
# Create force plot for single prediction
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    X_test[0],
    matplotlib=True
)
plt.savefig("shap_force.png")
```

### SHAP waterfall plot

```python
# Create waterfall plot
shap.waterfall_plot(shap.Explanation(
    values=shap_values[0],
    base_values=explainer.expected_value,
    data=X_test[0],
    feature_names=feature_names
))
plt.savefig("shap_waterfall.png")
```

## Compliance validation

### High-risk threshold

The SovereignGuardrail enforces a high-risk threshold:

```yaml
# config/sovereign_guardrail.yaml
explainability:
  enabled: true
  high_risk_threshold: 0.7  # Confidence score
```

Any inference with confidence ≥ 0.7 requires explanation.

### Audit trail

All high-risk inferences are logged to the tamper-proof audit trail:

```python
# Audit entry
{
  "timestamp": "2025-12-19T20:00:00.000Z",
  "action": "High_Risk_Inference",
  "inference": "outbreak_prediction",
  "confidence_score": 0.92,
  "explanation_provided": true,
  "shap_values": [...],
  "feature_importance": {...},
  "jurisdiction": "KDPA_KE",
  "compliance_frameworks": ["EU_AI_ACT", "GDPR", "KDPA"]
}
```

## Performance considerations

- **TreeExplainer**: Fast (milliseconds), exact for tree models
- **DeepExplainer**: Moderate (seconds), approximate for neural networks
- **KernelExplainer**: Slow (minutes), model-agnostic

For production deployments:
- Pre-compute explanations for common scenarios
- Cache SHAP values for repeated queries
- Use TreeExplainer when possible for speed

## Testing

### Test SHAP integration

```python
import unittest
from governance_kernel.vector_ledger import SovereignGuardrail, SovereigntyViolationError

class TestExplainability(unittest.TestCase):
    def test_high_risk_inference_requires_explanation(self):
        guardrail = SovereignGuardrail()
        
        # Should fail without explanation
        with self.assertRaises(SovereigntyViolationError):
            guardrail.validate_action(
                action_type='High_Risk_Inference',
                payload={
                    'inference': 'diagnosis',
                    'confidence_score': 0.95
                    # Missing: explanation, evidence_chain, feature_importance
                },
                jurisdiction='EU_AI_ACT'
            )
        
        # Should pass with complete explanation
        guardrail.validate_action(
            action_type='High_Risk_Inference',
            payload={
                'inference': 'diagnosis',
                'confidence_score': 0.95,
                'explanation': [0.5, 0.3, 0.2],
                'evidence_chain': ['symptom_a', 'symptom_b'],
                'feature_importance': {'symptom_a': 0.6, 'symptom_b': 0.4}
            },
            jurisdiction='EU_AI_ACT'
        )
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy autonomous surveillance agents
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="Vertex AI deployment"
    icon="google"
    href="/deployment/gcp"
  >
    Deploy models to Google Cloud
  </Card>
  <Card
    title="Audit trail"
    icon="file-contract"
    href="/governance/audit"
  >
    Configure tamper-proof logging
  </Card>
</CardGroup>
