---
title: AI explainability
description: Vertex AI + SHAP integration for Right to Explanation compliance
---

## Overview

iLuminara-Core implements **Right to Explanation** for all high-risk AI inferences using SHAP (SHapley Additive exPlanations) integrated with Google Cloud Vertex AI.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  EU AI Act §6, GDPR Art. 22 - Every high-risk clinical inference requires explainability
</Card>

## Architecture

```
┌─────────────────────────────────────┐
│      VERTEX AI MODEL                │
│  - AutoML Time-Series               │
│  - Custom Training                  │
│  - Outbreak Forecasting             │
└─────────────────────────────────────┘
              │
              │ Prediction
              ▼
┌─────────────────────────────────────┐
│      SHAP EXPLAINER                 │
│  - Feature Importance               │
│  - SHAP Values                      │
│  - Decision Rationale               │
└─────────────────────────────────────┘
              │
              │ Explanation
              ▼
┌─────────────────────────────────────┐
│   SOVEREIGNGUARDRAIL                │
│  - Validate Explainability          │
│  - Enforce EU AI Act §6             │
│  - Audit Trail                      │
└─────────────────────────────────────┘
```

## High-risk AI systems

According to EU AI Act §6, these systems require explainability:

<CardGroup cols={2}>
  <Card title="Clinical diagnosis" icon="stethoscope">
    AI-assisted disease diagnosis and treatment recommendations
  </Card>
  <Card title="Outbreak prediction" icon="chart-line">
    Forecasting disease outbreaks and resource allocation
  </Card>
  <Card title="Risk stratification" icon="ranking-star">
    Patient risk scoring and triage decisions
  </Card>
  <Card title="Resource allocation" icon="hospital">
    AI-driven allocation of medical resources
  </Card>
</CardGroup>

## Vertex AI integration

### Model training with explainability

```python
from google.cloud import aiplatform
from google.cloud.aiplatform import explain

# Initialize Vertex AI
aiplatform.init(
    project="iluminara-project",
    location="africa-south1"
)

# Configure explainability
explanation_metadata = explain.ExplanationMetadata(
    inputs={
        "fever": {"input_tensor_name": "fever"},
        "cough": {"input_tensor_name": "cough"},
        "location": {"input_tensor_name": "location"},
        "age": {"input_tensor_name": "age"}
    },
    outputs={
        "diagnosis": {"output_tensor_name": "diagnosis"}
    }
)

explanation_parameters = explain.ExplanationParameters(
    sampled_shapley_attribution=explain.SampledShapleyAttribution(
        path_count=10
    )
)

# Train model with explainability
model = aiplatform.Model.upload(
    display_name="cholera-outbreak-predictor",
    artifact_uri="gs://iluminara-models/cholera-v1",
    serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-8:latest",
    explanation_metadata=explanation_metadata,
    explanation_parameters=explanation_parameters
)
```

### Making explainable predictions

```python
from google.cloud import aiplatform

# Get model
model = aiplatform.Model("projects/123/locations/africa-south1/models/456")

# Make prediction with explanation
prediction = model.predict(
    instances=[{
        "fever": 1,
        "cough": 1,
        "diarrhea": 1,
        "vomiting": 1,
        "location": "Dadaab",
        "age": 35
    }],
    parameters={"explain": True}
)

# Extract explanation
explanations = prediction.explanations[0]
attributions = explanations.attributions[0]

print(f"Prediction: {prediction.predictions[0]}")
print(f"Feature attributions:")
for feature, attribution in attributions.feature_attributions.items():
    print(f"  {feature}: {attribution}")
```

## SHAP integration

### Local SHAP explainer

For edge deployments without Vertex AI connectivity:

```python
import shap
import numpy as np
from sklearn.ensemble import RandomForestClassifier

# Train local model
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# Create SHAP explainer
explainer = shap.TreeExplainer(model)

# Generate SHAP values
shap_values = explainer.shap_values(X_test)

# Visualize
shap.summary_plot(shap_values, X_test, feature_names=feature_names)
```

### SHAP for deep learning models

```python
import shap
import tensorflow as tf

# Load model
model = tf.keras.models.load_model("outbreak_predictor.h5")

# Create deep explainer
explainer = shap.DeepExplainer(model, X_train[:100])

# Generate SHAP values
shap_values = explainer.shap_values(X_test)

# Visualize
shap.force_plot(
    explainer.expected_value[0],
    shap_values[0][0],
    X_test[0],
    feature_names=feature_names
)
```

## SovereignGuardrail enforcement

All high-risk inferences are validated for explainability:

```python
from governance_kernel.vector_ledger import SovereignGuardrail, SovereigntyViolationError

guardrail = SovereignGuardrail()

# Validate high-risk inference
try:
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'cholera_diagnosis',
            'confidence_score': 0.92,
            'explanation': {
                'method': 'SHAP',
                'feature_attributions': {
                    'fever': 0.35,
                    'diarrhea': 0.40,
                    'vomiting': 0.15,
                    'location': 0.10
                }
            },
            'evidence_chain': [
                'Patient reported fever (38.5°C)',
                'Watery diarrhea observed',
                'Vomiting reported',
                'Location: Dadaab (known outbreak zone)'
            ],
            'consent_token': 'VALID_CONSENT_TOKEN',
            'consent_scope': 'diagnosis'
        },
        jurisdiction='EU_AI_ACT'
    )
    print("✅ Inference approved - Explainability validated")
    
except SovereigntyViolationError as e:
    print(f"❌ Inference blocked: {e}")
```

## Explanation requirements

### Minimum requirements (EU AI Act §6)

<Steps>
  <Step title="Confidence score">
    Numerical confidence (0.0 - 1.0) for the prediction
  </Step>
  <Step title="Feature attributions">
    SHAP values or feature importance scores
  </Step>
  <Step title="Evidence chain">
    Human-readable rationale for the decision
  </Step>
  <Step title="Decision rationale">
    Why this prediction was made
  </Step>
</Steps>

### Example explanation output

```json
{
  "prediction": "cholera",
  "confidence_score": 0.92,
  "explanation": {
    "method": "SHAP",
    "feature_attributions": {
      "fever": 0.35,
      "diarrhea": 0.40,
      "vomiting": 0.15,
      "location": 0.10
    },
    "evidence_chain": [
      "Patient reported fever (38.5°C)",
      "Watery diarrhea observed",
      "Vomiting reported",
      "Location: Dadaab (known outbreak zone)"
    ],
    "decision_rationale": "High probability of cholera based on symptom cluster (fever + diarrhea + vomiting) and geographic risk factors. Dadaab region has active cholera surveillance alerts."
  },
  "compliance": {
    "framework": "EU_AI_ACT",
    "article": "§6 (High-Risk AI)",
    "validated": true,
    "timestamp": "2025-12-23T15:00:00Z"
  }
}
```

## Visualization

### SHAP summary plot

```python
import shap
import matplotlib.pyplot as plt

# Generate SHAP values
shap_values = explainer.shap_values(X_test)

# Create summary plot
shap.summary_plot(
    shap_values,
    X_test,
    feature_names=['fever', 'cough', 'diarrhea', 'vomiting', 'location', 'age'],
    plot_type="bar"
)

plt.title("Feature Importance for Cholera Diagnosis")
plt.tight_layout()
plt.savefig("shap_summary.png")
```

### SHAP force plot

```python
# Individual prediction explanation
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    X_test[0],
    feature_names=feature_names,
    matplotlib=True
)
```

## Edge deployment

For offline edge nodes, use local SHAP explainers:

```python
from edge_node.ai_agents import ExplainableAgent

agent = ExplainableAgent(
    model_path="models/cholera_predictor.pkl",
    explainer_type="SHAP"
)

# Make explainable prediction
result = agent.predict_with_explanation(
    features={
        'fever': 1,
        'diarrhea': 1,
        'vomiting': 1,
        'location': 'Dadaab'
    }
)

print(f"Prediction: {result['prediction']}")
print(f"Confidence: {result['confidence']}")
print(f"Explanation: {result['explanation']}")
```

## Audit trail

All explainable inferences are logged to the tamper-proof audit trail:

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail(enable_tamper_proof_audit=True)

# Audit entry automatically created
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'diagnosis',
        'explanation': shap_explanation,
        'confidence_score': 0.92
    },
    jurisdiction='EU_AI_ACT'
)

# Retrieve audit history
history = guardrail.get_tamper_proof_audit_history(
    action_type='High_Risk_Inference',
    limit=100
)
```

## Performance considerations

- **SHAP computation time**: ~100-500ms per prediction
- **Vertex AI latency**: ~200-800ms including explanation
- **Edge SHAP**: ~50-200ms (local computation)
- **Batch explanations**: Use for non-real-time scenarios

## Testing explainability

```python
import pytest
from governance_kernel.vector_ledger import SovereignGuardrail

def test_high_risk_inference_requires_explanation():
    guardrail = SovereignGuardrail()
    
    # Should fail without explanation
    with pytest.raises(SovereigntyViolationError):
        guardrail.validate_action(
            action_type='High_Risk_Inference',
            payload={
                'inference': 'diagnosis',
                'confidence_score': 0.92
                # Missing: explanation, evidence_chain
            },
            jurisdiction='EU_AI_ACT'
        )
    
    # Should pass with explanation
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'diagnosis',
            'confidence_score': 0.92,
            'explanation': {'method': 'SHAP', 'feature_attributions': {...}},
            'evidence_chain': ['...'],
            'consent_token': 'VALID',
            'consent_scope': 'diagnosis'
        },
        jurisdiction='EU_AI_ACT'
    )
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy explainable AI agents
  </Card>
  <Card
    title="Governance"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="Vertex AI setup"
    icon="google"
    href="/deployment/gcp"
  >
    Deploy to Google Cloud Platform
  </Card>
  <Card
    title="Testing"
    icon="flask"
    href="/ai-agents/testing"
  >
    Test explainability requirements
  </Card>
</CardGroup>
