---
title: AI explainability
description: Right to Explanation with SHAP and Vertex AI integration
---

## Overview

iLuminara-Core enforces the **Right to Explanation** for all high-risk AI inferences, complying with EU AI Act §6 and GDPR Art. 22. Every clinical decision requires explainability through SHAP (SHapley Additive exPlanations) values.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  EU AI Act §6 (High-Risk AI), GDPR Art. 22 (Right to Explanation)
</Card>

## Vertex AI integration

### Model deployment

Deploy explainable models to Vertex AI with automatic SHAP integration:

```python
from google.cloud import aiplatform
from edge_node.ai_agents import ExplainableModel

# Initialize Vertex AI
aiplatform.init(
    project="iluminara-core",
    location="africa-south1"
)

# Deploy model with explainability
model = ExplainableModel(
    model_name="cholera-risk-predictor",
    framework="tensorflow",
    enable_shap=True
)

# Deploy to Vertex AI
endpoint = model.deploy_to_vertex_ai(
    machine_type="n1-standard-4",
    min_replica_count=1,
    max_replica_count=10
)
```

### High-risk inference with explanation

```python
from edge_node.ai_agents import VertexAIPredictor
from governance_kernel.vector_ledger import SovereignGuardrail

predictor = VertexAIPredictor(endpoint_id="cholera-predictor-endpoint")
guardrail = SovereignGuardrail()

# Make prediction
prediction = predictor.predict(
    features={
        "fever": 1,
        "diarrhea": 1,
        "vomiting": 1,
        "dehydration": 1,
        "location_risk": 0.8
    }
)

# Validate with governance kernel
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'cholera_diagnosis',
        'confidence_score': prediction['confidence'],
        'explanation': prediction['shap_values'],
        'evidence_chain': prediction['feature_importance'],
        'consent_token': 'VALID_TOKEN',
        'consent_scope': 'diagnosis'
    },
    jurisdiction='EU_AI_ACT'
)

print(f"Prediction: {prediction['diagnosis']}")
print(f"Confidence: {prediction['confidence']:.2%}")
print(f"SHAP values: {prediction['shap_values']}")
```

## SHAP explainability

### Feature importance

SHAP values show how each feature contributes to the prediction:

```python
import shap
import numpy as np

# Load model
model = load_model("cholera_risk_model.h5")

# Create SHAP explainer
explainer = shap.DeepExplainer(model, background_data)

# Calculate SHAP values
shap_values = explainer.shap_values(patient_features)

# Visualize
shap.summary_plot(shap_values, patient_features, feature_names=[
    "fever", "diarrhea", "vomiting", "dehydration", "location_risk"
])
```

### Example output

```
Feature Contributions (SHAP values):
  diarrhea:      +0.35  (Strong positive)
  vomiting:      +0.28  (Moderate positive)
  dehydration:   +0.22  (Moderate positive)
  location_risk: +0.15  (Weak positive)
  fever:         +0.05  (Minimal positive)

Base value: 0.10
Prediction: 1.05 → 74% probability of cholera
```

## Compliance enforcement

### Automatic validation

The SovereignGuardrail automatically validates all high-risk inferences:

```python
from governance_kernel.vector_ledger import SovereignGuardrail, SovereigntyViolationError

guardrail = SovereignGuardrail()

try:
    # Attempt inference without explanation
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'diagnosis',
            'confidence_score': 0.95,
            # Missing: explanation, evidence_chain
        },
        jurisdiction='EU_AI_ACT'
    )
except SovereigntyViolationError as e:
    print(f"❌ {e}")
    # Output: "Violates EU AI Act §6 (High-Risk AI) - Missing explanation"
```

### Explanation requirements

| Requirement | Description | Compliance |
|-------------|-------------|------------|
| **SHAP values** | Feature contribution scores | EU AI Act §6 |
| **Confidence score** | Model certainty (0-1) | GDPR Art. 22 |
| **Evidence chain** | Supporting data points | EU AI Act §8 |
| **Feature importance** | Ranked feature list | GDPR Art. 22 |
| **Decision rationale** | Human-readable explanation | EU AI Act §6 |

## Vertex AI AutoML integration

### Train explainable model

```python
from google.cloud import aiplatform

# Create AutoML training job
job = aiplatform.AutoMLTabularTrainingJob(
    display_name="cholera-risk-automl",
    optimization_prediction_type="classification",
    optimization_objective="maximize-au-prc"
)

# Train with explainability
model = job.run(
    dataset=dataset,
    target_column="cholera_positive",
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
    model_display_name="cholera-risk-model",
    # Enable explainability
    export_evaluated_data_items=True,
    export_evaluated_data_items_bigquery_destination_uri=f"bq://{project_id}.predictions.evaluated_data"
)
```

### Batch prediction with explanations

```python
# Batch prediction job
batch_prediction_job = model.batch_predict(
    job_display_name="cholera-risk-batch-prediction",
    gcs_source="gs://iluminara-data/patient-features.csv",
    gcs_destination_prefix="gs://iluminara-predictions/",
    machine_type="n1-standard-4",
    # Request explanations
    generate_explanation=True,
    explanation_metadata=aiplatform.explain.ExplanationMetadata(
        inputs={
            "fever": {},
            "diarrhea": {},
            "vomiting": {},
            "dehydration": {},
            "location_risk": {}
        },
        outputs={"cholera_risk": {}}
    )
)
```

## Local SHAP analysis

For edge deployments without Vertex AI connectivity:

```python
from edge_node.ai_agents import LocalExplainer

# Load local model
explainer = LocalExplainer(model_path="models/cholera_risk.h5")

# Generate explanation
explanation = explainer.explain(
    features={
        "fever": 1,
        "diarrhea": 1,
        "vomiting": 1,
        "dehydration": 1,
        "location_risk": 0.8
    },
    method="shap"  # or "lime", "feature_importance"
)

print(f"Prediction: {explanation['prediction']}")
print(f"SHAP values: {explanation['shap_values']}")
print(f"Feature importance: {explanation['feature_importance']}")
```

## Visualization

### SHAP waterfall plot

```python
import shap
import matplotlib.pyplot as plt

# Create waterfall plot
shap.plots.waterfall(shap_values[0])
plt.savefig("shap_waterfall.png")
```

### SHAP force plot

```python
# Create force plot
shap.plots.force(
    base_value=explainer.expected_value,
    shap_values=shap_values[0],
    features=patient_features,
    matplotlib=True
)
plt.savefig("shap_force.png")
```

## Audit trail integration

All explanations are logged to the tamper-proof audit trail:

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail(enable_tamper_proof_audit=True)

# Validate and log
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'actor': 'ml_system',
        'resource': 'patient_diagnosis',
        'explanation': shap_values.tolist(),
        'confidence_score': 0.95,
        'evidence_chain': ['fever', 'diarrhea', 'vomiting'],
        'consent_token': 'valid_token',
        'consent_scope': 'diagnosis'
    },
    jurisdiction='EU_AI_ACT'
)

# Retrieve audit history
history = guardrail.get_tamper_proof_audit_history(limit=10)
```

## Performance considerations

- **SHAP calculation time**: ~100-500ms per prediction
- **Batch processing**: Use Vertex AI for large-scale explanations
- **Edge deployment**: Pre-compute SHAP values for common scenarios
- **Caching**: Cache explanations for identical feature sets

## Next steps

<CardGroup cols={2}>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy autonomous surveillance agents
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="Vertex AI deployment"
    icon="google"
    href="/deployment/gcp"
  >
    Deploy models to Google Cloud
  </Card>
  <Card
    title="Federated learning"
    icon="lock"
    href="/ai-agents/federated-learning"
  >
    Privacy-preserving collaborative training
  </Card>
</CardGroup>
