---
title: Vertex AI + SHAP integration
description: Right to Explanation with explainable AI for high-risk clinical inferences
---

## Overview

iLuminara integrates **Google Cloud Vertex AI** with **SHAP (SHapley Additive exPlanations)** to provide the "Right to Explanation" required by EU AI Act §6, GDPR Art. 22, and other global frameworks.

Every high-risk clinical inference automatically triggers explainability analysis, ensuring compliance and building trust.

<Card
  title="Philosophy"
  icon="lightbulb"
>
  "Every high-risk clinical inference requires explainability. No black boxes in life-or-death decisions."
</Card>

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    VERTEX AI PIPELINE                        │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  1. Model Training (AutoML / Custom)                 │  │
│  │     - Time-series forecasting                        │  │
│  │     - Disease classification                         │  │
│  │     - Outbreak prediction                            │  │
│  └──────────────────────────────────────────────────────┘  │
│                            ▼                                 │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  2. Model Deployment (Endpoint)                      │  │
│  │     - Real-time prediction                           │  │
│  │     - Batch prediction                               │  │
│  │     - Online serving                                 │  │
│  └──────────────────────────────────────────────────────┘  │
│                            ▼                                 │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  3. Prediction Request                               │  │
│  │     - Input: Patient symptoms, location, history     │  │
│  │     - Output: Diagnosis, confidence score            │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
                            ▼
┌─────────────────────────────────────────────────────────────┐
│              EXPLAINABILITY LAYER (SHAP)                     │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  4. High-Risk Detection                              │  │
│  │     - IF confidence_score > 0.7 THEN trigger SHAP    │  │
│  └──────────────────────────────────────────────────────┘  │
│                            ▼                                 │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  5. SHAP Analysis                                    │  │
│  │     - TreeExplainer (for tree models)                │  │
│  │     - KernelExplainer (for any model)                │  │
│  │     - DeepExplainer (for neural networks)            │  │
│  └──────────────────────────────────────────────────────┘  │
│                            ▼                                 │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  6. Explanation Generation                           │  │
│  │     - Feature importance                             │  │
│  │     - SHAP values                                    │  │
│  │     - Waterfall plot                                 │  │
│  │     - Force plot                                     │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
                            ▼
┌─────────────────────────────────────────────────────────────┐
│              GOVERNANCE VALIDATION                           │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  7. SovereignGuardrail Check                         │  │
│  │     - Validate explainability requirements           │  │
│  │     - Log to tamper-proof audit trail                │  │
│  │     - Store explanation artifacts                    │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

## High-risk AI classification

Per EU AI Act §6, the following systems are classified as **high-risk**:

<AccordionGroup>
  <Accordion title="Medical device AI">
    - Diagnostic systems (e.g., cholera detection)
    - Treatment recommendation systems
    - Triage prioritization systems
  </Accordion>
  <Accordion title="Critical infrastructure AI">
    - Outbreak prediction models
    - Resource allocation algorithms
    - Emergency response coordination
  </Accordion>
  <Accordion title="Biometric identification">
    - Patient identification systems
    - Voice-based symptom detection
  </Accordion>
</AccordionGroup>

## Implementation

### Step 1: Train model on Vertex AI

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project="iluminara-core",
    location="africa-south1"
)

# Create AutoML training job
dataset = aiplatform.TabularDataset.create(
    display_name="cholera_outbreak_dataset",
    gcs_source="gs://iluminara-data/cholera_training.csv"
)

job = aiplatform.AutoMLTabularTrainingJob(
    display_name="cholera_prediction_model",
    optimization_prediction_type="classification",
    optimization_objective="maximize-au-prc"
)

model = job.run(
    dataset=dataset,
    target_column="cholera_positive",
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
    budget_milli_node_hours=1000,
    model_display_name="cholera_classifier_v1"
)
```

### Step 2: Deploy model to endpoint

```python
# Deploy model
endpoint = model.deploy(
    machine_type="n1-standard-4",
    min_replica_count=1,
    max_replica_count=10,
    accelerator_type="NVIDIA_TESLA_T4",
    accelerator_count=1
)

print(f"Model deployed to endpoint: {endpoint.resource_name}")
```

### Step 3: Make prediction with explainability

```python
from google.cloud import aiplatform
from governance_kernel.vector_ledger import SovereignGuardrail
import shap
import numpy as np

class ExplainableVertexAI:
    """
    Wrapper for Vertex AI predictions with automatic SHAP explainability.
    """
    
    def __init__(
        self,
        endpoint_id: str,
        project: str,
        location: str,
        high_risk_threshold: float = 0.7
    ):
        self.endpoint = aiplatform.Endpoint(endpoint_id)
        self.high_risk_threshold = high_risk_threshold
        self.guardrail = SovereignGuardrail(enable_tamper_proof_audit=True)
    
    def predict_with_explanation(
        self,
        instances: list,
        feature_names: list,
        background_data: np.ndarray = None
    ):
        """
        Make prediction and generate SHAP explanation if high-risk.
        """
        # Make prediction
        prediction = self.endpoint.predict(instances=instances)
        
        # Extract confidence score
        confidence_score = max(prediction.predictions[0])
        predicted_class = np.argmax(prediction.predictions[0])
        
        # Check if high-risk
        is_high_risk = confidence_score >= self.high_risk_threshold
        
        explanation = None
        if is_high_risk:
            # Generate SHAP explanation
            explanation = self._generate_shap_explanation(
                instances=instances,
                feature_names=feature_names,
                background_data=background_data
            )
            
            # Validate with SovereignGuardrail
            self.guardrail.validate_action(
                action_type='High_Risk_Inference',
                payload={
                    'actor': 'vertex_ai_model',
                    'resource': 'patient_diagnosis',
                    'explanation': explanation['summary'],
                    'confidence_score': confidence_score,
                    'evidence_chain': explanation['top_features'],
                    'consent_token': 'valid_token',
                    'consent_scope': 'diagnosis'
                },
                jurisdiction='EU_AI_ACT'
            )
        
        return {
            'prediction': predicted_class,
            'confidence_score': confidence_score,
            'is_high_risk': is_high_risk,
            'explanation': explanation,
            'compliance_validated': is_high_risk
        }
    
    def _generate_shap_explanation(
        self,
        instances: list,
        feature_names: list,
        background_data: np.ndarray
    ):
        """
        Generate SHAP explanation for prediction.
        """
        # Create explainer
        def model_predict(X):
            predictions = []
            for instance in X:
                pred = self.endpoint.predict(instances=[instance.tolist()])
                predictions.append(pred.predictions[0])
            return np.array(predictions)
        
        # Use KernelExplainer for model-agnostic explanations
        explainer = shap.KernelExplainer(
            model_predict,
            background_data
        )
        
        # Calculate SHAP values
        shap_values = explainer.shap_values(np.array(instances))
        
        # Get feature importance
        feature_importance = np.abs(shap_values[0]).mean(axis=0)
        top_features_idx = np.argsort(feature_importance)[-5:][::-1]
        
        top_features = [
            {
                'feature': feature_names[idx],
                'importance': float(feature_importance[idx]),
                'shap_value': float(shap_values[0][0][idx])
            }
            for idx in top_features_idx
        ]
        
        return {
            'shap_values': shap_values[0].tolist(),
            'feature_names': feature_names,
            'top_features': top_features,
            'summary': f"Top contributing features: {', '.join([f['feature'] for f in top_features])}"
        }

# Usage
explainer = ExplainableVertexAI(
    endpoint_id="projects/123/locations/africa-south1/endpoints/456",
    project="iluminara-core",
    location="africa-south1",
    high_risk_threshold=0.7
)

# Patient data
patient_instance = [{
    'fever': 1,
    'diarrhea': 1,
    'vomiting': 1,
    'dehydration': 1,
    'age': 35,
    'location_risk_score': 0.8
}]

feature_names = ['fever', 'diarrhea', 'vomiting', 'dehydration', 'age', 'location_risk_score']

# Background data for SHAP (sample of training data)
background_data = np.random.rand(100, 6)

# Make prediction with explanation
result = explainer.predict_with_explanation(
    instances=patient_instance,
    feature_names=feature_names,
    background_data=background_data
)

print(f"Prediction: {result['prediction']}")
print(f"Confidence: {result['confidence_score']:.2%}")
print(f"High-risk: {result['is_high_risk']}")

if result['explanation']:
    print("\\nTop contributing features:")
    for feature in result['explanation']['top_features']:
        print(f"  - {feature['feature']}: {feature['importance']:.3f}")
```

## SHAP visualization

### Waterfall plot

```python
import shap
import matplotlib.pyplot as plt

# Generate waterfall plot
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values[0][0],
        base_values=explainer.expected_value[0],
        data=patient_instance[0],
        feature_names=feature_names
    )
)
plt.savefig('shap_waterfall.png')
```

### Force plot

```python
# Generate force plot
shap.force_plot(
    explainer.expected_value[0],
    shap_values[0][0],
    patient_instance[0],
    feature_names=feature_names,
    matplotlib=True
)
plt.savefig('shap_force.png')
```

### Summary plot

```python
# Generate summary plot for multiple predictions
shap.summary_plot(
    shap_values[0],
    patient_instances,
    feature_names=feature_names
)
plt.savefig('shap_summary.png')
```

## Compliance validation

The explainability layer automatically validates compliance with:

<Steps>
  <Step title="EU AI Act §6">
    High-risk AI systems require transparency and explainability
  </Step>
  <Step title="GDPR Art. 22">
    Right to explanation for automated decision-making
  </Step>
  <Step title="EU AI Act §12">
    Record-keeping of high-risk AI decisions
  </Step>
  <Step title="EU AI Act §14">
    Human oversight of high-risk AI systems
  </Step>
</Steps>

## Audit trail

All high-risk inferences are logged to the tamper-proof audit trail:

```python
# Retrieve audit history
history = guardrail.get_tamper_proof_audit_history(
    action_type='High_Risk_Inference',
    limit=100
)

for entry in history:
    print(f"Timestamp: {entry['timestamp']}")
    print(f"Model: {entry['payload']['actor']}")
    print(f"Confidence: {entry['payload']['confidence_score']}")
    print(f"Explanation: {entry['payload']['explanation']}")
    print(f"Compliance: {entry['jurisdiction']}")
    print("---")
```

## Performance optimization

### Batch prediction with SHAP

```python
# For high-throughput scenarios, use batch prediction
batch_prediction_job = aiplatform.BatchPredictionJob.create(
    job_display_name="cholera_batch_prediction",
    model_name=model.resource_name,
    instances_format="csv",
    gcs_source="gs://iluminara-data/batch_input.csv",
    gcs_destination_prefix="gs://iluminara-data/batch_output/",
    machine_type="n1-standard-4",
    accelerator_type="NVIDIA_TESLA_T4",
    accelerator_count=1
)

# Wait for completion
batch_prediction_job.wait()

# Generate SHAP explanations for high-risk predictions
# (post-processing step)
```

### Caching SHAP explainers

```python
import pickle

# Cache explainer for reuse
with open('shap_explainer.pkl', 'wb') as f:
    pickle.dump(explainer, f)

# Load cached explainer
with open('shap_explainer.pkl', 'rb') as f:
    explainer = pickle.load(f)
```

## Integration with dashboards

Display SHAP explanations in Streamlit dashboards:

```python
import streamlit as st
import shap

st.title("Explainable AI - Cholera Prediction")

# Display prediction
st.metric("Prediction", "Cholera Positive", delta=f"{confidence_score:.1%} confidence")

# Display SHAP waterfall
st.subheader("Feature Importance")
fig = shap.waterfall_plot(shap_explanation, show=False)
st.pyplot(fig)

# Display top features
st.subheader("Top Contributing Features")
for feature in top_features:
    st.write(f"**{feature['feature']}**: {feature['importance']:.3f}")
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy autonomous surveillance agents
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Understand compliance enforcement
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/gcp"
  >
    Deploy to Google Cloud Platform
  </Card>
  <Card
    title="Bio-Interface API"
    icon="heartbeat"
    href="/integrations/bio-interface"
  >
    Mobile health app integration
  </Card>
</CardGroup>
