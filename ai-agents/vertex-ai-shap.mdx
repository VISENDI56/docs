---
title: Vertex AI + SHAP integration
description: Right to Explanation with explainable AI for high-risk clinical inferences
---

## Overview

iLuminara integrates **Google Cloud Vertex AI** with **SHAP (SHapley Additive exPlanations)** to provide the "Right to Explanation" required by EU AI Act Â§6 and GDPR Art. 22.

Every high-risk clinical inference automatically triggers explainability analysis, ensuring compliance with global AI governance frameworks.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  EU AI Act Â§6 â€¢ GDPR Art. 22 â€¢ NIST AI RMF â€¢ UNESCO AI Ethics
</Card>

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  VERTEX AI PIPELINE                     â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  AutoML      â”‚      â”‚  Custom      â”‚               â”‚
â”‚  â”‚  Time-Series â”‚      â”‚  Models      â”‚               â”‚
â”‚  â”‚  Forecasting â”‚      â”‚  (TensorFlow)â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚         â”‚                     â”‚                        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                    â”‚                                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚         â”‚  Prediction API     â”‚                        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SHAP EXPLAINABILITY LAYER                  â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  IF confidence_score > 0.7 (HIGH RISK)           â”‚  â”‚
â”‚  â”‚  THEN generate_explanation()                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ SHAP Values  â”‚  â”‚ Feature      â”‚  â”‚ Counterfact.â”‚  â”‚
â”‚  â”‚ Calculation  â”‚  â”‚ Importance   â”‚  â”‚ Examples    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SOVEREIGNGUARDRAIL VALIDATION                 â”‚
â”‚                                                         â”‚
â”‚  âœ“ Explanation completeness check                      â”‚
â”‚  âœ“ Evidence chain validation                           â”‚
â”‚  âœ“ Audit trail logging                                 â”‚
â”‚  âœ“ Compliance attestation                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## High-risk AI classification

According to EU AI Act Â§6, the following are classified as **high-risk AI systems**:

<AccordionGroup>
  <Accordion title="Clinical diagnosis">
    AI systems that diagnose diseases or recommend treatments
  </Accordion>
  <Accordion title="Outbreak prediction">
    AI systems that predict disease outbreaks affecting public health decisions
  </Accordion>
  <Accordion title="Resource allocation">
    AI systems that allocate scarce medical resources (beds, ventilators, vaccines)
  </Accordion>
  <Accordion title="Triage decisions">
    AI systems that prioritize patient care in emergency situations
  </Accordion>
</AccordionGroup>

## Implementation

### Step 1: Deploy model to Vertex AI

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project="iluminara-core",
    location="africa-south1",
    staging_bucket="gs://iluminara-models"
)

# Deploy AutoML time-series model
model = aiplatform.Model.upload(
    display_name="cholera-outbreak-forecaster",
    artifact_uri="gs://iluminara-models/cholera-v1",
    serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-11:latest"
)

endpoint = model.deploy(
    deployed_model_display_name="cholera-forecaster-v1",
    machine_type="n1-standard-4",
    min_replica_count=1,
    max_replica_count=10,
    traffic_percentage=100
)

print(f"âœ… Model deployed to endpoint: {endpoint.resource_name}")
```

### Step 2: Make prediction with explainability

```python
from google.cloud import aiplatform
from governance_kernel.vector_ledger import SovereignGuardrail
import shap
import numpy as np

# Initialize
endpoint = aiplatform.Endpoint("projects/123/locations/africa-south1/endpoints/456")
guardrail = SovereignGuardrail(enable_tamper_proof_audit=True)

# Prepare input data
instances = [{
    "location": "Dadaab",
    "population": 200000,
    "water_quality_index": 0.45,
    "sanitation_coverage": 0.60,
    "recent_cases": [12, 15, 18, 22, 28],
    "rainfall_mm": 45.2,
    "temperature_c": 32.5
}]

# Get prediction
prediction = endpoint.predict(instances=instances)
confidence_score = prediction.predictions[0]["confidence"]
forecast_cases = prediction.predictions[0]["forecast"]

print(f"Forecast: {forecast_cases} cases")
print(f"Confidence: {confidence_score:.2%}")

# Check if high-risk (requires explanation)
if confidence_score > 0.7:
    print("ğŸš¨ HIGH-RISK INFERENCE - Generating explanation...")
    
    # Generate SHAP explanation
    explainer = shap.KernelExplainer(
        model=lambda x: endpoint.predict(instances=x).predictions,
        data=shap.sample(instances, 100)
    )
    
    shap_values = explainer.shap_values(instances)
    
    # Extract feature importance
    feature_importance = {
        "recent_cases": float(shap_values[0][4]),
        "water_quality_index": float(shap_values[0][2]),
        "sanitation_coverage": float(shap_values[0][3]),
        "rainfall_mm": float(shap_values[0][5]),
        "temperature_c": float(shap_values[0][6])
    }
    
    # Sort by absolute importance
    sorted_features = sorted(
        feature_importance.items(),
        key=lambda x: abs(x[1]),
        reverse=True
    )
    
    print("\\nğŸ“Š Feature Importance (SHAP values):")
    for feature, value in sorted_features:
        print(f"   {feature}: {value:+.4f}")
    
    # Validate with SovereignGuardrail
    try:
        guardrail.validate_action(
            action_type='High_Risk_Inference',
            payload={
                'actor': 'vertex_ai_forecaster',
                'resource': 'cholera_outbreak_prediction',
                'explanation': f"SHAP values: {feature_importance}",
                'confidence_score': confidence_score,
                'evidence_chain': [
                    f"Recent cases trending upward: {instances[0]['recent_cases']}",
                    f"Water quality degraded: {instances[0]['water_quality_index']}",
                    f"Rainfall increased: {instances[0]['rainfall_mm']}mm"
                ],
                'consent_token': 'public_health_surveillance',
                'consent_scope': 'outbreak_prediction'
            },
            jurisdiction='EU_AI_ACT'
        )
        
        print("\\nâœ… Explanation validated - Compliant with EU AI Act Â§6")
        
    except Exception as e:
        print(f"\\nâŒ Compliance violation: {e}")
```

### Step 3: Visualize SHAP explanations

```python
import shap
import matplotlib.pyplot as plt

# Generate SHAP waterfall plot
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values[0],
        base_values=explainer.expected_value,
        data=instances[0],
        feature_names=list(instances[0].keys())
    )
)

plt.title("SHAP Explanation: Cholera Outbreak Forecast")
plt.tight_layout()
plt.savefig("shap_explanation.png", dpi=300, bbox_inches='tight')
print("âœ… SHAP visualization saved to shap_explanation.png")
```

## Explanation requirements

According to EU AI Act Â§13 and GDPR Art. 22, explanations must include:

<Steps>
  <Step title="Confidence score">
    Probability or confidence level of the prediction
  </Step>
  <Step title="Feature contributions">
    SHAP values showing how each input feature influenced the prediction
  </Step>
  <Step title="Evidence chain">
    Human-readable explanation of the reasoning process
  </Step>
  <Step title="Counterfactual examples">
    "What would need to change for a different prediction?"
  </Step>
  <Step title="Model lineage">
    Model version, training data, and deployment metadata
  </Step>
</Steps>

## Automated explainability pipeline

```python
from google.cloud import aiplatform
from governance_kernel.explainability import ExplainabilityEngine

class VertexAIExplainablePredictor:
    def __init__(self, endpoint_id: str, project: str, location: str):
        self.endpoint = aiplatform.Endpoint(
            f"projects/{project}/locations/{location}/endpoints/{endpoint_id}"
        )
        self.explainer = ExplainabilityEngine()
        self.guardrail = SovereignGuardrail(enable_tamper_proof_audit=True)
    
    def predict_with_explanation(self, instances: list) -> dict:
        """
        Make prediction and automatically generate explanation if high-risk.
        """
        # Get prediction
        prediction = self.endpoint.predict(instances=instances)
        confidence = prediction.predictions[0].get("confidence", 0.0)
        
        result = {
            "prediction": prediction.predictions[0],
            "confidence": confidence,
            "high_risk": confidence > 0.7,
            "explanation": None,
            "compliance_status": "PENDING"
        }
        
        # Generate explanation if high-risk
        if result["high_risk"]:
            explanation = self.explainer.generate_shap_explanation(
                model=self.endpoint,
                instances=instances
            )
            
            result["explanation"] = {
                "shap_values": explanation["shap_values"],
                "feature_importance": explanation["feature_importance"],
                "evidence_chain": explanation["evidence_chain"],
                "counterfactuals": explanation["counterfactuals"]
            }
            
            # Validate with SovereignGuardrail
            try:
                self.guardrail.validate_action(
                    action_type='High_Risk_Inference',
                    payload={
                        'actor': 'vertex_ai_model',
                        'resource': 'prediction',
                        'explanation': str(explanation["shap_values"]),
                        'confidence_score': confidence,
                        'evidence_chain': explanation["evidence_chain"],
                        'consent_token': 'valid_token',
                        'consent_scope': 'prediction'
                    },
                    jurisdiction='EU_AI_ACT'
                )
                
                result["compliance_status"] = "COMPLIANT"
                
            except Exception as e:
                result["compliance_status"] = "VIOLATION"
                result["compliance_error"] = str(e)
        
        else:
            result["compliance_status"] = "LOW_RISK_NO_EXPLANATION_REQUIRED"
        
        return result

# Usage
predictor = VertexAIExplainablePredictor(
    endpoint_id="1234567890",
    project="iluminara-core",
    location="africa-south1"
)

result = predictor.predict_with_explanation(instances=[{
    "location": "Nairobi",
    "symptoms": ["fever", "cough", "fatigue"],
    "vital_signs": {"temp": 38.5, "hr": 95, "bp": "120/80"}
}])

print(f"Prediction: {result['prediction']}")
print(f"High-risk: {result['high_risk']}")
print(f"Compliance: {result['compliance_status']}")

if result['explanation']:
    print(f"\\nTop 3 contributing features:")
    for feature, value in list(result['explanation']['feature_importance'].items())[:3]:
        print(f"  {feature}: {value:+.4f}")
```

## Model cards for transparency

Every Vertex AI model deployed in iLuminara includes a **Model Card** for transparency:

```yaml
model_card:
  model_details:
    name: "Cholera Outbreak Forecaster"
    version: "v1.2.0"
    type: "AutoML Time-Series"
    owner: "iLuminara Health Intelligence"
    license: "Proprietary"
  
  intended_use:
    primary_uses:
      - "72-hour cholera outbreak forecasting"
      - "Resource allocation planning"
      - "Early warning system triggers"
    
    out_of_scope:
      - "Individual patient diagnosis"
      - "Treatment recommendations"
      - "Vaccine allocation decisions"
  
  factors:
    relevant_factors:
      - "Population density"
      - "Water quality"
      - "Sanitation coverage"
      - "Recent case trends"
      - "Environmental factors (rainfall, temperature)"
    
    evaluation_factors:
      - "Geographic region (East Africa)"
      - "Population type (refugee camps, urban, rural)"
      - "Data quality (CBS vs EMR)"
  
  metrics:
    performance_measures:
      - "MAE (Mean Absolute Error): 12.3 cases"
      - "RMSE (Root Mean Squared Error): 18.7 cases"
      - "RÂ² Score: 0.87"
      - "95% Confidence Interval: Â±22 cases"
    
    decision_thresholds:
      - "High-risk: confidence > 0.7"
      - "Alert trigger: forecast > 50 cases"
  
  training_data:
    dataset: "iLuminara Historical Outbreak Database"
    size: "2.3M records (2018-2024)"
    sources:
      - "CBS (Community-Based Surveillance)"
      - "EMR (Electronic Medical Records)"
      - "IDSR (Integrated Disease Surveillance Response)"
    
    preprocessing:
      - "Golden Thread data fusion"
      - "Outlier removal (Z-score > 3)"
      - "Missing value imputation (forward fill)"
  
  ethical_considerations:
    risks:
      - "False positives may cause unnecessary panic"
      - "False negatives may delay outbreak response"
      - "Model bias toward urban areas with better data"
    
    mitigations:
      - "Human-in-the-loop validation required"
      - "Confidence thresholds calibrated per region"
      - "Regular model retraining with new data"
  
  caveats_and_recommendations:
    - "Model trained on East African data - may not generalize globally"
    - "Requires minimum 30 days of historical data for accurate forecasting"
    - "Performance degrades during novel outbreak scenarios"
    - "Should be used as decision support, not autonomous decision-making"
```

## Compliance attestation

Every prediction generates a compliance attestation:

```json
{
  "prediction_id": "pred_20250128_001",
  "timestamp": "2025-01-28T14:30:00Z",
  "model": {
    "name": "cholera-outbreak-forecaster",
    "version": "v1.2.0",
    "endpoint": "projects/iluminara/endpoints/123"
  },
  "prediction": {
    "forecast_cases": 87,
    "confidence": 0.92,
    "risk_level": "HIGH"
  },
  "explainability": {
    "method": "SHAP",
    "shap_values": {
      "recent_cases": 0.45,
      "water_quality": 0.32,
      "rainfall": 0.18,
      "temperature": 0.05
    },
    "evidence_chain": [
      "Recent cases increased 40% week-over-week",
      "Water quality index dropped to 0.45 (critical threshold: 0.50)",
      "Heavy rainfall (45mm) increases transmission risk"
    ]
  },
  "compliance": {
    "frameworks_checked": [
      "EU_AI_ACT",
      "GDPR_ART_22",
      "NIST_AI_RMF",
      "UNESCO_AI_ETHICS"
    ],
    "status": "COMPLIANT",
    "attestation_signature": "SHA256:a3f2b9c8...",
    "audit_trail_id": "audit_20250128_001"
  }
}
```

## Next steps

<CardGroup cols={2}>\n  <Card\n    title=\"AI agents\"\n    icon=\"brain-circuit\"\n    href=\"/ai-agents/overview\"\n  >\n    Deploy autonomous surveillance agents\n  </Card>\n  <Card\n    title=\"Governance kernel\"\n    icon=\"shield-check\"\n    href=\"/governance/overview\"\n  >\n    Understand compliance enforcement\n  </Card>\n  <Card\n    title=\"Deployment\"\n    icon=\"rocket\"\n    href=\"/deployment/gcp\"\n  >\n    Deploy Vertex AI models to production\n  </Card>\n  <Card\n    title=\"API reference\"\n    icon=\"terminal\"\n    href=\"/api-reference/overview\"\n  >\n    Integrate with REST API\n  </Card>\n</CardGroup>
