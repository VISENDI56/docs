---
title: Vertex AI + SHAP integration
description: Right to Explanation with explainable AI for high-risk clinical inferences
---

## Overview

iLuminara-Core integrates Google Cloud Vertex AI with SHAP (SHapley Additive exPlanations) to provide **Right to Explanation** for every high-risk clinical inference, ensuring compliance with EU AI Act §6 and GDPR Art. 22.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk AI decision requires explainability with SHAP values, feature importance, and evidence chains.
</Card>

## Architecture

```
┌─────────────────────────────────────────────────────────┐
│                  VERTEX AI PIPELINE                     │
│  ┌──────────────────────────────────────────────────┐  │
│  │  AutoML Time-Series Forecasting                  │  │
│  │  - 72-hour outbreak predictions                  │  │
│  │  - Hierarchical spatial forecasting              │  │
│  │  - 95% confidence intervals                      │  │
│  └──────────────────────────────────────────────────┘  │
│  ┌──────────────────────────────────────────────────┐  │
│  │  Custom Model Training                           │  │
│  │  - Disease classification                        │  │
│  │  - Risk stratification                           │  │
│  │  - Outbreak detection                            │  │
│  └──────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────┘
                        ▼
┌─────────────────────────────────────────────────────────┐
│                  SHAP EXPLAINER                         │
│  ┌──────────────────────────────────────────────────┐  │
│  │  TreeExplainer (for tree-based models)           │  │
│  │  KernelExplainer (for any model)                 │  │
│  │  DeepExplainer (for neural networks)             │  │
│  └──────────────────────────────────────────────────┘  │
│  ┌──────────────────────────────────────────────────┐  │
│  │  Explanation Output                              │  │
│  │  - SHAP values per feature                       │  │
│  │  - Feature importance ranking                    │  │
│  │  - Waterfall plots                               │  │
│  │  - Force plots                                   │  │
│  └──────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────┘
                        ▼
┌─────────────────────────────────────────────────────────┐
│              SOVEREIGNGUARDRAIL VALIDATION              │
│  - Validates explanation completeness                   │
│  - Enforces EU AI Act §6 requirements                   │
│  - Logs to tamper-proof audit trail                    │
└─────────────────────────────────────────────────────────┘
```

## Setup

### Install dependencies

```bash
pip install google-cloud-aiplatform shap pandas numpy scikit-learn
```

### Configure Vertex AI

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project="your-project-id",
    location="us-central1",
    staging_bucket="gs://your-bucket"
)
```

## Training with explainability

### AutoML time-series forecasting

```python
from google.cloud import aiplatform

# Create dataset
dataset = aiplatform.TimeSeriesDataset.create(
    display_name="cholera_outbreak_forecast",
    gcs_source="gs://your-bucket/outbreak_data.csv",
    time_column="timestamp",
    time_series_identifier_column="location",
    target_column="case_count"
)

# Train AutoML model
job = aiplatform.AutoMLForecastingTrainingJob(
    display_name="cholera_forecast_model",
    optimization_objective="minimize-rmse",
    column_specs={
        "case_count": "numeric",
        "temperature": "numeric",
        "rainfall": "numeric",
        "population_density": "numeric"
    }
)

model = job.run(
    dataset=dataset,
    target_column="case_count",
    time_column="timestamp",
    time_series_identifier_column="location",
    forecast_horizon=72,  # 72 hours
    data_granularity_unit="hour",
    data_granularity_count=1,
    budget_milli_node_hours=1000,
    model_display_name="cholera_forecast_v1"
)

print(f"✅ Model trained: {model.resource_name}")
```

### Custom model with SHAP

```python
import shap
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from google.cloud import aiplatform

# Train custom model
X_train, y_train = load_training_data()

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Create SHAP explainer
explainer = shap.TreeExplainer(model)

# Upload to Vertex AI
aiplatform.Model.upload(
    display_name="disease_classifier_v1",
    artifact_uri="gs://your-bucket/models/disease_classifier",
    serving_container_image_uri="us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest"
)

print("✅ Model uploaded to Vertex AI")
```

## Making explainable predictions

### High-risk inference with SHAP

```python
import shap
from governance_kernel.vector_ledger import SovereignGuardrail, SovereigntyViolationError

def explainable_prediction(model, explainer, input_data, patient_id):
    """
    Make a prediction with SHAP explanation.
    
    Enforces EU AI Act §6 and GDPR Art. 22 requirements.
    """
    # Make prediction
    prediction = model.predict_proba(input_data)[0]
    confidence_score = float(np.max(prediction))
    predicted_class = int(np.argmax(prediction))
    
    # Generate SHAP explanation
    shap_values = explainer.shap_values(input_data)
    
    # Extract feature importance
    feature_names = ["fever", "cough", "diarrhea", "vomiting", "age", "location_risk"]
    feature_importance = dict(zip(feature_names, shap_values[predicted_class][0]))
    
    # Sort by absolute importance
    sorted_features = sorted(
        feature_importance.items(),
        key=lambda x: abs(x[1]),
        reverse=True
    )
    
    # Build evidence chain
    evidence_chain = [
        f"{feature}: {value:.3f}" for feature, value in sorted_features[:5]
    ]
    
    # Validate with SovereignGuardrail
    guardrail = SovereignGuardrail()
    
    try:
        guardrail.validate_action(
            action_type='High_Risk_Inference',
            payload={
                'actor': 'vertex_ai_model',
                'resource': f'patient_{patient_id}',
                'explanation': str(feature_importance),
                'confidence_score': confidence_score,
                'evidence_chain': evidence_chain,
                'shap_values': shap_values[predicted_class][0].tolist(),
                'consent_token': 'VALID_TOKEN',
                'consent_scope': 'diagnosis'
            },
            jurisdiction='EU_AI_ACT'
        )
        
        return {
            'prediction': predicted_class,
            'confidence': confidence_score,
            'shap_values': feature_importance,
            'evidence_chain': evidence_chain,
            'explanation': f"Top factors: {', '.join([f[0] for f in sorted_features[:3]])}",
            'compliant': True
        }
    
    except SovereigntyViolationError as e:
        print(f"❌ Compliance violation: {e}")
        return None

# Example usage
input_data = np.array([[38.5, 1, 1, 0, 45, 0.7]])  # fever, cough, diarrhea, vomiting, age, location_risk

result = explainable_prediction(model, explainer, input_data, patient_id="PAT_12345")

if result:
    print(f"✅ Prediction: {result['prediction']}")
    print(f"   Confidence: {result['confidence']:.2%}")
    print(f"   Explanation: {result['explanation']}")
    print(f"   Evidence: {result['evidence_chain']}")
```

## Visualization

### SHAP waterfall plot

```python
import shap
import matplotlib.pyplot as plt

# Generate SHAP values
shap_values = explainer.shap_values(input_data)

# Create waterfall plot
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values[1][0],
        base_values=explainer.expected_value[1],
        data=input_data[0],
        feature_names=feature_names
    )
)

plt.title("SHAP Waterfall Plot - Cholera Diagnosis")
plt.tight_layout()
plt.savefig("shap_waterfall.png", dpi=300, bbox_inches='tight')
print("✅ Waterfall plot saved")
```

### SHAP force plot

```python
# Create force plot
shap.force_plot(
    explainer.expected_value[1],
    shap_values[1][0],
    input_data[0],
    feature_names=feature_names,
    matplotlib=True,
    show=False
)

plt.savefig("shap_force.png", dpi=300, bbox_inches='tight')
print("✅ Force plot saved")
```

## Batch prediction with explanations

```python
from google.cloud import aiplatform

def batch_predict_with_shap(model_endpoint, input_gcs_uri, output_gcs_uri):
    """
    Run batch predictions with SHAP explanations.
    """
    # Create batch prediction job
    job = model_endpoint.batch_predict(
        job_display_name="cholera_batch_prediction",
        gcs_source=input_gcs_uri,
        gcs_destination_prefix=output_gcs_uri,
        machine_type="n1-standard-4",
        starting_replica_count=1,
        max_replica_count=10,
        generate_explanation=True,  # Enable explanations
        explanation_metadata={
            "inputs": {
                "fever": {"input_tensor_name": "fever"},
                "cough": {"input_tensor_name": "cough"},
                "diarrhea": {"input_tensor_name": "diarrhea"}
            },
            "outputs": {
                "prediction": {"output_tensor_name": "prediction"}
            }
        }
    )
    
    print(f"✅ Batch prediction job created: {job.resource_name}")
    
    # Wait for completion
    job.wait()
    
    print(f"✅ Batch prediction complete")
    print(f"   Results: {output_gcs_uri}")

# Example usage
endpoint = aiplatform.Endpoint("projects/123/locations/us-central1/endpoints/456")

batch_predict_with_shap(
    model_endpoint=endpoint,
    input_gcs_uri="gs://your-bucket/batch_input.jsonl",
    output_gcs_uri="gs://your-bucket/batch_output/"
)
```

## Compliance validation

### EU AI Act §6 requirements

<AccordionGroup>
  <Accordion title="High-risk AI system identification">
    iLuminara classifies clinical diagnosis and outbreak prediction as high-risk AI systems requiring explainability.
  </Accordion>
  <Accordion title="Transparency obligations">
    Every prediction includes SHAP values, feature importance, and evidence chains.
  </Accordion>
  <Accordion title="Human oversight">
    Predictions are flagged for human review when confidence < 0.7 or explanation quality is low.
  </Accordion>
  <Accordion title="Record keeping">
    All predictions and explanations are logged to tamper-proof audit trail (Cloud Spanner).
  </Accordion>
</AccordionGroup>

### GDPR Art. 22 requirements

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# Validate right to explanation
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'explanation': shap_explanation,
        'confidence_score': 0.92,
        'evidence_chain': evidence_chain,
        'human_review_required': confidence_score < 0.7
    },
    jurisdiction='GDPR_EU'
)
```

## Performance optimization

### Model caching

```python
from functools import lru_cache

@lru_cache(maxsize=10)
def get_model_and_explainer(model_name):
    """Cache model and explainer for performance."""
    model = load_model(model_name)
    explainer = shap.TreeExplainer(model)
    return model, explainer
```

### Batch SHAP computation

```python
# Compute SHAP values for multiple samples at once
shap_values = explainer.shap_values(X_batch)

# Process in parallel
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=4) as executor:
    results = list(executor.map(process_shap_values, shap_values))
```

## Monitoring

### Track explanation quality

```python
def explanation_quality_score(shap_values, threshold=0.1):
    """
    Calculate explanation quality score.
    
    High-quality explanations have:
    - Clear dominant features (high max SHAP value)
    - Low noise (few features with tiny SHAP values)
    """
    abs_shap = np.abs(shap_values)
    max_shap = np.max(abs_shap)
    significant_features = np.sum(abs_shap > threshold)
    
    quality = max_shap * (1 / (1 + significant_features))
    return quality

# Monitor explanation quality
quality = explanation_quality_score(shap_values[1][0])

if quality < 0.5:
    print("⚠️ Low explanation quality - flagging for human review")
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy autonomous surveillance agents
  </Card>
  <Card
    title="Governance"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/gcp"
  >
    Deploy to Google Cloud Platform
  </Card>
  <Card
    title="API reference"
    icon="terminal"
    href="/api-reference/overview"
  >
    Integrate with REST API
  </Card>
</CardGroup>
