---
title: Vertex AI + SHAP integration
description: Right to Explanation with explainable AI for high-risk clinical inferences
---

## Overview

iLuminara-Core integrates **Vertex AI** with **SHAP (SHapley Additive exPlanations)** to provide the "Right to Explanation" required by EU AI Act ¬ß6 and GDPR Art. 22.

Every high-risk clinical inference automatically triggers explainability analysis, ensuring compliance and building trust.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  EU AI Act ¬ß6 (High-Risk AI), GDPR Art. 22 (Right to Explanation)
</Card>

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      VERTEX AI MODEL                ‚îÇ
‚îÇ  - AutoML Time-Series               ‚îÇ
‚îÇ  - Custom Training                  ‚îÇ
‚îÇ  - Batch/Online Prediction          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚îÇ Prediction
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   HIGH-RISK INFERENCE DETECTOR      ‚îÇ
‚îÇ  - Confidence threshold: 0.7        ‚îÇ
‚îÇ  - Clinical decision flag           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚îÇ If high-risk
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      SHAP EXPLAINER                 ‚îÇ
‚îÇ  - Feature importance               ‚îÇ
‚îÇ  - SHAP values                      ‚îÇ
‚îÇ  - Evidence chain                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚îÇ Explanation
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   SOVEREIGNGUARDRAIL VALIDATION     ‚îÇ
‚îÇ  - Verify explanation completeness  ‚îÇ
‚îÇ  - Audit trail logging              ‚îÇ
‚îÇ  - Compliance attestation           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Setup

### Install dependencies

```bash
pip install google-cloud-aiplatform shap scikit-learn pandas numpy
```

### Configure Vertex AI

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project="your-project-id",
    location="us-central1",
    staging_bucket="gs://your-bucket"
)
```

## High-risk inference detection

```python
from governance_kernel.vector_ledger import SovereignGuardrail, SovereigntyViolationError
import shap
import numpy as np

class ExplainablePredictor:
    """
    Wrapper for Vertex AI models that enforces explainability
    for high-risk clinical inferences.
    """
    
    def __init__(
        self,
        model,
        feature_names,
        high_risk_threshold=0.7,
        jurisdiction="GDPR_EU"
    ):
        self.model = model
        self.feature_names = feature_names
        self.high_risk_threshold = high_risk_threshold
        self.jurisdiction = jurisdiction
        self.guardrail = SovereignGuardrail()
        
        # Initialize SHAP explainer
        self.explainer = shap.Explainer(model)
    
    def predict_with_explanation(self, X, patient_id=None):
        """
        Make prediction and generate explanation if high-risk.
        
        Args:
            X: Input features
            patient_id: Patient identifier for audit trail
        
        Returns:
            dict with prediction, confidence, and explanation
        """
        # Make prediction
        prediction = self.model.predict(X)
        confidence = self.model.predict_proba(X).max()
        
        # Check if high-risk
        is_high_risk = confidence >= self.high_risk_threshold
        
        result = {
            "prediction": prediction[0],
            "confidence": float(confidence),
            "is_high_risk": is_high_risk,
            "patient_id": patient_id
        }
        
        # Generate explanation for high-risk inferences
        if is_high_risk:
            explanation = self._generate_explanation(X)
            result["explanation"] = explanation
            
            # Validate with SovereignGuardrail
            self._validate_explanation(result)
        
        return result
    
    def _generate_explanation(self, X):
        """Generate SHAP explanation"""
        # Calculate SHAP values
        shap_values = self.explainer(X)
        
        # Extract feature importance
        feature_importance = dict(zip(
            self.feature_names,
            shap_values.values[0].tolist()
        ))
        
        # Sort by absolute importance
        sorted_features = sorted(
            feature_importance.items(),
            key=lambda x: abs(x[1]),
            reverse=True
        )
        
        # Build evidence chain
        evidence_chain = [
            f"{feature}: {value:.3f}"
            for feature, value in sorted_features[:5]
        ]
        
        return {
            "method": "SHAP",
            "shap_values": shap_values.values[0].tolist(),
            "feature_importance": feature_importance,
            "top_features": sorted_features[:5],
            "evidence_chain": evidence_chain,
            "base_value": float(shap_values.base_values[0])
        }
    
    def _validate_explanation(self, result):
        """Validate explanation with SovereignGuardrail"""
        try:
            self.guardrail.validate_action(
                action_type='High_Risk_Inference',
                payload={
                    'actor': 'vertex_ai_model',
                    'resource': f"patient_{result['patient_id']}",
                    'explanation': result['explanation']['method'],
                    'confidence_score': result['confidence'],
                    'evidence_chain': result['explanation']['evidence_chain'],
                    'consent_token': 'valid_token',
                    'consent_scope': 'diagnosis'
                },
                jurisdiction=self.jurisdiction
            )
        except SovereigntyViolationError as e:
            raise ValueError(f"Explanation validation failed: {e}")
```

## Example: Cholera outbreak prediction

```python
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import numpy as np

# Train model
X_train = pd.DataFrame({
    'diarrhea_cases': [10, 5, 20, 15, 8],
    'vomiting_cases': [8, 3, 18, 12, 6],
    'dehydration_cases': [5, 2, 15, 10, 4],
    'water_quality_score': [0.3, 0.7, 0.2, 0.4, 0.6],
    'population_density': [1000, 500, 1500, 1200, 800]
})
y_train = [1, 0, 1, 1, 0]  # 1 = outbreak, 0 = no outbreak

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Create explainable predictor
predictor = ExplainablePredictor(
    model=model,
    feature_names=X_train.columns.tolist(),
    high_risk_threshold=0.7,
    jurisdiction="KDPA_KE"
)

# Make prediction with explanation
X_test = pd.DataFrame({
    'diarrhea_cases': [25],
    'vomiting_cases': [22],
    'dehydration_cases': [18],
    'water_quality_score': [0.15],
    'population_density': [1800]
})

result = predictor.predict_with_explanation(X_test, patient_id="DADAAB_001")

print(f"Prediction: {result['prediction']}")
print(f"Confidence: {result['confidence']:.2%}")
print(f"High-risk: {result['is_high_risk']}")

if result['is_high_risk']:
    print("\nüîç EXPLANATION REQUIRED (EU AI Act ¬ß6)")
    print(f"Method: {result['explanation']['method']}")
    print("\nTop contributing features:")
    for feature, importance in result['explanation']['top_features']:
        print(f"  - {feature}: {importance:.3f}")
    print("\nEvidence chain:")
    for evidence in result['explanation']['evidence_chain']:
        print(f"  ‚úì {evidence}")
```

## Vertex AI AutoML integration

```python
from google.cloud import aiplatform

def train_explainable_model(
    dataset_id,
    target_column,
    feature_columns,
    model_display_name="cholera-outbreak-predictor"
):
    """
    Train AutoML model with explainability enabled.
    """
    # Create dataset
    dataset = aiplatform.TabularDataset(dataset_id)
    
    # Train AutoML model
    job = aiplatform.AutoMLTabularTrainingJob(
        display_name=model_display_name,
        optimization_prediction_type="classification",
        optimization_objective="maximize-au-prc"
    )
    
    model = job.run(
        dataset=dataset,
        target_column=target_column,
        training_fraction_split=0.8,
        validation_fraction_split=0.1,
        test_fraction_split=0.1,
        model_display_name=model_display_name,
        # Enable explainability
        export_evaluated_data_items=True,
        export_evaluated_data_items_bigquery_destination_uri="bq://your-project.dataset.table"
    )
    
    return model

def predict_with_vertex_ai(
    model_resource_name,
    instances,
    explain=True
):
    """
    Make prediction with Vertex AI model and get explanation.
    """
    endpoint = aiplatform.Endpoint(model_resource_name)
    
    if explain:
        # Get prediction with explanation
        response = endpoint.explain(instances=instances)
        
        predictions = response.predictions
        explanations = response.explanations
        
        return {
            "predictions": predictions,
            "explanations": explanations,
            "attributions": [
                exp.attributions for exp in explanations
            ]
        }
    else:
        # Get prediction only
        predictions = endpoint.predict(instances=instances)
        return {"predictions": predictions.predictions}
```

## SHAP visualization

```python
import shap
import matplotlib.pyplot as plt

def visualize_shap_explanation(explainer, X, feature_names):
    """
    Generate SHAP visualizations for documentation.
    """
    # Calculate SHAP values
    shap_values = explainer(X)
    
    # Waterfall plot (single prediction)
    shap.plots.waterfall(shap_values[0], show=False)
    plt.savefig("shap_waterfall.png", bbox_inches='tight', dpi=300)
    plt.close()
    
    # Force plot (single prediction)
    shap.plots.force(shap_values[0], show=False, matplotlib=True)
    plt.savefig("shap_force.png", bbox_inches='tight', dpi=300)
    plt.close()
    
    # Summary plot (all predictions)
    shap.plots.beeswarm(shap_values, show=False)
    plt.savefig("shap_summary.png", bbox_inches='tight', dpi=300)
    plt.close()
    
    # Feature importance
    shap.plots.bar(shap_values, show=False)
    plt.savefig("shap_importance.png", bbox_inches='tight', dpi=300)
    plt.close()
    
    print("‚úÖ SHAP visualizations saved")
```

## Compliance validation

```python
from governance_kernel.vector_ledger import SovereignGuardrail

def validate_explainability_compliance(
    prediction_result,
    jurisdiction="EU_AI_ACT"
):
    """
    Validate that explanation meets compliance requirements.
    """
    guardrail = SovereignGuardrail()
    
    # Check required fields
    required_fields = [
        'confidence_score',
        'evidence_chain',
        'explanation'
    ]
    
    for field in required_fields:
        if field not in prediction_result:
            raise ValueError(f"Missing required field: {field}")
    
    # Validate with SovereignGuardrail
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'actor': 'ml_system',
            'resource': 'clinical_decision',
            'explanation': prediction_result['explanation']['method'],
            'confidence_score': prediction_result['confidence'],
            'evidence_chain': prediction_result['explanation']['evidence_chain'],
            'consent_token': 'valid_token',
            'consent_scope': 'diagnosis'
        },
        jurisdiction=jurisdiction
    )
    
    print("‚úÖ Explainability compliance validated")
    print(f"   Jurisdiction: {jurisdiction}")
    print(f"   Method: {prediction_result['explanation']['method']}")
    print(f"   Confidence: {prediction_result['confidence']:.2%}")
```

## Batch prediction with explanations

```python
def batch_predict_with_explanations(
    model,
    X_batch,
    patient_ids,
    output_path="predictions_with_explanations.json"
):
    """
    Process batch predictions with explanations for high-risk cases.
    """
    predictor = ExplainablePredictor(
        model=model,
        feature_names=X_batch.columns.tolist(),
        high_risk_threshold=0.7
    )
    
    results = []
    
    for i, (_, row) in enumerate(X_batch.iterrows()):
        X_single = row.values.reshape(1, -1)
        patient_id = patient_ids[i]
        
        result = predictor.predict_with_explanation(
            X_single,
            patient_id=patient_id
        )
        
        results.append(result)
        
        if result['is_high_risk']:
            print(f"‚ö†Ô∏è  High-risk prediction for {patient_id}")
            print(f"   Confidence: {result['confidence']:.2%}")
            print(f"   Top feature: {result['explanation']['top_features'][0]}")
    
    # Save results
    import json
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n‚úÖ Batch predictions saved to {output_path}")
    return results
```

## Real-time explanation API

```python
from flask import Flask, request, jsonify

app = Flask(__name__)

# Load model and create predictor
model = load_trained_model()
predictor = ExplainablePredictor(
    model=model,
    feature_names=['diarrhea_cases', 'vomiting_cases', 'dehydration_cases'],
    high_risk_threshold=0.7
)

@app.route('/predict', methods=['POST'])
def predict_endpoint():
    """
    API endpoint for predictions with automatic explanations.
    """
    data = request.json
    
    # Extract features
    X = pd.DataFrame([data['features']])
    patient_id = data.get('patient_id')
    
    # Make prediction with explanation
    result = predictor.predict_with_explanation(X, patient_id)
    
    return jsonify(result)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
```

## Testing explainability

```bash
# Test prediction endpoint
curl -X POST http://localhost:8080/predict \
  -H "Content-Type: application/json" \
  -d '{
    "features": {
      "diarrhea_cases": 25,
      "vomiting_cases": 22,
      "dehydration_cases": 18
    },
    "patient_id": "TEST_001"
  }'
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy autonomous surveillance agents
  </Card>
  <Card
    title="Governance"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="API reference"
    icon="terminal"
    href="/api-reference/overview"
  >
    Integrate with REST API
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/gcp"
  >
    Deploy to Google Cloud Platform
  </Card>
</CardGroup>
