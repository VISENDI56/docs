---
title: Vertex AI + SHAP integration
description: Right to Explanation enforcement with explainable AI for high-risk clinical inferences
---

## Overview

iLuminara-Core integrates **Google Cloud Vertex AI** with **SHAP (SHapley Additive exPlanations)** to provide mandatory explainability for all high-risk clinical AI decisions, enforcing:

- **EU AI Act §6** - High-Risk AI Systems Classification
- **EU AI Act §13** - Transparency and Provision of Information
- **GDPR Art. 22** - Right to Explanation for Automated Decision-Making
- **HIPAA §164.524** - Right of Access to Health Information

<Card
  title="Philosophy"
  icon="lightbulb"
>
  "Every high-risk clinical inference requires explainability. No black boxes in life-or-death decisions."
</Card>

## Architecture

```
┌──────────────────────────────────────────────────────────────┐
│                    VERTEX AI PIPELINE                         │
│  ┌────────────────────────────────────────────────────────┐  │
│  │  1. Data Ingestion (CBS + EMR + IDSR)                  │  │
│  │     └─ Golden Thread Fusion                            │  │
│  └────────────────────────────────────────────────────────┘  │
│  ┌────────────────────────────────────────────────────────┐  │
│  │  2. Feature Engineering                                │  │
│  │     └─ Temporal, Spatial, Clinical Features           │  │
│  └────────────────────────────────────────────────────────┘  │
│  ┌────────────────────────────────────────────────────────┐  │
│  │  3. Model Training (AutoML / Custom)                   │  │
│  │     └─ XGBoost, Random Forest, Neural Networks        │  │
│  └────────────────────────────────────────────────────────┘  │
│  ┌────────────────────────────────────────────────────────┐  │
│  │  4. SHAP Explainability Layer                          │  │
│  │     └─ TreeExplainer, DeepExplainer, KernelExplainer  │  │
│  └────────────────────────────────────────────────────────┘  │
│  ┌────────────────────────────────────────────────────────┐  │
│  │  5. SovereignGuardrail Validation                      │  │
│  │     └─ Confidence threshold, Evidence chain, Consent  │  │
│  └────────────────────────────────────────────────────────┘  │
│  ┌────────────────────────────────────────────────────────┐  │
│  │  6. Deployment (Endpoint + Monitoring)                 │  │
│  │     └─ Real-time inference with explainability        │  │
│  └────────────────────────────────────────────────────────┘  │
└──────────────────────────────────────────────────────────────┘
```

## High-risk AI classification

Per EU AI Act §6, iLuminara classifies AI systems as high-risk if they:

1. **Clinical diagnosis** - Predict disease presence/absence
2. **Treatment recommendations** - Suggest medical interventions
3. **Resource allocation** - Triage or prioritize patients
4. **Outbreak prediction** - Forecast epidemic trajectories
5. **Risk stratification** - Classify patient risk levels

**Threshold:** Confidence score ≥ 0.7 triggers mandatory explainability

## Implementation

### 1. Train model with Vertex AI

```python
from google.cloud import aiplatform
from google.cloud.aiplatform import gapic as aip

# Initialize Vertex AI
aiplatform.init(
    project="iluminara-core",
    location="africa-south1",
    staging_bucket="gs://iluminara-ml-staging"
)

# Create AutoML Tabular training job
dataset = aiplatform.TabularDataset.create(
    display_name="cholera_outbreak_prediction",
    gcs_source="gs://iluminara-data/cholera_training.csv"
)

job = aiplatform.AutoMLTabularTrainingJob(
    display_name="cholera_automl_v1",
    optimization_prediction_type="classification",
    optimization_objective="maximize-au-prc"
)

model = job.run(
    dataset=dataset,
    target_column="outbreak_confirmed",
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
    budget_milli_node_hours=1000,
    model_display_name="cholera_predictor_v1",
    disable_early_stopping=False
)

print(f"Model resource name: {model.resource_name}")
```

### 2. Deploy model with explainability

```python
# Deploy model to endpoint
endpoint = model.deploy(
    deployed_model_display_name="cholera_predictor_endpoint",
    machine_type="n1-standard-4",
    min_replica_count=1,
    max_replica_count=10,
    accelerator_type=None,
    explanation_metadata=aiplatform.explain.ExplanationMetadata(
        inputs={
            "fever_count": {"input_tensor_name": "fever_count"},
            "diarrhea_count": {"input_tensor_name": "diarrhea_count"},
            "vomiting_count": {"input_tensor_name": "vomiting_count"},
            "location_lat": {"input_tensor_name": "location_lat"},
            "location_lng": {"input_tensor_name": "location_lng"},
            "population_density": {"input_tensor_name": "population_density"},
            "water_quality_score": {"input_tensor_name": "water_quality_score"}
        },
        outputs={"outbreak_probability": {"output_tensor_name": "outbreak_probability"}}
    ),
    explanation_parameters=aiplatform.explain.ExplanationParameters(
        sampled_shapley_attribution=aiplatform.explain.SampledShapleyAttribution(
            path_count=10
        )
    )
)

print(f"Endpoint resource name: {endpoint.resource_name}")
```

### 3. Make prediction with SHAP explanation

```python
from governance_kernel.vector_ledger import SovereignGuardrail, SovereigntyViolationError

# Prepare input
instances = [{
    "fever_count": 45,
    "diarrhea_count": 38,
    "vomiting_count": 22,
    "location_lat": 0.0512,
    "location_lng": 40.3129,
    "population_density": 15000,
    "water_quality_score": 0.3
}]

# Get prediction with explanation
prediction = endpoint.predict(instances=instances)

# Extract results
confidence_score = prediction.predictions[0]["outbreak_probability"]
shap_values = prediction.explanations[0].attributions

# Build evidence chain
evidence_chain = []
for attribution in shap_values:
    feature_name = attribution.feature_attributes["name"]
    contribution = attribution.feature_attributes["value"]
    evidence_chain.append({
        "feature": feature_name,
        "contribution": contribution,
        "importance": abs(contribution)
    })

# Sort by importance
evidence_chain.sort(key=lambda x: x["importance"], reverse=True)

# Validate with SovereignGuardrail
guardrail = SovereignGuardrail()

try:
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'actor': 'vertex_ai_model',
            'resource': 'cholera_outbreak_prediction',
            'explanation': f"SHAP values: {evidence_chain[:5]}",
            'confidence_score': confidence_score,
            'evidence_chain': [e["feature"] for e in evidence_chain[:5]],
            'consent_token': 'PUBLIC_HEALTH_SURVEILLANCE',
            'consent_scope': 'outbreak_prediction'
        },
        jurisdiction='EU_AI_ACT'
    )
    
    print(f"✅ Prediction approved - Confidence: {confidence_score:.2%}")
    print(f"Top 5 contributing features:")
    for i, evidence in enumerate(evidence_chain[:5], 1):
        print(f"  {i}. {evidence['feature']}: {evidence['contribution']:.4f}")
    
except SovereigntyViolationError as e:
    print(f"❌ Prediction blocked: {e}")
```

## SHAP explainer types

### TreeExplainer (for tree-based models)

```python
import shap
import xgboost as xgb

# Train XGBoost model
model = xgb.XGBClassifier(
    max_depth=6,
    learning_rate=0.1,
    n_estimators=100,
    objective='binary:logistic'
)
model.fit(X_train, y_train)

# Create SHAP explainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Visualize
shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.waterfall_plot(shap.Explanation(
    values=shap_values[0],
    base_values=explainer.expected_value,
    data=X_test.iloc[0],
    feature_names=X_test.columns.tolist()
))
```

### DeepExplainer (for neural networks)

```python
import tensorflow as tf

# Train neural network
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy', 'AUC']
)

model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

# Create SHAP explainer
explainer = shap.DeepExplainer(model, X_train[:100])
shap_values = explainer.shap_values(X_test[:10])

# Visualize
shap.force_plot(
    explainer.expected_value[0],
    shap_values[0][0],
    X_test.iloc[0]
)
```

### KernelExplainer (model-agnostic)

```python
from sklearn.ensemble import RandomForestClassifier

# Train any model
model = RandomForestClassifier(n_estimators=100, max_depth=10)
model.fit(X_train, y_train)

# Create model-agnostic explainer
explainer = shap.KernelExplainer(
    model.predict_proba,
    shap.sample(X_train, 100)
)
shap_values = explainer.shap_values(X_test[:10])

# Visualize
shap.force_plot(
    explainer.expected_value[1],
    shap_values[1][0],
    X_test.iloc[0]
)
```

## Compliance validation

### EU AI Act §13 - Transparency requirements

```python
def generate_ai_transparency_report(
    model_name: str,
    prediction: dict,
    shap_values: list,
    confidence_score: float
) -> dict:
    """
    Generate transparency report per EU AI Act §13
    """
    return {
        "model_identification": {
            "name": model_name,
            "version": "v1.0.0",
            "training_date": "2025-01-15",
            "provider": "iLuminara-Core"
        },
        "intended_purpose": "Cholera outbreak prediction for public health surveillance",
        "risk_classification": "HIGH_RISK" if confidence_score >= 0.7 else "LOW_RISK",
        "prediction": {
            "outcome": prediction["outbreak_probability"],
            "confidence": confidence_score,
            "timestamp": "2025-01-20T10:00:00Z"
        },
        "explanation": {
            "method": "SHAP (SHapley Additive exPlanations)",
            "top_features": [
                {
                    "feature": attr["feature"],
                    "contribution": attr["contribution"],
                    "importance_rank": i + 1
                }
                for i, attr in enumerate(shap_values[:5])
            ]
        },
        "human_oversight": {
            "required": True,
            "reviewer": "Dr. Amina Hassan, Epidemiologist",
            "review_status": "PENDING"
        },
        "data_governance": {
            "training_data_source": "CBS + EMR + IDSR (Golden Thread)",
            "data_quality_score": 0.92,
            "bias_mitigation": "Stratified sampling across geographic regions"
        },
        "compliance": {
            "frameworks": ["EU_AI_ACT", "GDPR_ART_22", "HIPAA"],
            "audit_trail_id": "AUDIT_20250120_001"
        }
    }
```

### GDPR Art. 22 - Right to explanation

```python
def provide_gdpr_explanation(
    data_subject_id: str,
    decision: str,
    shap_values: list,
    confidence_score: float
) -> dict:
    """
    Provide GDPR Art. 22 compliant explanation
    """
    return {
        "data_subject_id": data_subject_id,
        "decision": decision,
        "automated_decision_making": True,
        "legal_basis": "GDPR Art. 9(2)(i) - Public Health",
        "explanation": {
            "plain_language": f"The AI system predicted a {confidence_score:.0%} probability of cholera outbreak based on analysis of {len(shap_values)} factors.",
            "key_factors": [
                f"{attr['feature']}: {'increased' if attr['contribution'] > 0 else 'decreased'} risk by {abs(attr['contribution']):.2%}"
                for attr in shap_values[:3]
            ]
        },
        "right_to_object": {
            "available": True,
            "process": "Contact Data Protection Officer at dpo@iluminara.health"
        },
        "right_to_human_review": {
            "available": True,
            "process": "Request manual review by epidemiologist"
        },
        "data_sources": ["Community-Based Surveillance", "Electronic Medical Records", "IDSR Reports"],
        "retention_period": "180 days (HOT storage)",
        "contact": {
            "dpo": "dpo@iluminara.health",
            "regulator": "Data Protection Commissioner (Kenya)"
        }
    }
```

## Integration with iLuminara stack

### Golden Thread + Vertex AI

```python
from edge_node.sync_protocol.golden_thread import GoldenThread
from google.cloud import aiplatform

# Fuse data streams
gt = GoldenThread()
fused_data = gt.fuse_data_streams(
    cbs_signal={"location": "Dadaab", "symptom": "diarrhea", "count": 38},
    emr_record={"location": "Dadaab", "diagnosis": "cholera", "count": 22},
    patient_id="AGGREGATE_DADAAB"
)

# Prepare features for Vertex AI
features = {
    "fever_count": fused_data.symptom_counts.get("fever", 0),
    "diarrhea_count": fused_data.symptom_counts.get("diarrhea", 0),
    "vomiting_count": fused_data.symptom_counts.get("vomiting", 0),
    "location_lat": fused_data.location["lat"],
    "location_lng": fused_data.location["lng"],
    "verification_score": fused_data.verification_score,
    "data_quality": fused_data.quality_metrics["completeness"]
}

# Get prediction with explanation
endpoint = aiplatform.Endpoint("projects/123/locations/africa-south1/endpoints/456")
prediction = endpoint.predict(instances=[features])

# Log to audit trail
guardrail.log_audit_event(
    event_type="AI_PREDICTION",
    payload={
        "model": "cholera_predictor_v1",
        "input": features,
        "output": prediction.predictions[0],
        "explanation": prediction.explanations[0],
        "verification_score": fused_data.verification_score
    }
)
```

### Offline SHAP for edge deployment

```python
import joblib
import shap

# Export trained model for edge
model = joblib.load("cholera_model.pkl")
explainer = shap.TreeExplainer(model)

# Save explainer
joblib.dump(explainer, "cholera_explainer.pkl")

# On edge device (offline)
explainer = joblib.load("cholera_explainer.pkl")
shap_values = explainer.shap_values(X_local)

# Generate explanation without internet
explanation = {
    "confidence": model.predict_proba(X_local)[0][1],
    "shap_values": shap_values[0].tolist(),
    "feature_names": X_local.columns.tolist()
}
```

## Monitoring & alerting

### Model performance monitoring

```python
from google.cloud import aiplatform_v1

# Create model monitoring job
monitoring_job = aiplatform_v1.ModelDeploymentMonitoringJob(
    display_name="cholera_model_monitoring",
    endpoint=endpoint.resource_name,
    model_deployment_monitoring_objective_configs=[
        aiplatform_v1.ModelDeploymentMonitoringObjectiveConfig(
            deployed_model_id=deployed_model.id,
            objective_config=aiplatform_v1.ModelMonitoringObjectiveConfig(
                training_dataset=aiplatform_v1.ModelMonitoringObjectiveConfig.TrainingDataset(
                    target_field="outbreak_confirmed",
                    data_format="csv",
                    gcs_source=aiplatform_v1.GcsSource(
                        uris=["gs://iluminara-data/cholera_training.csv"]
                    )
                ),
                training_prediction_skew_detection_config=aiplatform_v1.ModelMonitoringObjectiveConfig.TrainingPredictionSkewDetectionConfig(
                    skew_thresholds={
                        "fever_count": aiplatform_v1.ThresholdConfig(value=0.3),
                        "diarrhea_count": aiplatform_v1.ThresholdConfig(value=0.3)
                    }
                )
            )
        )
    ],
    model_deployment_monitoring_schedule_config=aiplatform_v1.ModelDeploymentMonitoringScheduleConfig(
        monitor_interval={"seconds": 3600}  # Hourly
    ),
    logging_sampling_strategy=aiplatform_v1.SamplingStrategy(
        random_sample_config=aiplatform_v1.SamplingStrategy.RandomSampleConfig(
            sample_rate=1.0
        )
    )
)
```

### Explainability drift detection

```python
import numpy as np

def detect_explanation_drift(
    baseline_shap_values: np.ndarray,
    current_shap_values: np.ndarray,
    threshold: float = 0.3
) -> dict:
    """
    Detect if SHAP explanations have drifted significantly
    """
    # Calculate mean absolute difference
    drift = np.mean(np.abs(baseline_shap_values - current_shap_values))
    
    # Calculate feature importance correlation
    baseline_importance = np.abs(baseline_shap_values).mean(axis=0)
    current_importance = np.abs(current_shap_values).mean(axis=0)
    correlation = np.corrcoef(baseline_importance, current_importance)[0, 1]
    
    return {
        "drift_detected": drift > threshold,
        "drift_magnitude": float(drift),
        "importance_correlation": float(correlation),
        "action_required": "RETRAIN_MODEL" if drift > threshold else "CONTINUE_MONITORING"
    }
```

## Best practices

<Steps>
  <Step title="Always validate high-risk predictions">
    Use SovereignGuardrail to enforce explainability for confidence ≥ 0.7
  </Step>
  <Step title="Provide plain-language explanations">
    Translate SHAP values into human-readable insights for clinicians
  </Step>
  <Step title="Enable human oversight">
    High-risk decisions require manual review by qualified personnel
  </Step>
  <Step title="Monitor explanation drift">
    Track changes in feature importance over time
  </Step>
  <Step title="Document everything">
    Maintain tamper-proof audit trail of all AI decisions
  </Step>
</Steps>

## Next steps

<CardGroup cols={2}>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy autonomous surveillance agents
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Understand compliance enforcement
  </Card>
  <Card
    title="Golden Thread"
    icon="link"
    href="/architecture/golden-thread"
  >
    Data fusion for AI training
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/gcp"
  >
    Deploy Vertex AI to production
  </Card>
</CardGroup>
