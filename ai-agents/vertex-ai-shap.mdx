---
title: Vertex AI + SHAP integration
description: Right to Explanation with SHAP explainability for high-risk clinical inferences
---

## Overview

iLuminara-Core integrates Google Cloud Vertex AI with SHAP (SHapley Additive exPlanations) to provide **Right to Explanation** for every high-risk clinical inference, ensuring compliance with EU AI Act §6 and GDPR Art. 22.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk inference automatically triggers SHAP analysis to comply with EU AI Act §6 (High-Risk AI) and GDPR Art. 22 (Right to Explanation).
</Card>

## Architecture

```
┌─────────────────────────────────────┐
│         VERTEX AI                   │
│  ┌──────────────────────────────┐  │
│  │  AutoML Model Training       │  │
│  │  - Time-series forecasting   │  │
│  │  - Disease classification    │  │
│  │  - Risk prediction           │  │
│  └──────────────────────────────┘  │
└─────────────────────────────────────┘
              │
              │ Prediction
              ▼
┌─────────────────────────────────────┐
│         SHAP EXPLAINER              │
│  ┌──────────────────────────────┐  │
│  │  Feature Importance          │  │
│  │  - SHAP values               │  │
│  │  - Force plots               │  │
│  │  - Waterfall charts          │  │
│  └──────────────────────────────┘  │
└─────────────────────────────────────┘
              │
              │ Explanation
              ▼
┌─────────────────────────────────────┐
│         SOVEREIGN GUARDRAIL         │
│  ┌──────────────────────────────┐  │
│  │  Compliance Validation       │  │
│  │  - Confidence threshold      │  │
│  │  - Evidence chain            │  │
│  │  - Audit logging             │  │
│  └──────────────────────────────┘  │
└─────────────────────────────────────┘
```

## High-risk inference detection

The system automatically detects high-risk clinical inferences:

```python
from governance_kernel.vector_ledger import SovereignGuardrail
import shap
import numpy as np

# Initialize guardrail
guardrail = SovereignGuardrail()

# Make prediction with Vertex AI model
prediction = vertex_ai_model.predict(patient_features)
confidence_score = prediction['confidence']

# Check if high-risk (threshold: 0.7)
if confidence_score >= 0.7:
    # REQUIRED: Generate SHAP explanation
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(patient_features)
    
    # Validate with SovereignGuardrail
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': prediction['diagnosis'],
            'confidence_score': confidence_score,
            'explanation': shap_values.tolist(),
            'evidence_chain': [
                f"Feature: {feature}, SHAP: {shap_val:.3f}"
                for feature, shap_val in zip(feature_names, shap_values[0])
            ],
            'consent_token': patient_consent_token
        },
        jurisdiction='EU_AI_ACT'
    )
```

## SHAP integration

### Installation

```bash
pip install shap google-cloud-aiplatform
```

### Basic usage

```python
import shap
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(project='your-project-id', location='us-central1')

# Load model
model = aiplatform.Model('projects/123/locations/us-central1/models/456')

# Create SHAP explainer
explainer = shap.TreeExplainer(model)

# Generate explanations
shap_values = explainer.shap_values(X_test)

# Visualize
shap.summary_plot(shap_values, X_test, feature_names=feature_names)
```

## Explanation methods

### SHAP values

Quantifies the contribution of each feature to the prediction:

```python
import shap
import matplotlib.pyplot as plt

# Generate SHAP values
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(patient_data)

# Feature contributions
for feature, shap_val in zip(feature_names, shap_values[0]):
    print(f"{feature}: {shap_val:.3f}")

# Output:
# fever_duration: 0.234
# cough_severity: 0.189
# age: 0.156
# location_risk: 0.098
```

### Force plots

Visualizes how features push prediction from baseline:

```python
# Generate force plot
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    patient_data.iloc[0],
    feature_names=feature_names,
    matplotlib=True
)
plt.savefig('force_plot.png')
```

### Waterfall charts

Shows cumulative feature contributions:

```python
# Generate waterfall chart
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values[0],
        base_values=explainer.expected_value,
        data=patient_data.iloc[0],
        feature_names=feature_names
    )
)
plt.savefig('waterfall.png')
```

## Vertex AI model training

### Time-series forecasting

```python
from google.cloud import aiplatform

# Initialize
aiplatform.init(project='iluminara-health', location='us-central1')

# Create dataset
dataset = aiplatform.TimeSeriesDataset.create(
    display_name='cholera_outbreak_forecast',
    gcs_source='gs://iluminara-data/cholera_cases.csv',
    time_column='date',
    time_series_identifier_column='location',
    target_column='case_count'
)

# Train AutoML model
job = aiplatform.AutoMLForecastingTrainingJob(
    display_name='cholera_forecast_model',
    optimization_objective='minimize-rmse',
    column_transformations=[
        {'numeric': {'column_name': 'temperature'}},
        {'numeric': {'column_name': 'rainfall'}},
        {'categorical': {'column_name': 'location'}}
    ]
)

model = job.run(
    dataset=dataset,
    target_column='case_count',
    time_column='date',
    time_series_identifier_column='location',
    forecast_horizon=72,  # 72 hours
    data_granularity_unit='hour',
    data_granularity_count=1,
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1
)
```

### Disease classification

```python
# Create tabular dataset
dataset = aiplatform.TabularDataset.create(
    display_name='disease_classification',
    gcs_source='gs://iluminara-data/symptoms.csv'
)

# Train AutoML classifier
job = aiplatform.AutoMLTabularTrainingJob(
    display_name='disease_classifier',
    optimization_objective='maximize-au-prc',
    column_transformations=[
        {'categorical': {'column_name': 'symptom_1'}},
        {'categorical': {'column_name': 'symptom_2'}},
        {'numeric': {'column_name': 'age'}},
        {'numeric': {'column_name': 'temperature'}}
    ]
)

model = job.run(
    dataset=dataset,
    target_column='disease',
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
    model_display_name='disease_classifier_v1'
)
```

## Compliance validation

### EU AI Act §6 (High-Risk AI)

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# Validate high-risk inference
try:
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'cholera_diagnosis',
            'confidence_score': 0.92,
            'explanation': shap_values.tolist(),
            'evidence_chain': [
                'Severe diarrhea (SHAP: 0.45)',
                'Dehydration (SHAP: 0.32)',
                'Location: Dadaab (SHAP: 0.15)'
            ],
            'consent_token': 'VALID_CONSENT_TOKEN',
            'consent_scope': 'diagnosis'
        },
        jurisdiction='EU_AI_ACT'
    )
    print("✅ Compliance validated")
except SovereigntyViolationError as e:
    print(f"❌ Violation: {e}")
```

### GDPR Art. 22 (Right to Explanation)

```python
# Generate human-readable explanation
def generate_explanation(shap_values, feature_names, prediction):
    explanation = {
        'prediction': prediction,
        'confidence': confidence_score,
        'reasoning': [],
        'top_features': []
    }
    
    # Sort features by absolute SHAP value
    feature_importance = sorted(
        zip(feature_names, shap_values[0]),
        key=lambda x: abs(x[1]),
        reverse=True
    )
    
    # Top 5 features
    for feature, shap_val in feature_importance[:5]:
        direction = "increases" if shap_val > 0 else "decreases"
        explanation['reasoning'].append(
            f"{feature} {direction} risk by {abs(shap_val):.2%}"
        )
        explanation['top_features'].append({
            'feature': feature,
            'contribution': float(shap_val)
        })
    
    return explanation

# Example output
explanation = generate_explanation(shap_values, feature_names, 'cholera')
print(explanation)
# {
#   'prediction': 'cholera',
#   'confidence': 0.92,
#   'reasoning': [
#     'severe_diarrhea increases risk by 45%',
#     'dehydration increases risk by 32%',
#     'location_dadaab increases risk by 15%'
#   ]
# }
```

## Audit trail

All high-risk inferences are logged with tamper-proof audit:

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail(enable_tamper_proof_audit=True)

# Inference is automatically logged
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'actor': 'ml_system',
        'resource': 'patient_diagnosis',
        'explanation': shap_values.tolist(),
        'confidence_score': 0.92,
        'evidence_chain': evidence_chain,
        'consent_token': 'valid_token'
    },
    jurisdiction='EU_AI_ACT'
)

# Retrieve audit history
history = guardrail.get_tamper_proof_audit_history(limit=10)

# Verify chain integrity
integrity = guardrail.verify_audit_chain_integrity()
print(f"Chain valid: {integrity['chain_valid']}")
```

## Performance optimization

### Batch predictions

```python
# Batch predict for efficiency
batch_predictions = model.batch_predict(
    job_display_name='batch_cholera_forecast',
    gcs_source='gs://iluminara-data/batch_input.csv',
    gcs_destination_prefix='gs://iluminara-data/batch_output/',
    machine_type='n1-standard-4',
    accelerator_type='NVIDIA_TESLA_T4',
    accelerator_count=1
)
```

### Model caching

```python
import joblib

# Cache SHAP explainer
explainer = shap.TreeExplainer(model)
joblib.dump(explainer, 'shap_explainer.pkl')

# Load cached explainer
explainer = joblib.load('shap_explainer.pkl')
```

## Integration with dashboard

Display explanations in the Streamlit dashboard:

```python
import streamlit as st
import shap

st.title("High-Risk Inference Explanation")

# Display prediction
st.metric("Diagnosis", prediction['disease'], f"{confidence_score:.1%} confidence")

# Display SHAP waterfall
st.subheader("Feature Contributions")
fig = shap.waterfall_plot(shap_explanation, show=False)
st.pyplot(fig)

# Display evidence chain
st.subheader("Evidence Chain")
for evidence in evidence_chain:
    st.write(f"- {evidence}")

# Compliance status
st.success("✅ EU AI Act §6 compliant - Explanation provided")
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy autonomous surveillance
  </Card>
  <Card
    title="Deploy to GCP"
    icon="cloud"
    href="/deployment/overview"
  >
    Production deployment guide
  </Card>
  <Card
    title="Security stack"
    icon="shield-halved"
    href="/security/overview"
  >
    Fortress security architecture
  </Card>
</CardGroup>
