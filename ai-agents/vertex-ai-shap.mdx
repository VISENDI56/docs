---
title: Vertex AI + SHAP integration
description: Right to Explanation with explainable AI for high-risk clinical decisions
---

## Overview

iLuminara-Core integrates **Vertex AI Explainable AI** with **SHAP (SHapley Additive exPlanations)** to fulfill "Right to Explanation" requirements under EU AI Act §6 and GDPR Art. 22.

<Card
  title="Philosophy"
  icon="lightbulb"
>
  "Every high-risk clinical inference requires explainability. Black boxes violate dignity."
</Card>

## Compliance requirements

| Framework | Article | Requirement |
|-----------|---------|-------------|
| **EU AI Act** | §6 | High-risk AI systems must provide explanations |
| **GDPR** | Art. 22 | Right to explanation for automated decisions |
| **NIST AI RMF** | Measure | Explainability and interpretability |
| **HIPAA** | §164.524 | Right to access PHI and decision rationale |

## Architecture

```
┌────────────────────────────────────────────────────────────────┐
│                   VERTEX AI EXPLAINABLE AI                      │
└────────────────────────────────────────────────────────────────┘
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
   ┌────▼────┐      ┌──────▼──────┐    ┌──────▼──────┐
   │  SHAP   │      │   LIME      │    │  FEATURE    │
   │ VALUES  │      │ ANALYSIS    │    │ IMPORTANCE  │
   └────┬────┘      └──────┬──────┘    └──────┬──────┘
        │                  │                  │
        └──────────────────┼──────────────────┘
                     ▼
          ┌────────────────────────┐
          │  EXPLANATION CACHE     │
          │  (Audit Trail)         │
          └────────────────────────┘
                     ▼
          ┌────────────────────────┐
          │  SOVEREIGNGUARDRAIL    │
          │  (Compliance Check)    │
          └────────────────────────┘
```

## SHAP analysis

### What is SHAP?

SHAP (SHapley Additive exPlanations) is a game-theoretic approach to explain machine learning model predictions by computing the contribution of each feature to the prediction.

**Key properties:**
- **Local accuracy**: Explanation matches model prediction
- **Missingness**: Missing features have zero contribution
- **Consistency**: Higher feature value = higher contribution

### Basic usage

```python
from governance_kernel.humanitarian_constraints import VertexAIExplainableAI

# Initialize explainer
explainer = VertexAIExplainableAI(
    project_id="iluminara-core",
    region="us-central1"
)

# Explain a prediction
explanation = explainer.explain_prediction(
    model_id="outbreak-predictor-v1",
    input_data={
        "cbs_signals": 45,
        "z_score": 3.8,
        "location": "Dadaab",
        "population_density": 12000,
        "water_access": 0.6
    },
    prediction="OUTBREAK_LIKELY",
    feature_names=["cbs_signals", "z_score", "location", "population_density", "water_access"]
)

# Get SHAP values
shap_values = explanation.shap_values
print(f"SHAP values: {shap_values}")

# Get top contributors
top_factors = explanation.get_top_contributors(n=3)
for factor in top_factors:
    print(f"{factor['feature']}: {factor['contribution']:.3f}")

# Output:
# z_score: 0.412
# cbs_signals: 0.287
# population_density: 0.156
```

## Explanation types

### 1. Feature importance

Global explanation showing which features matter most across all predictions.

```python
# Get feature importance
importance = explainer.get_feature_importance(
    model_id="outbreak-predictor-v1"
)

for feature, score in importance.items():
    print(f"{feature}: {score:.3f}")
```

### 2. SHAP values

Local explanation showing how each feature contributed to a specific prediction.

```python
# SHAP values for a single prediction
shap_values = explanation.shap_values

# Visualize (if using matplotlib)
import matplotlib.pyplot as plt
explainer.plot_shap_waterfall(shap_values, feature_names)
```

### 3. Counterfactual explanations

"What would need to change for a different outcome?"

```python
# Generate counterfactual
counterfactual = explainer.generate_counterfactual(
    model_id="outbreak-predictor-v1",
    input_data=input_data,
    desired_outcome="NO_OUTBREAK"
)

print(f"To avoid outbreak, change:")
for change in counterfactual.changes:
    print(f"  {change['feature']}: {change['from']} → {change['to']}")
```

## Integration with SovereignGuardrail

All high-risk inferences require SHAP explanations:

```python
from governance_kernel.vector_ledger import SovereignGuardrail
from governance_kernel.humanitarian_constraints import VertexAIExplainableAI

guardrail = SovereignGuardrail()
explainer = VertexAIExplainableAI()

# 1. Make prediction
prediction = model.predict(input_data)

# 2. Generate SHAP explanation
explanation = explainer.explain_prediction(
    model_id="risk-model",
    input_data=input_data,
    prediction=prediction,
    feature_names=feature_names
)

# 3. Validate with SovereignGuardrail
try:
    guardrail.validate_action(
        action_type="High_Risk_Inference",
        payload={
            "explanation": explanation.to_dict(),
            "confidence_score": 0.87,
            "evidence_chain": explanation.get_top_contributors(n=5),
            "consent_token": "TOKEN-123",
            "consent_scope": "diagnosis"
        },
        jurisdiction="EU_AI_ACT"
    )
    print("✅ Explanation meets EU AI Act requirements")
except SovereigntyViolationError as e:
    print(f"❌ Violation: {e}")
```

## Vertex AI deployment

### Prerequisites

1. Enable Vertex AI API:
```bash
gcloud services enable aiplatform.googleapis.com
```

2. Create service account:
```bash
gcloud iam service-accounts create vertex-ai-explainer \
  --display-name="Vertex AI Explainer"

gcloud projects add-iam-policy-binding ${PROJECT_ID} \
  --member="serviceAccount:vertex-ai-explainer@${PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/aiplatform.user"
```

### Deploy model with explainability

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project="iluminara-core",
    location="us-central1"
)

# Upload model with explainability
model = aiplatform.Model.upload(
    display_name="outbreak-predictor-v1",
    artifact_uri="gs://iluminara-models/outbreak-predictor",
    serving_container_image_uri="us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest",
    explanation_metadata={
        "inputs": {
            "cbs_signals": {"input_tensor_name": "cbs_signals"},
            "z_score": {"input_tensor_name": "z_score"},
            "location": {"input_tensor_name": "location"}
        },
        "outputs": {
            "prediction": {"output_tensor_name": "prediction"}
        }
    },
    explanation_parameters={
        "sampled_shapley_attribution": {
            "path_count": 10
        }
    }
)

# Deploy endpoint
endpoint = model.deploy(
    machine_type="n1-standard-4",
    min_replica_count=1,
    max_replica_count=10
)
```

## Explanation caching

All explanations are cached for audit trails:

```python
# Explanation is automatically cached
explanation = explainer.explain_prediction(
    model_id="risk-model",
    input_data=input_data,
    prediction=prediction,
    feature_names=feature_names
)

# Retrieve cached explanation
cached = explainer.get_explanation(
    decision_id=explanation.decision_id
)

# Generate audit report
audit_report = cached.to_dict()
print(json.dumps(audit_report, indent=2))
```

**Cache retention:** 30 days (configurable)

## Performance considerations

### Latency

- **SHAP calculation**: ~100-500ms per prediction
- **Feature importance**: ~1-2s (cached for 1 hour)
- **Counterfactual generation**: ~500ms-2s

### Optimization

```python
# Batch explanations
explanations = explainer.explain_batch(
    model_id="risk-model",
    input_batch=[input1, input2, input3],
    predictions=[pred1, pred2, pred3],
    feature_names=feature_names
)

# Use cached feature importance
importance = explainer.get_feature_importance(
    model_id="risk-model",
    use_cache=True
)
```

## Compliance validation

### EU AI Act §6 requirements

<Steps>
  <Step title="High-risk classification">
    Medical diagnosis, outbreak prediction, resource allocation
  </Step>
  <Step title="Explanation provision">
    SHAP values, feature importance, counterfactuals
  </Step>
  <Step title="Human oversight">
    Explanations enable informed human review
  </Step>
  <Step title="Record-keeping">
    All explanations cached for 30 days minimum
  </Step>
</Steps>

### GDPR Art. 22 requirements

<Steps>
  <Step title="Right to explanation">
    User can request explanation for any automated decision
  </Step>
  <Step title="Meaningful information">
    SHAP values provide actionable insights
  </Step>
  <Step title="Human intervention">
    Explanations enable human override
  </Step>
</Steps>

## Example: Outbreak prediction

```python
from governance_kernel.humanitarian_constraints import VertexAIExplainableAI
from edge_node.ai_agents import EpidemiologicalForecastingAgent

# Initialize agents
explainer = VertexAIExplainableAI()
forecaster = EpidemiologicalForecastingAgent(location="Dadaab")

# Make prediction
forecast = forecaster.forecast_outbreak(
    disease="cholera",
    historical_data=historical_cases,
    forecast_horizon_days=14
)

# Generate explanation
explanation = explainer.explain_prediction(
    model_id="cholera-forecaster",
    input_data={
        "historical_cases": historical_cases,
        "water_quality": 0.4,
        "sanitation_coverage": 0.6,
        "population_density": 12000,
        "rainfall": 150
    },
    prediction=f"R0={forecast.estimated_r0:.2f}",
    feature_names=["historical_cases", "water_quality", "sanitation_coverage", 
                   "population_density", "rainfall"]
)

# Display explanation
print(f"Outbreak forecast: R0 = {forecast.estimated_r0:.2f}")
print(f"\\nTop contributing factors:")
for factor in explanation.get_top_contributors(n=3):
    print(f"  {factor['feature']}: {factor['contribution']:.3f}")

# Output:
# Outbreak forecast: R0 = 2.8
# 
# Top contributing factors:
#   water_quality: 0.412
#   sanitation_coverage: 0.287
#   population_density: 0.156
```

## Testing

Run the SHAP integration tests:

```bash
python -m pytest tests/test_vertex_ai_shap.py -v
```

Run the example:

```bash
python governance_kernel/humanitarian_examples.py
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy autonomous surveillance agents
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Understand compliance enforcement
  </Card>
  <Card
    title="Humanitarian constraints"
    icon="heart"
    href="/governance/humanitarian"
  >
    Apply ethical constraints
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/gcp"
  >
    Deploy to Google Cloud Platform
  </Card>
</CardGroup>
