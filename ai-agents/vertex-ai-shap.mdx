---
title: Vertex AI + SHAP integration
description: Right to Explanation for high-risk clinical AI with explainable predictions
---

## Overview

iLuminara-Core integrates Google Cloud Vertex AI with SHAP (SHapley Additive exPlanations) to provide **Right to Explanation** for every high-risk clinical inference, ensuring compliance with EU AI Act §6 and GDPR Art. 22.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk inference automatically triggers SHAP analysis for explainability.
</Card>

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    VERTEX AI MODEL                          │
│  - AutoML Time-Series Forecasting                          │
│  - Custom Training (TensorFlow, PyTorch)                   │
│  - Batch/Online Prediction                                 │
└─────────────────────────────────────────────────────────────┘
                            ▼
┌─────────────────────────────────────────────────────────────┐
│                  SHAP EXPLAINER                             │
│  - TreeExplainer (XGBoost, Random Forest)                  │
│  - DeepExplainer (Neural Networks)                         │
│  - KernelExplainer (Model-agnostic)                        │
└─────────────────────────────────────────────────────────────┘
                            ▼
┌─────────────────────────────────────────────────────────────┐
│              SOVEREIGNGUARDRAIL VALIDATION                  │
│  - Confidence threshold check (>0.7 = high-risk)           │
│  - Explanation requirement enforcement                      │
│  - Evidence chain validation                               │
└─────────────────────────────────────────────────────────────┘
                            ▼
┌─────────────────────────────────────────────────────────────┐
│                  AUDIT TRAIL                                │
│  - Tamper-proof logging (Cloud Spanner)                    │
│  - Cryptographic signatures (Cloud KMS)                    │
│  - Compliance attestation                                  │
└─────────────────────────────────────────────────────────────┘
```

## High-risk inference detection

The SovereignGuardrail automatically detects high-risk inferences:

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# High-risk threshold: confidence > 0.7
if prediction_confidence > 0.7:
    # Requires SHAP explanation
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'cholera_diagnosis',
            'confidence_score': 0.92,
            'explanation': shap_values,
            'evidence_chain': ['fever', 'diarrhea', 'dehydration'],
            'consent_token': 'valid_token',
            'consent_scope': 'diagnosis'
        },
        jurisdiction='EU_AI_ACT'
    )
```

## Vertex AI model training

### AutoML time-series forecasting

```python
from google.cloud import aiplatform

aiplatform.init(project='iluminara-core', location='us-central1')

# Create dataset
dataset = aiplatform.TimeSeriesDataset.create(
    display_name='cholera_outbreak_forecast',
    gcs_source='gs://iluminara-data/outbreak_timeseries.csv',
    time_column='timestamp',
    time_series_identifier_column='location',
    target_column='case_count'
)

# Train AutoML model
job = aiplatform.AutoMLForecastingTrainingJob(
    display_name='cholera_forecast_model',
    optimization_objective='minimize-rmse',
    column_transformations=[
        {'numeric': {'column_name': 'temperature'}},
        {'numeric': {'column_name': 'rainfall'}},
        {'numeric': {'column_name': 'population_density'}}
    ]
)

model = job.run(
    dataset=dataset,
    target_column='case_count',
    time_column='timestamp',
    time_series_identifier_column='location',
    forecast_horizon=72,  # 72-hour forecast
    data_granularity_unit='hour',
    data_granularity_count=1,
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1
)

print(f"Model resource name: {model.resource_name}")
```

### Custom training with explainability

```python
from google.cloud import aiplatform
from google.cloud.aiplatform import gapic as aip

# Define custom training job
job = aiplatform.CustomTrainingJob(
    display_name='cholera_risk_classifier',
    script_path='training/train_model.py',
    container_uri='gcr.io/cloud-aiplatform/training/tf-cpu.2-12:latest',
    requirements=['shap', 'scikit-learn', 'xgboost'],
    model_serving_container_image_uri='gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-12:latest'
)

# Train with explainability
model = job.run(
    dataset=dataset,
    replica_count=1,
    machine_type='n1-standard-4',
    accelerator_type='NVIDIA_TESLA_T4',
    accelerator_count=1,
    args=[
        '--epochs=100',
        '--batch-size=32',
        '--learning-rate=0.001',
        '--enable-shap=true'
    ]
)
```

## SHAP integration

### TreeExplainer (XGBoost, Random Forest)

```python
import shap
import xgboost as xgb
from google.cloud import aiplatform

# Load model from Vertex AI
endpoint = aiplatform.Endpoint('projects/123/locations/us-central1/endpoints/456')

# Load training data for SHAP baseline
X_train = load_training_data()

# Create SHAP explainer
explainer = shap.TreeExplainer(model)

# Generate SHAP values for prediction
X_test = [[38.5, 95, 1, 1, 0]]  # [temp, heart_rate, diarrhea, vomiting, fever]
shap_values = explainer.shap_values(X_test)

# Visualize
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    X_test[0],
    feature_names=['temperature', 'heart_rate', 'diarrhea', 'vomiting', 'fever']
)
```

### DeepExplainer (Neural Networks)

```python
import shap
import tensorflow as tf
from google.cloud import aiplatform

# Load model
model = tf.keras.models.load_model('gs://iluminara-models/cholera_nn')

# Create SHAP explainer with background data
background = X_train[:100]  # Use 100 samples as background
explainer = shap.DeepExplainer(model, background)

# Generate SHAP values
shap_values = explainer.shap_values(X_test)

# Visualize
shap.summary_plot(shap_values, X_test, feature_names=feature_names)
```

### KernelExplainer (Model-agnostic)

```python
import shap

# Define prediction function
def predict_fn(X):
    instances = [{'features': x.tolist()} for x in X]
    predictions = endpoint.predict(instances=instances)
    return predictions.predictions

# Create SHAP explainer
explainer = shap.KernelExplainer(predict_fn, X_train[:50])

# Generate SHAP values
shap_values = explainer.shap_values(X_test)

# Visualize
shap.waterfall_plot(shap.Explanation(
    values=shap_values[0],
    base_values=explainer.expected_value,
    data=X_test[0],
    feature_names=feature_names
))
```

## Complete inference pipeline

```python
from google.cloud import aiplatform
from governance_kernel.vector_ledger import SovereignGuardrail, SovereigntyViolationError
import shap
import numpy as np

class ExplainableInferenceEngine:
    """
    Inference engine with mandatory explainability for high-risk predictions.
    """
    
    def __init__(self, endpoint_id: str, jurisdiction: str = 'EU_AI_ACT'):
        self.endpoint = aiplatform.Endpoint(endpoint_id)
        self.guardrail = SovereignGuardrail()
        self.jurisdiction = jurisdiction
        self.high_risk_threshold = 0.7
        
        # Load SHAP explainer
        self.explainer = self._load_explainer()
    
    def predict_with_explanation(
        self,
        features: np.ndarray,
        patient_id: str,
        consent_token: str
    ) -> dict:
        """
        Make prediction with mandatory SHAP explanation for high-risk cases.
        """
        # Make prediction
        instances = [{'features': features.tolist()}]
        prediction = self.endpoint.predict(instances=instances)
        
        confidence = prediction.predictions[0]['confidence']
        diagnosis = prediction.predictions[0]['diagnosis']
        
        # Check if high-risk
        if confidence > self.high_risk_threshold:
            # Generate SHAP explanation
            shap_values = self.explainer.shap_values(features.reshape(1, -1))
            
            # Extract evidence chain
            evidence_chain = self._extract_evidence_chain(shap_values[0], features)
            
            # Validate with SovereignGuardrail
            try:
                self.guardrail.validate_action(
                    action_type='High_Risk_Inference',
                    payload={
                        'inference': diagnosis,
                        'confidence_score': confidence,
                        'explanation': shap_values.tolist(),
                        'evidence_chain': evidence_chain,
                        'patient_id': patient_id,
                        'consent_token': consent_token,
                        'consent_scope': 'diagnosis'
                    },
                    jurisdiction=self.jurisdiction
                )
            except SovereigntyViolationError as e:
                raise ValueError(f"High-risk inference rejected: {e}")
            
            return {
                'diagnosis': diagnosis,
                'confidence': confidence,
                'risk_level': 'HIGH',
                'explanation': {
                    'shap_values': shap_values.tolist(),
                    'evidence_chain': evidence_chain,
                    'feature_importance': self._get_feature_importance(shap_values[0])
                },
                'compliance': {
                    'framework': self.jurisdiction,
                    'validated': True,
                    'timestamp': datetime.utcnow().isoformat()
                }
            }
        else:
            # Low-risk prediction (no explanation required)
            return {
                'diagnosis': diagnosis,
                'confidence': confidence,
                'risk_level': 'LOW',
                'explanation': None,
                'compliance': {
                    'framework': self.jurisdiction,
                    'validated': True,
                    'timestamp': datetime.utcnow().isoformat()
                }
            }
    
    def _extract_evidence_chain(self, shap_values: np.ndarray, features: np.ndarray) -> list:
        """Extract top contributing features as evidence chain."""
        feature_names = ['temperature', 'heart_rate', 'diarrhea', 'vomiting', 'fever']
        
        # Get top 3 contributing features
        top_indices = np.argsort(np.abs(shap_values))[-3:][::-1]
        
        evidence = []
        for idx in top_indices:
            evidence.append({
                'feature': feature_names[idx],
                'value': float(features[idx]),
                'contribution': float(shap_values[idx])
            })
        
        return evidence
    
    def _get_feature_importance(self, shap_values: np.ndarray) -> dict:
        """Get feature importance scores."""
        feature_names = ['temperature', 'heart_rate', 'diarrhea', 'vomiting', 'fever']
        
        importance = {}
        for i, name in enumerate(feature_names):
            importance[name] = float(np.abs(shap_values[i]))
        
        return importance
    
    def _load_explainer(self):
        """Load SHAP explainer (placeholder)."""
        # In production, load from Cloud Storage
        return None

# Usage
engine = ExplainableInferenceEngine(
    endpoint_id='projects/123/locations/us-central1/endpoints/456',
    jurisdiction='EU_AI_ACT'
)

# Make prediction with explanation
result = engine.predict_with_explanation(
    features=np.array([38.5, 95, 1, 1, 0]),
    patient_id='PAT_12345',
    consent_token='valid_token'
)

print(f"Diagnosis: {result['diagnosis']}")
print(f"Confidence: {result['confidence']:.2%}")
print(f"Risk Level: {result['risk_level']}")

if result['explanation']:
    print(f"Evidence Chain: {result['explanation']['evidence_chain']}")
```

## Batch prediction with explainability

```python
from google.cloud import aiplatform

# Create batch prediction job
batch_prediction_job = aiplatform.BatchPredictionJob.create(
    job_display_name='cholera_risk_batch',
    model_name='projects/123/locations/us-central1/models/456',
    instances_format='jsonl',
    predictions_format='jsonl',
    gcs_source='gs://iluminara-data/batch_input.jsonl',
    gcs_destination_prefix='gs://iluminara-predictions/',
    machine_type='n1-standard-4',
    accelerator_type='NVIDIA_TESLA_T4',
    accelerator_count=1,
    generate_explanation=True,  # Enable SHAP explanations
    explanation_metadata={
        'inputs': {
            'temperature': {'input_tensor_name': 'temperature'},
            'heart_rate': {'input_tensor_name': 'heart_rate'},
            'diarrhea': {'input_tensor_name': 'diarrhea'},
            'vomiting': {'input_tensor_name': 'vomiting'},
            'fever': {'input_tensor_name': 'fever'}
        },
        'outputs': {
            'diagnosis': {'output_tensor_name': 'diagnosis'}
        }
    }
)

# Wait for completion
batch_prediction_job.wait()

print(f"Batch prediction complete: {batch_prediction_job.resource_name}")
```

## Compliance reporting

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail(enable_tamper_proof_audit=True)

# Get audit history for high-risk inferences
audit_history = guardrail.get_tamper_proof_audit_history(
    action_type='High_Risk_Inference',
    limit=100
)

# Generate compliance report
report = {
    'total_inferences': len(audit_history),
    'high_risk_count': sum(1 for a in audit_history if a['payload'].get('confidence_score', 0) > 0.7),
    'explained_count': sum(1 for a in audit_history if 'explanation' in a['payload']),
    'compliance_rate': 1.0,  # 100% if all high-risk have explanations
    'frameworks': ['EU_AI_ACT', 'GDPR_ART_22'],
    'period': {
        'start': audit_history[-1]['timestamp'],
        'end': audit_history[0]['timestamp']
    }
}

print(f"Compliance Report: {report}")
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="Deploy to Vertex AI"
    icon="google"
    href="/deployment/gcp"
  >
    Deploy models to production
  </Card>
  <Card
    title="SovereignGuardrail"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Integrate with autonomous agents
  </Card>
  <Card
    title="Audit trail"
    icon="file-contract"
    href="/governance/audit"
  >
    Configure tamper-proof logging
  </Card>
</CardGroup>
