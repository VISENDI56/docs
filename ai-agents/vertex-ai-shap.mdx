---
title: Vertex AI + SHAP integration
description: Right to Explanation with explainable AI for high-risk clinical inferences
---

## Overview

iLuminara integrates **Google Cloud Vertex AI** with **SHAP (SHapley Additive exPlanations)** to provide the "Right to Explanation" required by EU AI Act Â§6 and GDPR Art. 22.

Every high-risk clinical inference automatically triggers explainability analysis, ensuring compliance with global AI ethics frameworks.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  EU AI Act Â§6 â€¢ GDPR Art. 22 â€¢ UNESCO AI Ethics â€¢ OECD AI Principles
</Card>

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VERTEX AI PIPELINE                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Model Training (AutoML / Custom)                   â”‚  â”‚
â”‚  â”‚     - Time-series forecasting                          â”‚  â”‚
â”‚  â”‚     - Disease classification                           â”‚  â”‚
â”‚  â”‚     - Outbreak prediction                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  2. Model Deployment (Endpoint)                        â”‚  â”‚
â”‚  â”‚     - Real-time prediction                             â”‚  â”‚
â”‚  â”‚     - Batch prediction                                 â”‚  â”‚
â”‚  â”‚     - Online serving                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  3. Prediction Request                                 â”‚  â”‚
â”‚  â”‚     - Input: Patient symptoms, location, history       â”‚  â”‚
â”‚  â”‚     - Output: Diagnosis, confidence score              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SHAP EXPLAINABILITY                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  4. SHAP Analysis (if confidence > 0.7)                â”‚  â”‚
â”‚  â”‚     - TreeExplainer for tree-based models              â”‚  â”‚
â”‚  â”‚     - KernelExplainer for black-box models             â”‚  â”‚
â”‚  â”‚     - DeepExplainer for neural networks                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  5. Explanation Generation                             â”‚  â”‚
â”‚  â”‚     - Feature importance (SHAP values)                 â”‚  â”‚
â”‚  â”‚     - Force plot visualization                         â”‚  â”‚
â”‚  â”‚     - Waterfall chart                                  â”‚  â”‚
â”‚  â”‚     - Decision path                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GOVERNANCE VALIDATION                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  6. SovereignGuardrail Check                           â”‚  â”‚
â”‚  â”‚     - Verify explanation completeness                  â”‚  â”‚
â”‚  â”‚     - Validate evidence chain                          â”‚  â”‚
â”‚  â”‚     - Log to tamper-proof audit trail                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## High-risk AI classification

Per EU AI Act Â§6, iLuminara classifies AI systems as high-risk if they meet any of these criteria:

<AccordionGroup>
  <Accordion title="Clinical diagnosis">
    AI systems that provide or influence medical diagnoses
  </Accordion>
  <Accordion title="Treatment recommendations">
    AI systems that recommend treatment plans or interventions
  </Accordion>
  <Accordion title="Resource allocation">
    AI systems that allocate scarce medical resources (beds, ventilators, vaccines)
  </Accordion>
  <Accordion title="Outbreak prediction">
    AI systems that predict disease outbreaks and trigger emergency responses
  </Accordion>
  <Accordion title="Triage decisions">
    AI systems that prioritize patients for treatment
  </Accordion>
</AccordionGroup>

**Threshold:** Any prediction with confidence score > 0.7 is automatically classified as high-risk and requires SHAP explanation.

## Implementation

### 1. Train model on Vertex AI

```python
from google.cloud import aiplatform

aiplatform.init(project="iluminara-core", location="africa-south1")

# Create AutoML time-series forecasting job
dataset = aiplatform.TimeSeriesDataset.create(
    display_name="cholera_outbreak_forecast",
    gcs_source="gs://iluminara-data/cholera_cases.csv",
    time_column="date",
    time_series_identifier_column="location",
    target_column="case_count"
)

job = aiplatform.AutoMLForecastingTrainingJob(
    display_name="cholera_forecast_model",
    optimization_objective="minimize-rmse",
    column_specs={
        "temperature": "numeric",
        "rainfall": "numeric",
        "population_density": "numeric"
    }
)

model = job.run(
    dataset=dataset,
    target_column="case_count",
    time_column="date",
    time_series_identifier_column="location",
    forecast_horizon=14,  # 14-day forecast
    data_granularity_unit="day",
    data_granularity_count=1,
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1
)

print(f"Model trained: {model.resource_name}")
```

### 2. Deploy model endpoint

```python
# Deploy model to endpoint
endpoint = model.deploy(
    deployed_model_display_name="cholera_forecast_v1",
    machine_type="n1-standard-4",
    min_replica_count=1,
    max_replica_count=10,
    accelerator_type="NVIDIA_TESLA_T4",
    accelerator_count=1
)

print(f"Endpoint deployed: {endpoint.resource_name}")
```

### 3. Make prediction with SHAP explanation

```python
import shap
import numpy as np
from governance_kernel.vector_ledger import SovereignGuardrail

# Initialize SHAP explainer
explainer = shap.TreeExplainer(model)

# Make prediction
instances = [{
    "location": "Dadaab",
    "temperature": 32.5,
    "rainfall": 15.2,
    "population_density": 850,
    "previous_cases": [12, 15, 18, 22, 28]
}]

prediction = endpoint.predict(instances=instances)
confidence_score = prediction.predictions[0]["confidence"]

# If high-risk, generate SHAP explanation
if confidence_score > 0.7:
    # Calculate SHAP values
    shap_values = explainer.shap_values(np.array(instances))
    
    # Generate explanation
    explanation = {
        "prediction": prediction.predictions[0],
        "confidence_score": confidence_score,
        "shap_values": shap_values.tolist(),
        "feature_importance": {
            "temperature": float(shap_values[0][0]),
            "rainfall": float(shap_values[0][1]),
            "population_density": float(shap_values[0][2]),
            "previous_cases": float(shap_values[0][3])
        },
        "evidence_chain": [
            f"High temperature ({instances[0]['temperature']}Â°C) increases transmission",
            f"Recent rainfall ({instances[0]['rainfall']}mm) creates breeding sites",
            f"Dense population ({instances[0]['population_density']}/kmÂ²) accelerates spread",
            f"Rising case trend: {instances[0]['previous_cases']}"
        ]
    }
    
    # Validate with SovereignGuardrail
    guardrail = SovereignGuardrail(enable_tamper_proof_audit=True)
    
    try:
        guardrail.validate_action(
            action_type='High_Risk_Inference',
            payload={
                'actor': 'vertex_ai_model',
                'resource': 'cholera_outbreak_prediction',
                'explanation': str(explanation),
                'confidence_score': confidence_score,
                'evidence_chain': explanation['evidence_chain'],
                'consent_token': 'public_health_surveillance',
                'consent_scope': 'outbreak_prediction'
            },
            jurisdiction='EU_AI_ACT'
        )
        
        print("âœ… High-risk inference validated and logged")
        print(f"Explanation: {explanation}")
        
    except Exception as e:
        print(f"âŒ Governance violation: {e}")
```

### 4. Visualize SHAP explanation

```python
import matplotlib.pyplot as plt

# Force plot (individual prediction)
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    instances[0],
    matplotlib=True
)
plt.savefig("shap_force_plot.png")

# Waterfall chart
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values[0],
        base_values=explainer.expected_value,
        data=instances[0]
    )
)
plt.savefig("shap_waterfall.png")

# Summary plot (global feature importance)
shap.summary_plot(shap_values, instances, plot_type="bar")
plt.savefig("shap_summary.png")
```

## Explainability methods

iLuminara supports multiple SHAP explainers based on model type:

### TreeExplainer (tree-based models)

```python
import xgboost as xgb

# Train XGBoost model
model = xgb.XGBClassifier()
model.fit(X_train, y_train)

# Create TreeExplainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)
```

**Use cases:** XGBoost, LightGBM, CatBoost, Random Forest

### KernelExplainer (black-box models)

```python
# Create KernelExplainer for any model
explainer = shap.KernelExplainer(
    model.predict_proba,
    shap.sample(X_train, 100)  # Background dataset
)
shap_values = explainer.shap_values(X_test)
```

**Use cases:** Any model (slower but model-agnostic)

### DeepExplainer (neural networks)

```python
import tensorflow as tf

# Create DeepExplainer for TensorFlow model
explainer = shap.DeepExplainer(
    model,
    X_train[:100]  # Background dataset
)
shap_values = explainer.shap_values(X_test)
```

**Use cases:** TensorFlow, Keras, PyTorch

## Compliance validation

Every high-risk inference is validated against multiple frameworks:

```python
from governance_kernel.compliance_matrix import ComplianceMatrix, SectoralContext

matrix = ComplianceMatrix()

# Validate AI inference
result = matrix.check_sectoral_compliance(
    context=SectoralContext.AI_ETHICS,
    payload={
        "model_type": "outbreak_prediction",
        "confidence_score": 0.95,
        "explanation_provided": True,
        "shap_values": shap_values.tolist(),
        "evidence_chain": explanation['evidence_chain'],
        "human_oversight": True
    }
)

if result["status"] == "COMPLIANT":
    print("âœ… AI inference compliant with:")
    for framework in result["frameworks_checked"]:
        print(f"   - {framework}")
else:
    print("âŒ Compliance violations:")
    for violation in result["violations"]:
        print(f"   - {violation['framework']}: {violation['message']}")
```

**Frameworks checked:**
- EU AI Act Â§6 (High-Risk AI)
- EU AI Act Â§13 (Transparency)
- GDPR Art. 22 (Automated Decision-Making)
- UNESCO AI Ethics Recommendation
- OECD AI Principles
- IEEE Ethically Aligned Design

## Real-world example: Cholera outbreak prediction

```python
from cloud_oracle.vertex_ai_integration import VertexAIPredictor
from governance_kernel.vector_ledger import SovereignGuardrail

# Initialize predictor
predictor = VertexAIPredictor(
    project="iluminara-core",
    location="africa-south1",
    endpoint_id="cholera_forecast_endpoint"
)

# Predict outbreak risk
result = predictor.predict_with_explanation(
    location="Dadaab Refugee Camp",
    features={
        "temperature": 34.2,
        "rainfall": 22.5,
        "population_density": 1200,
        "water_quality_index": 0.45,
        "sanitation_coverage": 0.62,
        "previous_cases_7d": [8, 12, 15, 18, 24, 31, 42]
    }
)

# Output:
# {
#   "prediction": {
#     "outbreak_probability": 0.87,
#     "expected_cases_14d": 156,
#     "peak_date": "2025-01-25",
#     "confidence": 0.92
#   },
#   "explanation": {
#     "shap_values": {
#       "temperature": 0.15,
#       "rainfall": 0.22,
#       "population_density": 0.18,
#       "water_quality_index": 0.28,
#       "sanitation_coverage": -0.12,
#       "case_trend": 0.31
#     },
#     "top_factors": [
#       "Rising case trend (+31% in 7 days)",
#       "Poor water quality (index: 0.45)",
#       "Heavy rainfall (22.5mm) creating contamination risk"
#     ],
#     "evidence_chain": [
#       "Cases doubled in past week (8 â†’ 42)",
#       "Water quality below WHO threshold (0.45 < 0.70)",
#       "Rainfall exceeded 20mm (cholera transmission threshold)",
#       "High population density (1200/kmÂ²) accelerates spread"
#     ]
#   },
#   "governance": {
#     "frameworks_validated": [
#       "EU AI Act Â§6",
#       "GDPR Art. 22",
#       "WHO IHR Art. 6"
#     ],
#     "audit_log_id": "audit_20250128_001234",
#     "compliance_status": "APPROVED"
#   }
# }
```

## Integration with dashboard

SHAP explanations are automatically displayed in the iLuminara Command Console:

```python
import streamlit as st
import shap

# Display SHAP force plot in Streamlit
st.title("ğŸ” AI Explanation Dashboard")

st.subheader("Prediction")
st.metric("Outbreak Probability", f"{result['prediction']['outbreak_probability']:.1%}")
st.metric("Expected Cases (14d)", result['prediction']['expected_cases_14d'])

st.subheader("Explanation (SHAP Values)")
fig = shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    features,
    matplotlib=True
)
st.pyplot(fig)

st.subheader("Evidence Chain")
for evidence in result['explanation']['evidence_chain']:
    st.write(f"â€¢ {evidence}")

st.subheader("Compliance Status")
st.success(f"âœ… Validated against {len(result['governance']['frameworks_validated'])} frameworks")
```

## Performance optimization

### Batch prediction with SHAP

```python
# Batch predict with explanations
batch_instances = [...]  # 1000 instances

# Use Vertex AI batch prediction
batch_job = aiplatform.BatchPredictionJob.create(
    job_display_name="cholera_batch_forecast",
    model_name=model.resource_name,
    instances_format="jsonl",
    gcs_source="gs://iluminara-data/batch_input.jsonl",
    gcs_destination_prefix="gs://iluminara-data/batch_output/",
    machine_type="n1-standard-16",
    accelerator_type="NVIDIA_TESLA_T4",
    accelerator_count=2
)

# Generate SHAP explanations in parallel
from concurrent.futures import ThreadPoolExecutor

def explain_prediction(instance):
    shap_values = explainer.shap_values(instance)
    return {
        "instance": instance,
        "shap_values": shap_values.tolist()
    }

with ThreadPoolExecutor(max_workers=10) as executor:
    explanations = list(executor.map(explain_prediction, batch_instances))
```

### Caching SHAP explainers

```python
from functools import lru_cache

@lru_cache(maxsize=10)
def get_explainer(model_id: str):
    """Cache SHAP explainers to avoid recomputation"""
    model = load_model(model_id)
    return shap.TreeExplainer(model)

# Reuse cached explainer
explainer = get_explainer("cholera_forecast_v1")
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy autonomous surveillance agents
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Understand compliance enforcement
  </Card>
  <Card
    title="Cloud Oracle"
    icon="cloud"
    href="/architecture/overview"
  >
    Multi-scale outbreak forecasting
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/gcp"
  >
    Deploy to Google Cloud Platform
  </Card>
</CardGroup>
