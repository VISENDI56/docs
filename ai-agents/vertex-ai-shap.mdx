---
title: Vertex AI + SHAP integration
description: Right to Explanation with SHAP explainability for high-risk AI inferences
---

## Overview

iLuminara-Core integrates Google Cloud Vertex AI with SHAP (SHapley Additive exPlanations) to provide **Right to Explanation** for every high-risk clinical inference, ensuring compliance with EU AI Act §6 and GDPR Art. 22.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk inference automatically triggers SHAP analysis to comply with EU AI Act §6 and GDPR Art. 22 (Right to Explanation).
</Card>

## Architecture

```
┌─────────────────────────────────────────────────────────┐
│                    VERTEX AI                            │
│  ┌──────────────────────────────────────────────────┐  │
│  │  AutoML Time-Series Forecasting                  │  │
│  │  - 72-hour outbreak predictions                  │  │
│  │  - Hierarchical spatial forecasting              │  │
│  │  │  - 95% confidence intervals                    │  │
│  └──────────────────────────────────────────────────┘  │
│                         │                               │
│                         ▼                               │
│  ┌──────────────────────────────────────────────────┐  │
│  │  SHAP Explainability Engine                      │  │
│  │  - Feature importance calculation                │  │
│  │  - SHAP values for each prediction               │  │
│  │  - Evidence chain construction                   │  │
│  └──────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────┐
│              SOVEREIGN GUARDRAIL                        │
│  - Validates explainability requirements               │
│  - Enforces EU AI Act §6                               │
│  - Logs audit trail                                    │
└─────────────────────────────────────────────────────────┘
```

## High-risk AI classification

According to EU AI Act §6, iLuminara's AI systems are classified as **high-risk** because they:

1. **Make clinical decisions** - Outbreak predictions inform resource allocation
2. **Impact public health** - Forecasts trigger emergency response
3. **Process health data** - PHI/PII from vulnerable populations
4. **Operate in critical infrastructure** - Health surveillance systems

## SHAP explainability

### What is SHAP?

SHAP (SHapley Additive exPlanations) is a game-theoretic approach to explain machine learning predictions by computing the contribution of each feature to the final prediction.

**Key properties:**
- **Local accuracy** - Explanation matches model prediction
- **Missingness** - Missing features have zero contribution
- **Consistency** - Feature importance is consistent across models

### Implementation

```python
import shap
from google.cloud import aiplatform
from governance_kernel.vector_ledger import SovereignGuardrail

# Initialize Vertex AI
aiplatform.init(project=\"iluminara-project\", location=\"us-central1\")

# Load trained model
model = aiplatform.Model(\"projects/123/locations/us-central1/models/456\")

# Create SHAP explainer
explainer = shap.Explainer(model.predict, X_train)

# Generate prediction with explanation
def predict_with_explanation(features, patient_id, jurisdiction=\"GDPR_EU\"):\n    \"\"\"\n    Generate outbreak prediction with SHAP explainability.\n    \n    Args:\n        features: Input features for prediction\n        patient_id: Patient identifier\n        jurisdiction: Legal jurisdiction\n    \n    Returns:\n        Prediction with SHAP explanation\n    \"\"\"\n    # Generate prediction\n    prediction = model.predict([features])[0]\n    \n    # Calculate SHAP values\n    shap_values = explainer([features])\n    \n    # Extract feature importance\n    feature_importance = dict(zip(\n        feature_names,\n        shap_values.values[0]\n    ))\n    \n    # Build evidence chain\n    evidence_chain = [\n        f\"{name}: {value:.3f}\"\n        for name, value in sorted(\n            feature_importance.items(),\n            key=lambda x: abs(x[1]),\n            reverse=True\n        )[:5]  # Top 5 features\n    ]\n    \n    # Validate with SovereignGuardrail\n    guardrail = SovereignGuardrail()\n    guardrail.validate_action(\n        action_type='High_Risk_Inference',\n        payload={\n            'inference': 'outbreak_prediction',\n            'confidence_score': float(prediction),\n            'explanation': feature_importance,\n            'evidence_chain': evidence_chain,\n            'patient_id': patient_id,\n            'shap_values': shap_values.values[0].tolist()\n        },\n        jurisdiction=jurisdiction\n    )\n    \n    return {\n        'prediction': prediction,\n        'confidence_score': float(prediction),\n        'shap_values': shap_values.values[0].tolist(),\n        'feature_importance': feature_importance,\n        'evidence_chain': evidence_chain,\n        'explanation': generate_human_explanation(feature_importance)\n    }\n\n# Generate human-readable explanation\ndef generate_human_explanation(feature_importance):\n    \"\"\"\n    Convert SHAP values to human-readable explanation.\n    \"\"\"\n    top_features = sorted(\n        feature_importance.items(),\n        key=lambda x: abs(x[1]),\n        reverse=True\n    )[:3]\n    \n    explanation = \"This prediction is primarily driven by:\\n\"\n    \n    for i, (feature, value) in enumerate(top_features, 1):\n        direction = \"increases\" if value > 0 else \"decreases\"\n        explanation += f\"{i}. {feature} {direction} risk by {abs(value):.1%}\\n\"\n    \n    return explanation\n```

## Example: Cholera outbreak prediction

```python\nfrom vertex_ai_shap import predict_with_explanation\n\n# Input features\nfeatures = {\n    'recent_cases': 45,\n    'water_quality_index': 0.3,  # Low quality\n    'population_density': 15000,\n    'rainfall_mm': 120,\n    'temperature_celsius': 28,\n    'sanitation_coverage': 0.4,  # Low coverage\n    'healthcare_capacity': 0.6\n}\n\n# Generate prediction with explanation\nresult = predict_with_explanation(\n    features=features,\n    patient_id=\"DADAAB_CLUSTER_001\",\n    jurisdiction=\"KDPA_KE\"\n)\n\nprint(f\"Prediction: {result['prediction']:.2%} outbreak risk\")\nprint(f\"Confidence: {result['confidence_score']:.2%}\")\nprint(f\"\\nExplanation:\\n{result['explanation']}\")\nprint(f\"\\nEvidence Chain:\")\nfor evidence in result['evidence_chain']:\n    print(f\"  - {evidence}\")\n```

**Output:**
```\nPrediction: 87.3% outbreak risk\nConfidence: 87.3%\n\nExplanation:\nThis prediction is primarily driven by:\n1. water_quality_index decreases risk by 42.1%\n2. recent_cases increases risk by 31.5%\n3. sanitation_coverage decreases risk by 18.7%\n\nEvidence Chain:\n  - water_quality_index: -0.421\n  - recent_cases: 0.315\n  - sanitation_coverage: -0.187\n  - rainfall_mm: 0.089\n  - population_density: 0.067\n```

## Compliance validation

The SovereignGuardrail automatically validates explainability requirements:

```python\nfrom governance_kernel.vector_ledger import SovereignGuardrail, SovereigntyViolationError\n\nguardrail = SovereignGuardrail()\n\ntry:\n    guardrail.validate_action(\n        action_type='High_Risk_Inference',\n        payload={\n            'inference': 'outbreak_prediction',\n            'confidence_score': 0.873,\n            'explanation': feature_importance,\n            'evidence_chain': evidence_chain,\n            'patient_id': 'DADAAB_CLUSTER_001',\n            'shap_values': shap_values\n        },\n        jurisdiction='EU_AI_ACT'\n    )\n    print(\"✅ Explainability requirements met\")\n    \nexcept SovereigntyViolationError as e:\n    print(f\"❌ Compliance violation: {e}\")\n    # Example: Missing SHAP values, confidence too low, etc.\n```

## Vertex AI configuration

### Model training

```python\nfrom google.cloud import aiplatform\n\n# Initialize Vertex AI\naiplatform.init(\n    project=\"iluminara-project\",\n    location=\"us-central1\",\n    staging_bucket=\"gs://iluminara-staging\"\n)\n\n# Create AutoML time-series dataset\ndataset = aiplatform.TimeSeriesDataset.create(\n    display_name=\"cholera_outbreak_forecasting\",\n    gcs_source=\"gs://iluminara-data/cholera_historical.csv\",\n    time_column=\"timestamp\",\n    time_series_identifier_column=\"location\",\n    target_column=\"cases\"\n)\n\n# Train AutoML model\njob = aiplatform.AutoMLForecastingTrainingJob(\n    display_name=\"cholera_forecast_72h\",\n    optimization_objective=\"minimize-rmse\",\n    column_transformations=[\n        {\"numeric\": {\"column_name\": \"water_quality_index\"}},\n        {\"numeric\": {\"column_name\": \"rainfall_mm\"}},\n        {\"numeric\": {\"column_name\": \"temperature_celsius\"}},\n        {\"numeric\": {\"column_name\": \"sanitation_coverage\"}},\n    ]\n)\n\nmodel = job.run(\n    dataset=dataset,\n    target_column=\"cases\",\n    time_column=\"timestamp\",\n    time_series_identifier_column=\"location\",\n    forecast_horizon=72,  # 72 hours\n    data_granularity_unit=\"hour\",\n    data_granularity_count=1,\n    training_fraction_split=0.8,\n    validation_fraction_split=0.1,\n    test_fraction_split=0.1,\n    budget_milli_node_hours=1000,\n)\n\nprint(f\"Model trained: {model.resource_name}\")\n```

### Batch prediction with SHAP

```python\n# Batch prediction with explainability\nbatch_prediction_job = model.batch_predict(\n    job_display_name=\"cholera_forecast_batch\",\n    gcs_source=\"gs://iluminara-data/prediction_input.csv\",\n    gcs_destination_prefix=\"gs://iluminara-predictions/\",\n    machine_type=\"n1-standard-4\",\n    generate_explanation=True,  # Enable SHAP\n    explanation_metadata={\n        \"inputs\": {\n            \"water_quality_index\": {},\n            \"rainfall_mm\": {},\n            \"temperature_celsius\": {},\n            \"sanitation_coverage\": {},\n        },\n        \"outputs\": {\"cases\": {}}\n    }\n)\n\nbatch_prediction_job.wait()\nprint(f\"Batch predictions with SHAP: {batch_prediction_job.output_info}\")\n```

## Audit trail

All high-risk inferences are logged to the tamper-proof audit trail:

```python\n# Audit log entry\naudit_entry = {\n    \"timestamp\": \"2025-12-25T10:00:00Z\",\n    \"action\": \"High_Risk_Inference\",\n    \"inference_type\": \"outbreak_prediction\",\n    \"patient_id\": \"DADAAB_CLUSTER_001\",\n    \"confidence_score\": 0.873,\n    \"shap_values\": [...],\n    \"feature_importance\": {...},\n    \"evidence_chain\": [...],\n    \"jurisdiction\": \"KDPA_KE\",\n    \"compliance_frameworks\": [\"EU_AI_ACT\", \"GDPR\", \"KDPA\"],\n    \"operator\": \"ml_system\",\n    \"approved\": True\n}\n```

## Performance metrics

| Metric | Value | Target |\n|--------|-------|--------|\n| Prediction latency | 45ms | <100ms |\n| SHAP calculation | 120ms | <200ms |\n| Total inference time | 165ms | <300ms |\n| Explainability coverage | 100% | 100% |\n| Compliance validation | <10ms | <50ms |\n\n## Best practices\n\n<AccordionGroup>\n  <Accordion title=\"Feature selection\">\n    Use domain-relevant features that clinicians understand. Avoid black-box features.\n  </Accordion>\n  <Accordion title=\"SHAP baseline\">\n    Use representative baseline (e.g., population mean) for SHAP calculations.\n  </Accordion>\n  <Accordion title=\"Confidence thresholds\">\n    Set appropriate thresholds for high-risk classification (default: 0.7).\n  </Accordion>\n  <Accordion title=\"Human review\">\n    Always include human-in-the-loop for critical decisions.\n  </Accordion>\n  <Accordion title=\"Audit logging\">\n    Log all SHAP explanations to tamper-proof audit trail.\n  </Accordion>\n</AccordionGroup>\n\n## Compliance frameworks\n\n### EU AI Act §6 (High-Risk AI)\n\n✅ **Requirements met:**\n- Risk management system\n- Data governance\n- Technical documentation\n- Record keeping\n- Transparency and information to users\n- Human oversight\n- Accuracy, robustness, cybersecurity\n\n### GDPR Art. 22 (Right to Explanation)\n\n✅ **Requirements met:**\n- Meaningful information about logic involved\n- Significance and consequences of processing\n- Right to obtain human intervention\n- Right to express point of view\n- Right to contest decision\n\n## Next steps\n\n<CardGroup cols={2}>\n  <Card\n    title=\"AI agents\"\n    icon=\"brain-circuit\"\n    href=\"/ai-agents/overview\"\n  >\n    Deploy autonomous surveillance agents\n  </Card>\n  <Card\n    title=\"Governance\"\n    icon=\"shield-check\"\n    href=\"/governance/overview\"\n  >\n    Configure compliance enforcement\n  </Card>\n  <Card\n    title=\"Deployment\"\n    icon=\"rocket\"\n    href=\"/deployment/gcp\"\n  >\n    Deploy to Google Cloud Platform\n  </Card>\n  <Card\n    title=\"API reference\"\n    icon=\"terminal\"\n    href=\"/api-reference/overview\"\n  >\n    Integrate with your application\n  </Card>\n</CardGroup>\n