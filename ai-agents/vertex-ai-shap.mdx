---
title: Vertex AI + SHAP integration
description: Right to Explanation enforcement with SHAP explainability for high-risk AI inferences
---

## Overview

iLuminara-Core integrates **Vertex AI** with **SHAP (SHapley Additive exPlanations)** to enforce the "Right to Explanation" mandated by the EU AI Act §6 and GDPR Art. 22.

Every high-risk clinical inference automatically triggers explainability analysis, ensuring compliance with global AI governance frameworks.

<Card
  title="Philosophy"
  icon="lightbulb"
>
  "Every clinical decision with consequence must be explicable."
</Card>

## Compliance requirements

### EU AI Act §6 (High-Risk AI)

High-risk AI systems in healthcare require:
- **Transparency** - Clear explanation of decision logic
- **Human oversight** - Ability to override AI decisions
- **Accuracy** - Documented performance metrics
- **Robustness** - Resilience to errors and attacks

### GDPR Art. 22 (Automated Decision-Making)

Data subjects have the right to:
- **Explanation** - Understand the logic behind automated decisions
- **Human intervention** - Request human review
- **Contest** - Challenge automated decisions

## Architecture

```
┌──────────────────────────────────────────────────────────┐
│                    VERTEX AI MODEL                        │
│  (AutoML, Custom Training, Pre-trained Models)           │
└──────────────────────────────────────────────────────────┘
                            │
                            │ Prediction
                            ▼
┌──────────────────────────────────────────────────────────┐
│                  SHAP EXPLAINER                          │
│  - TreeExplainer (XGBoost, LightGBM, CatBoost)          │
│  - DeepExplainer (Neural Networks)                       │
│  - KernelExplainer (Model-agnostic)                      │
└──────────────────────────────────────────────────────────┘
                            │
                            │ Explanation
                            ▼
┌──────────────────────────────────────────────────────────┐
│              SOVEREIGN GUARDRAIL                         │
│  Validates explainability requirements                   │
│  - Confidence score                                       │
│  - Evidence chain                                         │
│  - Feature contributions                                  │
└──────────────────────────────────────────────────────────┘
                            │
                            │ Audit
                            ▼
┌──────────────────────────────────────────────────────────┐
│           TAMPER-PROOF AUDIT TRAIL                       │
│  Cryptographically signed explanation logs               │
└──────────────────────────────────────────────────────────┘
```

## Implementation

### Basic usage

```python
from google.cloud import aiplatform
from governance_kernel.vector_ledger import SovereignGuardrail
import shap
import numpy as np

# Initialize Vertex AI
aiplatform.init(project='iluminara-core', location='africa-south1')

# Load model
endpoint = aiplatform.Endpoint('projects/123/locations/africa-south1/endpoints/456')

# Make prediction
instances = [{
    'fever': 1,
    'cough': 1,
    'difficulty_breathing': 0,
    'age': 45,
    'comorbidities': 1
}]

prediction = endpoint.predict(instances=instances)
confidence_score = prediction.predictions[0]['confidence']

# Generate SHAP explanation
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(np.array(instances))

# Extract feature contributions
feature_contributions = {
    'fever': float(shap_values[0][0]),
    'cough': float(shap_values[0][1]),
    'difficulty_breathing': float(shap_values[0][2]),
    'age': float(shap_values[0][3]),
    'comorbidities': float(shap_values[0][4])
}

# Validate with SovereignGuardrail
guardrail = SovereignGuardrail(enable_tamper_proof_audit=True)

try:
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'actor': 'vertex_ai_model',
            'resource': 'patient_diagnosis',
            'explanation': f'SHAP values: {feature_contributions}',
            'confidence_score': confidence_score,
            'evidence_chain': ['fever', 'cough', 'comorbidities'],
            'consent_token': 'valid_token',
            'consent_scope': 'diagnosis'
        },
        jurisdiction='EU_AI_ACT'
    )
    print("✅ Explainability requirements met")
except Exception as e:
    print(f"❌ Compliance violation: {e}")
```

### Automated explainability wrapper

```python
from typing import Dict, Any, List
import shap

class ExplainableVertexAI:
    """
    Wrapper for Vertex AI models that automatically generates SHAP explanations
    and validates compliance with SovereignGuardrail.
    """
    
    def __init__(
        self,
        endpoint: aiplatform.Endpoint,
        feature_names: List[str],
        jurisdiction: str = 'EU_AI_ACT',
        enable_audit: bool = True
    ):
        self.endpoint = endpoint
        self.feature_names = feature_names
        self.jurisdiction = jurisdiction
        self.guardrail = SovereignGuardrail(enable_tamper_proof_audit=enable_audit)
    
    def predict_with_explanation(
        self,
        instances: List[Dict[str, Any]],
        consent_token: str,
        consent_scope: str = 'diagnosis'
    ) -> Dict[str, Any]:
        """
        Make prediction with automatic SHAP explanation and compliance validation.
        
        Args:
            instances: Input features for prediction
            consent_token: Valid consent token
            consent_scope: Scope of consent
        
        Returns:
            Dictionary with prediction, explanation, and compliance status
        """
        # Make prediction
        prediction = self.endpoint.predict(instances=instances)
        confidence_score = prediction.predictions[0]['confidence']
        predicted_class = prediction.predictions[0]['class']
        
        # Generate SHAP explanation
        explainer = shap.KernelExplainer(
            lambda x: self.endpoint.predict(instances=x).predictions,
            instances
        )
        shap_values = explainer.shap_values(instances)
        
        # Extract feature contributions
        feature_contributions = {
            name: float(shap_values[0][i])
            for i, name in enumerate(self.feature_names)
        }
        
        # Identify evidence chain (top contributing features)
        evidence_chain = sorted(
            feature_contributions.items(),
            key=lambda x: abs(x[1]),
            reverse=True
        )[:3]
        evidence_chain = [feature for feature, _ in evidence_chain]
        
        # Validate compliance
        try:
            self.guardrail.validate_action(
                action_type='High_Risk_Inference',
                payload={
                    'actor': 'vertex_ai_model',
                    'resource': 'patient_diagnosis',
                    'explanation': f'SHAP values: {feature_contributions}',
                    'confidence_score': confidence_score,
                    'evidence_chain': evidence_chain,
                    'consent_token': consent_token,
                    'consent_scope': consent_scope
                },
                jurisdiction=self.jurisdiction
            )
            compliance_status = 'COMPLIANT'
        except Exception as e:
            compliance_status = f'VIOLATION: {e}'
        
        return {
            'prediction': predicted_class,
            'confidence_score': confidence_score,
            'feature_contributions': feature_contributions,
            'evidence_chain': evidence_chain,
            'compliance_status': compliance_status,
            'explanation_method': 'SHAP (Kernel Explainer)'
        }

# Usage
explainable_model = ExplainableVertexAI(
    endpoint=endpoint,
    feature_names=['fever', 'cough', 'difficulty_breathing', 'age', 'comorbidities'],
    jurisdiction='EU_AI_ACT'
)

result = explainable_model.predict_with_explanation(
    instances=[{
        'fever': 1,
        'cough': 1,
        'difficulty_breathing': 0,
        'age': 45,
        'comorbidities': 1
    }],
    consent_token='valid_token',
    consent_scope='diagnosis'
)

print(f"Prediction: {result['prediction']}")
print(f"Confidence: {result['confidence_score']:.2%}")
print(f"Evidence: {result['evidence_chain']}")
print(f"Compliance: {result['compliance_status']}")
```

## SHAP explainer types

### TreeExplainer (Recommended for XGBoost, LightGBM)

Fast and exact for tree-based models.

```python
import shap
import xgboost as xgb

# Train model
model = xgb.XGBClassifier()
model.fit(X_train, y_train)

# Create explainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Visualize
shap.summary_plot(shap_values, X_test, feature_names=feature_names)
```

### DeepExplainer (Neural Networks)

Optimized for deep learning models.

```python
import shap
import tensorflow as tf

# Load model
model = tf.keras.models.load_model('model.h5')

# Create explainer
explainer = shap.DeepExplainer(model, X_train[:100])
shap_values = explainer.shap_values(X_test)
```

### KernelExplainer (Model-Agnostic)

Works with any model but slower.

```python
import shap

# Create explainer
explainer = shap.KernelExplainer(
    model.predict_proba,
    shap.sample(X_train, 100)
)
shap_values = explainer.shap_values(X_test)
```

## Visualization

### Feature importance plot

```python
import shap
import matplotlib.pyplot as plt

# Generate SHAP values
shap_values = explainer.shap_values(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test, feature_names=feature_names)
plt.savefig('shap_summary.png')

# Force plot (single prediction)
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    X_test.iloc[0],
    feature_names=feature_names
)
```

### Waterfall plot

```python
# Waterfall plot for single prediction
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values[0],
        base_values=explainer.expected_value,
        data=X_test.iloc[0],
        feature_names=feature_names
    )
)
```

## Deployment to Vertex AI

### Train and deploy explainable model

```python
from google.cloud import aiplatform

# Initialize
aiplatform.init(project='iluminara-core', location='africa-south1')

# Create training job
job = aiplatform.CustomTrainingJob(
    display_name='explainable-cholera-predictor',
    script_path='train.py',
    container_uri='gcr.io/cloud-aiplatform/training/tf-cpu.2-11:latest',
    requirements=['shap', 'xgboost', 'scikit-learn']
)

# Train model
model = job.run(
    dataset=dataset,
    model_display_name='cholera-predictor-v1',
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1
)

# Deploy to endpoint
endpoint = model.deploy(
    machine_type='n1-standard-4',
    min_replica_count=1,
    max_replica_count=10,
    traffic_percentage=100
)

print(f"✅ Model deployed: {endpoint.resource_name}")
```

## Compliance validation

### Automated testing

```python
import pytest
from governance_kernel.vector_ledger import SovereignGuardrail, SovereigntyViolationError

def test_explainability_compliance():
    """Test that high-risk inferences meet explainability requirements."""
    guardrail = SovereignGuardrail()
    
    # Valid explanation
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'explanation': 'SHAP values: [0.8, 0.1, 0.1]',
            'confidence_score': 0.95,
            'evidence_chain': ['fever', 'cough', 'positive_test'],
            'consent_token': 'valid_token',
            'consent_scope': 'diagnosis'
        },
        jurisdiction='EU_AI_ACT'
    )
    
    # Missing explanation should fail
    with pytest.raises(SovereigntyViolationError):
        guardrail.validate_action(
            action_type='High_Risk_Inference',
            payload={
                'confidence_score': 0.95,
                'consent_token': 'valid_token'
            },
            jurisdiction='EU_AI_ACT'
        )
```

## Performance considerations

- **TreeExplainer**: ~1ms per prediction (fast)
- **DeepExplainer**: ~10ms per prediction (moderate)
- **KernelExplainer**: ~100ms per prediction (slow)

For production deployments:
1. Use TreeExplainer for tree-based models
2. Pre-compute explanations for batch predictions
3. Cache explanations for repeated queries
4. Use GPU acceleration for DeepExplainer

## Next steps

<CardGroup cols={2}>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy autonomous surveillance agents
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Understand compliance enforcement
  </Card>
  <Card
    title="API reference"
    icon="code"
    href="/api-reference/overview"
  >
    Integrate explainability into your API
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/overview"
  >
    Deploy to Vertex AI
  </Card>
</CardGroup>
