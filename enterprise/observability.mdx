---
title: Observability & monitoring
description: Comprehensive monitoring, logging, and alerting for production iLuminara deployments
---

iLuminara's observability stack provides complete visibility into system health, performance, and security across distributed edge-cloud deployments.

## Observability pillars

### Metrics
Quantitative measurements of system behavior:
- **Infrastructure**: CPU, memory, disk, network
- **Application**: Request rate, latency, error rate
- **Business**: User engagement, model accuracy, bio-credits minted

### Logs
Structured event records:
- **Application logs**: Service events and errors
- **Audit logs**: Compliance and security events
- **Access logs**: API and data access

### Traces
Distributed request tracking:
- **End-to-end latency**: Request flow across services
- **Bottleneck identification**: Slow components
- **Dependency mapping**: Service relationships

## Monitoring stack

### Architecture

```
┌─────────────────────────────────────────────────────────┐
│              Observability Architecture                 │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  ┌─────────────────────────────────────────────────┐  │
│  │              Grafana Dashboards                  │  │
│  │  (Visualization + Alerting)                     │  │
│  └─────────────────────────────────────────────────┘  │
│                        │                                │
│         ┌──────────────┼──────────────┐                │
│         ▼              ▼              ▼                │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐            │
│  │Prometheus│  │  Loki    │  │  Tempo   │            │
│  │(Metrics) │  │  (Logs)  │  │ (Traces) │            │
│  └──────────┘  └──────────┘  └──────────┘            │
│         ▲              ▲              ▲                │
│         │              │              │                │
│  ┌──────────────────────────────────────────────┐    │
│  │         Instrumented Services                 │    │
│  ├──────────────────────────────────────────────┤    │
│  │ • NIM Inference (OpenTelemetry)              │    │
│  │ • API Gateway (Prometheus exporter)          │    │
│  │ • Ghost-Mesh (Custom metrics)                │    │
│  │ • Omni-Law (Audit logs)                      │    │
│  └──────────────────────────────────────────────┘    │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Prometheus (Metrics)

#### Configuration

```yaml prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'iluminara-prod'
    region: 'us-east-1'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

# Load rules
rule_files:
  - "alerts/*.yml"

# Scrape configurations
scrape_configs:
  # Prometheus itself
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Kubernetes service discovery
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__

  # NIM inference service
  - job_name: 'nim-inference'
    static_configs:
      - targets: ['nim-inference:8000']
    metrics_path: '/metrics'

  # Node exporter (system metrics)
  - job_name: 'node-exporter'
    kubernetes_sd_configs:
      - role: node
    relabel_configs:
      - source_labels: [__address__]
        regex: '(.*):10250'
        replacement: '${1}:9100'
        target_label: __address__

  # GPU metrics (DCGM exporter)
  - job_name: 'dcgm-exporter'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        action: keep
        regex: dcgm-exporter
```

#### Custom metrics instrumentation

```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# Define metrics
inference_requests = Counter(
    'nim_inference_requests_total',
    'Total number of inference requests',
    ['model', 'status']
)

inference_latency = Histogram(
    'nim_inference_latency_seconds',
    'Inference latency in seconds',
    ['model'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0]
)

active_requests = Gauge(
    'nim_active_requests',
    'Number of active inference requests',
    ['model']
)

model_accuracy = Gauge(
    'nim_model_accuracy',
    'Model accuracy score',
    ['model', 'dataset']
)

# Instrument code
def process_inference(model_name, input_data):
    """Process inference request with metrics."""
    active_requests.labels(model=model_name).inc()
    
    start_time = time.time()
    try:
        result = run_inference(model_name, input_data)
        
        # Record success
        inference_requests.labels(model=model_name, status='success').inc()
        
        return result
    except Exception as e:
        # Record failure
        inference_requests.labels(model=model_name, status='error').inc()
        raise
    finally:
        # Record latency
        latency = time.time() - start_time
        inference_latency.labels(model=model_name).observe(latency)
        
        active_requests.labels(model=model_name).dec()

# Start metrics server
start_http_server(8000)
```

### Grafana (Visualization)

#### Dashboard configuration

```json grafana-dashboard.json
{
  "dashboard": {
    "title": "iLuminara - System Overview",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [
          {
            "expr": "rate(nim_inference_requests_total[5m])",
            "legendFormat": "{{model}} - {{status}}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "P95 Latency",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(nim_inference_latency_seconds_bucket[5m]))",
            "legendFormat": "{{model}}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "GPU Utilization",
        "targets": [
          {
            "expr": "DCGM_FI_DEV_GPU_UTIL",
            "legendFormat": "GPU {{gpu}}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Error Rate",
        "targets": [
          {
            "expr": "rate(nim_inference_requests_total{status=\"error\"}[5m]) / rate(nim_inference_requests_total[5m])",
            "legendFormat": "{{model}}"
          }
        ],
        "type": "graph"
      }
    ]
  }
}
```

### Loki (Logs)

#### Configuration

```yaml loki-config.yaml
auth_enabled: false

server:
  http_listen_port: 3100

ingester:
  lifecycler:
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
  chunk_idle_period: 5m
  chunk_retain_period: 30s

schema_config:
  configs:
    - from: 2024-01-01
      store: boltdb-shipper
      object_store: s3
      schema: v11
      index:
        prefix: loki_index_
        period: 24h

storage_config:
  boltdb_shipper:
    active_index_directory: /loki/index
    cache_location: /loki/cache
    shared_store: s3
  aws:
    s3: s3://us-east-1/iluminara-logs
    s3forcepathstyle: true

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h

chunk_store_config:
  max_look_back_period: 0s

table_manager:
  retention_deletes_enabled: true
  retention_period: 720h  # 30 days
```

#### Log aggregation

```python
import logging
import json
from pythonjsonlogger import jsonlogger

# Configure structured logging
logger = logging.getLogger()
logHandler = logging.StreamHandler()

formatter = jsonlogger.JsonFormatter(
    '%(timestamp)s %(level)s %(name)s %(message)s',
    rename_fields={
        'levelname': 'level',
        'name': 'logger',
        'asctime': 'timestamp'
    }
)

logHandler.setFormatter(formatter)
logger.addHandler(logHandler)
logger.setLevel(logging.INFO)

# Log with structured data
logger.info(
    "Inference completed",
    extra={
        'model': 'llama-3-8b',
        'latency_ms': 150,
        'tokens_generated': 100,
        'user_id': 'user_12345',
        'request_id': 'req_67890'
    }
)
```

### Tempo (Traces)

#### OpenTelemetry instrumentation

```python
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.requests import RequestsInstrumentor

# Initialize tracer
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

# Configure exporter
otlp_exporter = OTLPSpanExporter(
    endpoint="tempo:4317",
    insecure=True
)

# Add span processor
span_processor = BatchSpanProcessor(otlp_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# Auto-instrument HTTP requests
RequestsInstrumentor().instrument()

# Manual instrumentation
@tracer.start_as_current_span("process_inference")
def process_inference(model_name, input_data):
    """Process inference with distributed tracing."""
    span = trace.get_current_span()
    
    # Add span attributes
    span.set_attribute("model.name", model_name)
    span.set_attribute("input.length", len(input_data))
    
    # Call downstream services (automatically traced)
    with tracer.start_as_current_span("load_model"):
        model = load_model(model_name)
    
    with tracer.start_as_current_span("run_inference"):
        result = model.predict(input_data)
    
    # Add result attributes
    span.set_attribute("output.length", len(result))
    
    return result
```

## Alerting

### Alert rules

```yaml alerts/inference.yml
groups:
  - name: inference_alerts
    interval: 30s
    rules:
      # High error rate
      - alert: HighInferenceErrorRate
        expr: |
          rate(nim_inference_requests_total{status="error"}[5m])
          / rate(nim_inference_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
          component: nim-inference
        annotations:
          summary: "High inference error rate ({{ $value | humanizePercentage }})"
          description: "Model {{ $labels.model }} has error rate above 5% for 5 minutes"

      # High latency
      - alert: HighInferenceLatency
        expr: |
          histogram_quantile(0.95,
            rate(nim_inference_latency_seconds_bucket[5m])
          ) > 2.0
        for: 10m
        labels:
          severity: warning
          component: nim-inference
        annotations:
          summary: "High inference latency ({{ $value }}s)"
          description: "Model {{ $labels.model }} P95 latency above 2s for 10 minutes"

      # GPU utilization
      - alert: LowGPUUtilization
        expr: DCGM_FI_DEV_GPU_UTIL < 20
        for: 30m
        labels:
          severity: info
          component: gpu
        annotations:
          summary: "Low GPU utilization ({{ $value }}%)"
          description: "GPU {{ $labels.gpu }} utilization below 20% for 30 minutes"

      # Service down
      - alert: ServiceDown
        expr: up{job="nim-inference"} == 0
        for: 2m
        labels:
          severity: critical
          component: nim-inference
        annotations:
          summary: "Service is down"
          description: "NIM inference service has been down for 2 minutes"
```

### Alertmanager configuration

```yaml alertmanager.yml
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  routes:
    # Critical alerts to PagerDuty
    - match:
        severity: critical
      receiver: 'pagerduty'
      continue: true
    
    # All alerts to Slack
    - match_re:
        severity: critical|warning
      receiver: 'slack'

receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/alerts'

  - name: 'slack'
    slack_configs:
      - channel: '#iluminara-alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true

  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_KEY'
        description: '{{ .GroupLabels.alertname }}'
```

## Edge monitoring

### Lightweight monitoring for edge devices

```python
import psutil
import GPUtil
import requests
import time

class EdgeMonitor:
    """
    Lightweight monitoring for edge devices.
    Sends metrics to regional hub when connectivity available.
    """
    def __init__(self, hub_url, device_id):
        self.hub_url = hub_url
        self.device_id = device_id
        self.metrics_buffer = []
    
    def collect_metrics(self):
        """Collect system metrics."""
        metrics = {
            'timestamp': time.time(),
            'device_id': self.device_id,
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_percent': psutil.disk_usage('/').percent,
            'temperature': self._get_temperature(),
            'battery_percent': self._get_battery_percent(),
            'network_bytes_sent': psutil.net_io_counters().bytes_sent,
            'network_bytes_recv': psutil.net_io_counters().bytes_recv
        }
        
        # Add GPU metrics if available
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                metrics['gpu_utilization'] = gpu.load * 100
                metrics['gpu_memory_used'] = gpu.memoryUsed
                metrics['gpu_temperature'] = gpu.temperature
        except:
            pass
        
        return metrics
    
    def send_metrics(self, metrics):
        """Send metrics to regional hub."""
        try:
            response = requests.post(
                f"{self.hub_url}/api/v1/metrics",
                json=metrics,
                timeout=5
            )
            return response.status_code == 200
        except:
            # Buffer metrics if hub unreachable
            self.metrics_buffer.append(metrics)
            return False
    
    def flush_buffer(self):
        """Flush buffered metrics when connectivity restored."""
        if not self.metrics_buffer:
            return
        
        try:
            response = requests.post(
                f"{self.hub_url}/api/v1/metrics/batch",
                json=self.metrics_buffer,
                timeout=10
            )
            if response.status_code == 200:
                self.metrics_buffer = []
        except:
            pass
    
    def run(self, interval=60):
        """Run monitoring loop."""
        while True:
            metrics = self.collect_metrics()
            
            if not self.send_metrics(metrics):
                # Try to flush buffer
                self.flush_buffer()
            
            time.sleep(interval)
```

## Compliance monitoring

### Omni-Law audit logging

```python
import hashlib
import json
from datetime import datetime

class ComplianceMonitor:
    """
    Monitor compliance with Omni-Law framework.
    Generate audit logs for all data operations.
    """
    def __init__(self, blockchain_client):
        self.blockchain = blockchain_client
    
    def log_data_access(self, user_id, data_type, operation, justification):
        """
        Log data access for compliance audit.
        
        Args:
            user_id: User accessing data
            data_type: Type of data (PHI, PII, genomic, etc.)
            operation: Operation (read, write, delete)
            justification: Legal basis for access
        """
        audit_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'user_id': user_id,
            'data_type': data_type,
            'operation': operation,
            'justification': justification,
            'frameworks_checked': self._get_applicable_frameworks(data_type)
        }
        
        # Hash for integrity
        audit_hash = hashlib.sha256(
            json.dumps(audit_entry, sort_keys=True).encode()
        ).hexdigest()
        
        audit_entry['hash'] = audit_hash
        
        # Store on blockchain for immutability
        self.blockchain.store_audit_log(audit_entry)
        
        # Also send to monitoring system
        self._send_to_monitoring(audit_entry)
    
    def _get_applicable_frameworks(self, data_type):
        """Determine which compliance frameworks apply."""
        frameworks = []
        
        if data_type in ['PHI', 'medical_record']:
            frameworks.extend(['HIPAA', 'GDPR'])
        
        if data_type == 'genomic':
            frameworks.extend(['PABS', 'Nagoya_Protocol'])
        
        if data_type == 'PII':
            frameworks.append('GDPR')
        
        return frameworks
```

## Performance benchmarks

| Metric | Target | Measurement |
|--------|--------|-------------|
| Metrics collection overhead | < 1% CPU | Prometheus scrape |
| Log ingestion rate | > 100k events/sec | Loki |
| Trace sampling rate | 1-10% | Tempo |
| Dashboard load time | < 2 sec | Grafana |
| Alert latency | < 30 sec | Alertmanager |

## Next steps

<CardGroup cols={2}>
  <Card title="CI/CD pipeline" icon="code-branch" href="/enterprise/cicd-pipeline">
    Automated testing and deployment
  </Card>
  <Card title="Security" icon="shield" href="/enterprise/security">
    Security best practices
  </Card>
  <Card title="Incident response" icon="siren" href="/enterprise/incident-response">
    Handling production incidents
  </Card>
  <Card title="Cost optimization" icon="dollar-sign" href="/enterprise/cost-optimization">
    Reduce monitoring costs
  </Card>
</CardGroup>
