---
title: JEPA/MPC architecture
description: Joint-Embedding Predictive Architecture and Model Predictive Control replacing reinforcement learning
---

iLuminara uses JEPA (Joint-Embedding Predictive Architecture) combined with MPC (Model Predictive Control) instead of traditional reinforcement learning, providing more sample-efficient, interpretable, and robust decision-making.

## Why JEPA/MPC over RL?

### Problems with reinforcement learning

**Sample inefficiency**: RL requires millions of interactions to learn simple tasks  
**Reward engineering**: Designing reward functions is difficult and error-prone  
**Sim-to-real gap**: Policies trained in simulation often fail in real world  
**Black box**: Difficult to understand or debug learned policies  
**Safety concerns**: Exploration can cause dangerous actions

### Advantages of JEPA/MPC

**Sample efficiency**: Learn world models from observation, not trial-and-error  
**Interpretability**: Explicit world model can be inspected and validated  
**Safety**: Plan actions using model, avoid dangerous exploration  
**Transfer learning**: World models generalize across tasks  
**Uncertainty quantification**: Model confidence guides decision-making

## JEPA architecture

### Core concept

JEPA learns to predict future states in an abstract representation space, not pixel space:

```
┌─────────────────────────────────────────────────────────┐
│                    JEPA Architecture                    │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Observation (t)          Observation (t+k)            │
│       │                          │                      │
│       ▼                          ▼                      │
│  ┌─────────┐              ┌─────────┐                 │
│  │ Encoder │              │ Encoder │                 │
│  │   s_x   │              │   s_y   │                 │
│  └─────────┘              └─────────┘                 │
│       │                          │                      │
│       │                          │                      │
│       ▼                          ▼                      │
│  Representation z_x      Representation z_y            │
│       │                          │                      │
│       │                          │                      │
│       ▼                          │                      │
│  ┌─────────────────┐            │                      │
│  │   Predictor     │            │                      │
│  │   (predicts     │            │                      │
│  │   z_y from z_x) │            │                      │
│  └─────────────────┘            │                      │
│       │                          │                      │
│       ▼                          ▼                      │
│  Predicted ẑ_y  ←─────→  Actual z_y                   │
│                                                         │
│  Loss: D(ẑ_y, z_y)  (distance in representation space)│
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Key components

#### Encoder (s_x)
Maps observations to abstract representations:
```python
class JEPAEncoder(nn.Module):
    """
    Encode observations into abstract representation space.
    Uses Vision Transformer (ViT) for visual observations.
    """
    def __init__(self, obs_dim, repr_dim=256):
        super().__init__()
        self.vit = VisionTransformer(
            image_size=224,
            patch_size=16,
            num_layers=12,
            num_heads=8,
            hidden_dim=768,
            mlp_dim=3072
        )
        self.projection = nn.Linear(768, repr_dim)
    
    def forward(self, obs):
        # obs: (batch, channels, height, width)
        features = self.vit(obs)  # (batch, 768)
        representation = self.projection(features)  # (batch, repr_dim)
        return representation
```

#### Predictor
Predicts future representations from current representation:
```python
class JEPAPredictor(nn.Module):
    """
    Predict future representation from current representation.
    Uses Transformer to model temporal dynamics.
    """
    def __init__(self, repr_dim=256, num_layers=6):
        super().__init__()
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=repr_dim,
                nhead=8,
                dim_feedforward=1024,
                dropout=0.1
            ),
            num_layers=num_layers
        )
    
    def forward(self, z_current, horizon=10):
        # z_current: (batch, repr_dim)
        # Predict representations for next 'horizon' timesteps
        
        predictions = []
        z = z_current.unsqueeze(1)  # (batch, 1, repr_dim)
        
        for t in range(horizon):
            z = self.transformer(z)
            predictions.append(z[:, -1, :])  # Last timestep
        
        return torch.stack(predictions, dim=1)  # (batch, horizon, repr_dim)
```

#### Training objective
Learn representations that enable accurate prediction:
```python
def jepa_loss(encoder, predictor, obs_current, obs_future, k=10):
    """
    JEPA training loss.
    
    Args:
        encoder: Observation encoder
        predictor: Future predictor
        obs_current: Current observation
        obs_future: Future observation (k steps ahead)
        k: Prediction horizon
    
    Returns:
        Loss value
    """
    # Encode observations
    z_current = encoder(obs_current)
    z_future = encoder(obs_future)
    
    # Predict future representation
    z_predicted = predictor(z_current, horizon=k)[:, k-1, :]
    
    # Loss: distance in representation space
    loss = F.mse_loss(z_predicted, z_future.detach())
    
    # Optional: Add variance regularization to prevent collapse
    variance_loss = -torch.log(z_predicted.var(dim=0).mean() + 1e-6)
    
    return loss + 0.1 * variance_loss
```

## Model Predictive Control (MPC)

### Core concept

Use learned world model to plan actions by simulating future trajectories:

```
┌─────────────────────────────────────────────────────────┐
│              Model Predictive Control                   │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Current State s_t                                      │
│       │                                                  │
│       ▼                                                  │
│  ┌─────────────────────────────────────┐               │
│  │  Sample candidate action sequences  │               │
│  │  a_t, a_{t+1}, ..., a_{t+H}        │               │
│  └─────────────────────────────────────┘               │
│       │                                                  │
│       ▼                                                  │
│  ┌─────────────────────────────────────┐               │
│  │  Simulate trajectories using JEPA   │               │
│  │  s_t → s_{t+1} → ... → s_{t+H}     │               │
│  └─────────────────────────────────────┘               │
│       │                                                  │
│       ▼                                                  │
│  ┌─────────────────────────────────────┐               │
│  │  Evaluate trajectories with cost    │               │
│  │  function J(s, a)                   │               │
│  └─────────────────────────────────────┘               │
│       │                                                  │
│       ▼                                                  │
│  Select best action sequence                            │
│  Execute first action a_t                               │
│  Replan at next timestep                                │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Implementation

```python
class MPCController:
    """
    Model Predictive Control using JEPA world model.
    """
    def __init__(self, jepa_model, horizon=10, num_samples=100):
        self.jepa = jepa_model
        self.horizon = horizon
        self.num_samples = num_samples
    
    def plan(self, current_obs, goal, constraints=None):
        """
        Plan action sequence to reach goal.
        
        Args:
            current_obs: Current observation
            goal: Goal state or representation
            constraints: Optional constraints (safety, resource limits)
        
        Returns:
            Best action to take now
        """
        # Encode current observation
        z_current = self.jepa.encoder(current_obs)
        
        # Sample candidate action sequences
        action_sequences = self._sample_actions(self.horizon, self.num_samples)
        
        # Simulate trajectories
        trajectories = []
        costs = []
        
        for actions in action_sequences:
            trajectory = self._simulate_trajectory(z_current, actions)
            cost = self._evaluate_trajectory(trajectory, goal, actions, constraints)
            
            trajectories.append(trajectory)
            costs.append(cost)
        
        # Select best action sequence
        best_idx = np.argmin(costs)
        best_actions = action_sequences[best_idx]
        
        # Return first action (MPC receding horizon)
        return best_actions[0], {
            'predicted_trajectory': trajectories[best_idx],
            'expected_cost': costs[best_idx],
            'num_candidates': self.num_samples
        }
    
    def _simulate_trajectory(self, z_current, actions):
        """
        Simulate trajectory using JEPA predictor.
        
        Args:
            z_current: Current representation
            actions: Sequence of actions
        
        Returns:
            Predicted trajectory in representation space
        """
        trajectory = [z_current]
        z = z_current
        
        for action in actions:
            # Predict next state given action
            z_next = self.jepa.predictor(z, action)
            trajectory.append(z_next)
            z = z_next
        
        return torch.stack(trajectory)
    
    def _evaluate_trajectory(self, trajectory, goal, actions, constraints):
        """
        Evaluate trajectory cost.
        
        Cost components:
        - Distance to goal
        - Action cost (energy, wear)
        - Constraint violations
        - Uncertainty penalty
        """
        # Goal reaching cost
        final_state = trajectory[-1]
        goal_cost = torch.norm(final_state - goal)
        
        # Action cost
        action_cost = torch.sum(torch.norm(actions, dim=-1))
        
        # Constraint violations
        constraint_cost = 0
        if constraints:
            for constraint in constraints:
                violations = constraint.evaluate(trajectory)
                constraint_cost += torch.sum(torch.relu(violations))
        
        # Uncertainty penalty (prefer confident predictions)
        uncertainty = self._estimate_uncertainty(trajectory)
        uncertainty_cost = uncertainty.mean()
        
        # Total cost
        total_cost = (
            1.0 * goal_cost +
            0.1 * action_cost +
            10.0 * constraint_cost +
            0.5 * uncertainty_cost
        )
        
        return total_cost.item()
    
    def _sample_actions(self, horizon, num_samples):
        """
        Sample candidate action sequences.
        
        Strategies:
        - Random sampling
        - Cross-Entropy Method (CEM)
        - Model Predictive Path Integral (MPPI)
        """
        # Simple random sampling for demonstration
        action_dim = self.jepa.action_dim
        actions = torch.randn(num_samples, horizon, action_dim)
        
        # Clip to action bounds
        actions = torch.clamp(actions, -1, 1)
        
        return actions
    
    def _estimate_uncertainty(self, trajectory):
        """
        Estimate prediction uncertainty.
        
        Methods:
        - Ensemble disagreement
        - Dropout uncertainty
        - Learned uncertainty head
        """
        # Placeholder: use trajectory variance as proxy
        return trajectory.var(dim=0)
```

## Use cases in iLuminara

### Agro-voltaic optimization (Stack 8)

Use JEPA/MPC to optimize solar panel tilt:

```python
class AgroVoltaicMPC:
    """
    MPC for agro-voltaic panel tilt optimization.
    """
    def __init__(self):
        self.jepa = self._load_jepa_model()
        self.mpc = MPCController(self.jepa, horizon=24)  # 24 hour horizon
    
    def optimize_tilt(self, current_state, weather_forecast):
        """
        Optimize panel tilt for next 24 hours.
        
        Current state:
        - Panel tilt angle
        - Crop temperature
        - Soil moisture
        - Solar irradiance
        
        Goal:
        - Maximize energy generation
        - Minimize crop stress
        """
        # Define goal (multi-objective)
        goal = {
            'energy_generation': 'maximize',
            'crop_stress': 'minimize'
        }
        
        # Define constraints
        constraints = [
            TiltRateConstraint(max_rate=5.0),  # degrees per hour
            CropTemperatureConstraint(max_temp=35.0),  # Celsius
            MechanicalConstraint(min_angle=0, max_angle=90)
        ]
        
        # Plan action
        action, info = self.mpc.plan(
            current_obs=current_state,
            goal=goal,
            constraints=constraints
        )
        
        return action, info
```

### Reverse logistics routing (Stack 10)

Use JEPA/MPC for dynamic route optimization:

```python
class ReverseLogisticsMPC:
    """
    MPC for e-waste collection routing.
    """
    def __init__(self):
        self.jepa = self._load_jepa_model()
        self.mpc = MPCController(self.jepa, horizon=10)
    
    def plan_route(self, current_location, collection_points, traffic_forecast):
        """
        Plan optimal collection route considering:
        - Traffic conditions
        - Collection point priorities
        - Vehicle capacity
        - Time windows
        """
        # Current state
        state = {
            'location': current_location,
            'remaining_capacity': self.vehicle_capacity,
            'time': current_time,
            'collected_points': []
        }
        
        # Goal: collect all high-priority points
        goal = {
            'collected_points': collection_points,
            'minimize_distance': True,
            'minimize_time': True
        }
        
        # Constraints
        constraints = [
            CapacityConstraint(max_capacity=self.vehicle_capacity),
            TimeWindowConstraint(collection_points),
            TrafficConstraint(traffic_forecast)
        ]
        
        # Plan next waypoint
        next_waypoint, info = self.mpc.plan(
            current_obs=state,
            goal=goal,
            constraints=constraints
        )
        
        return next_waypoint, info
```

### Ghost-Mesh protocol selection (Stack 6)

Use JEPA/MPC to select optimal networking protocol:

```python
class GhostMeshMPC:
    """
    MPC for adaptive protocol selection.
    """
    def __init__(self):
        self.jepa = self._load_jepa_model()
        self.mpc = MPCController(self.jepa, horizon=5)
    
    def select_protocol(self, network_state, message_queue):
        """
        Select optimal protocol considering:
        - Spectrum conditions
        - Message priority
        - Energy budget
        - Latency requirements
        """
        # Current state
        state = {
            'spectrum': network_state['spectrum'],
            'battery_level': network_state['battery'],
            'queue_size': len(message_queue),
            'current_protocol': network_state['protocol']
        }
        
        # Goal: deliver messages efficiently
        goal = {
            'messages_delivered': 'maximize',
            'energy_consumption': 'minimize',
            'latency': 'minimize'
        }
        
        # Constraints
        constraints = [
            EnergyConstraint(min_battery=0.2),  # 20% reserve
            LatencyConstraint(max_latency=message_queue[0].deadline),
            SpectrumConstraint(available_bands=network_state['available_bands'])
        ]
        
        # Select protocol
        protocol, info = self.mpc.plan(
            current_obs=state,
            goal=goal,
            constraints=constraints
        )
        
        return protocol, info
```

## Training JEPA models

### Data collection

Collect observation sequences from real deployments:

```python
class JEPADataCollector:
    """
    Collect data for JEPA training.
    """
    def __init__(self, buffer_size=100000):
        self.buffer = ReplayBuffer(buffer_size)
    
    def collect_episode(self, env, policy=None):
        """
        Collect episode of observations.
        
        Args:
            env: Environment
            policy: Optional policy (can be random)
        """
        obs = env.reset()
        done = False
        
        episode = []
        
        while not done:
            # Take action (random or from policy)
            if policy:
                action = policy(obs)
            else:
                action = env.action_space.sample()
            
            # Step environment
            next_obs, reward, done, info = env.step(action)
            
            # Store transition
            episode.append({
                'obs': obs,
                'action': action,
                'next_obs': next_obs,
                'reward': reward
            })
            
            obs = next_obs
        
        # Add episode to buffer
        self.buffer.add_episode(episode)
        
        return episode
```

### Training loop

```python
def train_jepa(encoder, predictor, dataloader, num_epochs=100):
    """
    Train JEPA model.
    
    Args:
        encoder: Observation encoder
        predictor: Future predictor
        dataloader: DataLoader with observation sequences
        num_epochs: Number of training epochs
    """
    optimizer = torch.optim.AdamW(
        list(encoder.parameters()) + list(predictor.parameters()),
        lr=1e-4,
        weight_decay=0.01
    )
    
    for epoch in range(num_epochs):
        total_loss = 0
        
        for batch in dataloader:
            obs_current = batch['obs_current']
            obs_future = batch['obs_future']
            k = batch['horizon']
            
            # Compute loss
            loss = jepa_loss(encoder, predictor, obs_current, obs_future, k)
            
            # Backprop
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch}: Loss = {avg_loss:.4f}")
        
        # Save checkpoint
        if epoch % 10 == 0:
            torch.save({
                'encoder': encoder.state_dict(),
                'predictor': predictor.state_dict(),
                'epoch': epoch
            }, f'jepa_checkpoint_{epoch}.pt')
```

## Advantages over RL

### Sample efficiency
JEPA learns from passive observation, not trial-and-error:
- **RL**: Requires 1M+ environment interactions
- **JEPA**: Learns from 10k observation sequences

### Safety
MPC plans using model, avoids dangerous exploration:
- **RL**: May take dangerous actions during exploration
- **JEPA/MPC**: Simulates actions before execution

### Interpretability
Explicit world model can be inspected:
- **RL**: Black-box policy, hard to debug
- **JEPA/MPC**: Can visualize predicted trajectories

### Transfer learning
World models generalize across tasks:
- **RL**: Must retrain for each task
- **JEPA/MPC**: Same model, different cost functions

## Performance benchmarks

| Task | RL (PPO) | JEPA/MPC | Improvement |
|------|----------|----------|-------------|
| Agro-voltaic optimization | 100k steps | 10k observations | 10x sample efficiency |
| Route planning | 500k steps | 5k observations | 100x sample efficiency |
| Protocol selection | 200k steps | 8k observations | 25x sample efficiency |
| Training time | 48 hours | 6 hours | 8x faster |

## Next steps

<CardGroup cols={2}>
  <Card title="Deployment strategies" icon="rocket" href="/enterprise/deployment-strategies">
    Docker, Kubernetes, Serverless
  </Card>
  <Card title="Model training" icon="brain" href="/ml-health/model-training">
    Training JEPA models at scale
  </Card>
  <Card title="MPC tuning" icon="sliders" href="/enterprise/mpc-tuning">
    Tuning MPC parameters
  </Card>
  <Card title="Observability" icon="chart-line" href="/enterprise/observability">
    Monitoring JEPA/MPC systems
  </Card>
</CardGroup>
