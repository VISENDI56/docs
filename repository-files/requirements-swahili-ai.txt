# iLuminara-Core Swahili-AI Stack
# Fine-tuning pipeline for voice-to-JSON transformation
# Synchronized with main requirements.txt

# Base ML Framework (must match requirements.txt)
torch==2.1.2
transformers==4.53.0

# LoRA Fine-tuning
bitsandbytes==0.41.3
peft==0.7.1
accelerate==0.25.0

# Dataset Management
datasets==2.16.1
huggingface-hub==0.20.2

# Training Utilities
wandb==0.16.2
tensorboard==2.15.1
tqdm==4.66.1

# Model Optimization
optimum==1.16.1
auto-gptq==0.6.0

# Evaluation
evaluate==0.4.1
rouge-score==0.1.2
sacrebleu==2.4.0

# Audio Processing (FRENASA Integration)
librosa==0.10.1
soundfile==0.12.1
torchaudio==2.1.2

# Swahili NLP
sentencepiece==0.1.99
tokenizers==0.15.0

# Vertex AI Integration (must match requirements.txt)
google-cloud-aiplatform==1.38.1
google-cloud-storage==2.14.0

# Configuration
pyyaml==6.0.1
python-dotenv==1.0.0

# Rank=16 LoRA Configuration
# Training hyperparameters:
# - lora_r: 16
# - lora_alpha: 32
# - lora_dropout: 0.05
# - target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
# - task_type: CAUSAL_LM
pyasn1>=0.6.2 # not directly required, pinned by Snyk to avoid a vulnerability
