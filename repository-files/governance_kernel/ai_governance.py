"""
AI Governance Module
Implements EU AI Act, FDA CDS Software Guidance, and ISO 42001 compliance

Features:
- High-risk AI conformity assessment
- SHAP explainability for clinical decisions
- Post-market performance monitoring
- Bias detection and mitigation
- Transparency logging (Silent Flux integration)
"""

from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import numpy as np
import logging
import json

logger = logging.getLogger(__name__)


class AIRiskClass(Enum):
    """EU AI Act risk classification"""
    UNACCEPTABLE = "unacceptable"  # Prohibited (e.g., social scoring)
    HIGH = "high"  # Health AI, critical infrastructure
    LIMITED = "limited"  # Chatbots, emotion recognition
    MINIMAL = "minimal"  # Spam filters, video games


class ExplainabilityMethod(Enum):
    """Explainability techniques"""
    SHAP = "shap"  # SHapley Additive exPlanations
    LIME = "lime"  # Local Interpretable Model-agnostic Explanations
    FEATURE_IMPORTANCE = "feature_importance"
    ATTENTION_WEIGHTS = "attention_weights"


@dataclass
class AIInference:
    """AI inference with explainability"""
    model_id: str
    input_features: Dict[str, Any]
    prediction: Any
    confidence_score: float
    risk_class: AIRiskClass
    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())
    explainability: Optional[Dict] = None
    bias_metrics: Optional[Dict] = None
    human_oversight: bool = False


@dataclass
class ConformityAssessment:
    """EU AI Act conformity assessment"""
    assessment_id: str
    model_id: str
    risk_class: AIRiskClass
    assessment_date: str
    assessor: str
    technical_documentation: Dict
    quality_management: Dict
    risk_management: Dict
    data_governance: Dict
    transparency_obligations: Dict
    human_oversight_measures: Dict
    accuracy_metrics: Dict
    robustness_metrics: Dict
    cybersecurity_measures: Dict
    conformity_status: str  # CONFORMANT | NON_CONFORMANT | PENDING


class AIGovernance:
    """
    AI Governance Engine
    
    Implements:
    - EU AI Act (Regulation 2024/1689)
    - FDA Clinical Decision Support Software Guidance
    - ISO/IEC 42001 AI Management Systems
    - IMDRF AI-SaMD Principles
    """
    
    def __init__(
        self,
        enable_shap: bool = True,
        enable_bias_detection: bool = True,
        enable_silent_flux: bool = False
    ):
        self.enable_shap = enable_shap
        self.enable_bias_detection = enable_bias_detection
        self.enable_silent_flux = enable_silent_flux
        
        # Conformity assessments registry
        self.conformity_assessments: Dict[str, ConformityAssessment] = {}
        
        # Post-market monitoring
        self.performance_log: List[Dict] = []
        
        logger.info("ü§ñ AI Governance Engine initialized")
    
    def classify_ai_risk(
        self,
        use_case: str,
        domain: str,
        impact: str
    ) -> AIRiskClass:
        """
        Classify AI system risk per EU AI Act Art. 6
        
        Args:
            use_case: Description of AI use case
            domain: Application domain (health, finance, etc.)
            impact: Potential impact (high, medium, low)
        
        Returns:
            AIRiskClass
        """
        # Unacceptable risk (prohibited)
        prohibited_keywords = ["social_scoring", "subliminal_manipulation", "exploitation_vulnerability"]
        if any(kw in use_case.lower() for kw in prohibited_keywords):
            return AIRiskClass.UNACCEPTABLE
        
        # High risk (Annex III)
        high_risk_domains = ["health", "critical_infrastructure", "law_enforcement", "education"]
        if domain.lower() in high_risk_domains or impact.lower() == "high":
            return AIRiskClass.HIGH
        
        # Limited risk (transparency obligations)
        limited_risk_keywords = ["chatbot", "emotion_recognition", "deepfake"]
        if any(kw in use_case.lower() for kw in limited_risk_keywords):
            return AIRiskClass.LIMITED
        
        # Minimal risk (no obligations)
        return AIRiskClass.MINIMAL
    
    def generate_shap_explanation(
        self,
        model: Any,
        input_features: Dict[str, Any],
        background_data: Optional[np.ndarray] = None
    ) -> Dict[str, Any]:
        """
        Generate SHAP explainability for AI inference
        
        Complies with:
        - EU AI Act Art. 13 (Transparency)
        - GDPR Art. 22 (Right to Explanation)
        - FDA CDS Software Guidance
        
        Args:
            model: Trained model
            input_features: Input feature dictionary
            background_data: Background dataset for SHAP
        
        Returns:
            SHAP explanation dictionary
        """
        if not self.enable_shap:
            logger.warning("‚ö†Ô∏è SHAP explainability disabled")
            return {}
        
        try:
            import shap
            
            # Convert input to array
            feature_array = np.array(list(input_features.values())).reshape(1, -1)
            
            # Create SHAP explainer
            if background_data is not None:
                explainer = shap.KernelExplainer(model.predict, background_data)
            else:
                explainer = shap.Explainer(model)
            
            # Calculate SHAP values
            shap_values = explainer(feature_array)
            
            # Extract feature contributions
            feature_names = list(input_features.keys())
            contributions = {}
            
            for i, feature in enumerate(feature_names):
                contributions[feature] = float(shap_values.values[0][i])
            
            # Sort by absolute contribution
            sorted_contributions = dict(
                sorted(contributions.items(), key=lambda x: abs(x[1]), reverse=True)
            )
            
            explanation = {
                "method": "SHAP",
                "base_value": float(shap_values.base_values[0]),
                "feature_contributions": sorted_contributions,
                "top_3_features": list(sorted_contributions.keys())[:3],
                "explanation_quality": "HIGH",
                "timestamp": datetime.utcnow().isoformat()
            }
            
            logger.info(f"‚úÖ SHAP explanation generated - Top features: {explanation['top_3_features']}")
            return explanation
        
        except ImportError:
            logger.error("‚ùå SHAP library not installed: pip install shap")
            return {"error": "SHAP not available"}
        except Exception as e:
            logger.error(f"‚ùå SHAP generation failed: {e}")
            return {"error": str(e)}
    
    def detect_bias(
        self,
        predictions: List[Any],
        protected_attributes: Dict[str, List[Any]],
        ground_truth: Optional[List[Any]] = None
    ) -> Dict[str, Any]:
        """
        Detect bias in AI predictions
        
        Metrics:
        - Demographic parity
        - Equal opportunity
        - Disparate impact
        
        Args:
            predictions: Model predictions
            protected_attributes: Protected attributes (gender, race, etc.)
            ground_truth: True labels (optional)
        
        Returns:
            Bias metrics dictionary
        """
        if not self.enable_bias_detection:
            return {}
        
        bias_metrics = {
            "timestamp": datetime.utcnow().isoformat(),
            "metrics": {}
        }
        
        try:
            # Calculate demographic parity
            for attr_name, attr_values in protected_attributes.items():
                unique_values = set(attr_values)
                
                # Calculate positive prediction rate per group
                group_rates = {}
                for value in unique_values:
                    mask = [i for i, v in enumerate(attr_values) if v == value]
                    group_predictions = [predictions[i] for i in mask]
                    positive_rate = sum(group_predictions) / len(group_predictions) if group_predictions else 0
                    group_rates[str(value)] = positive_rate
                
                # Calculate disparate impact
                max_rate = max(group_rates.values())
                min_rate = min(group_rates.values())
                disparate_impact = min_rate / max_rate if max_rate > 0 else 0
                
                bias_metrics["metrics"][attr_name] = {
                    "group_rates": group_rates,
                    "disparate_impact": disparate_impact,
                    "bias_detected": disparate_impact < 0.8  # 80% rule
                }
            
            # Overall bias assessment
            bias_detected = any(
                m["bias_detected"] for m in bias_metrics["metrics"].values()
            )
            
            bias_metrics["overall_bias_detected"] = bias_detected
            bias_metrics["compliance_status"] = "NON_COMPLIANT" if bias_detected else "COMPLIANT"
            
            if bias_detected:
                logger.warning(f"‚ö†Ô∏è Bias detected in AI predictions")
            else:
                logger.info(f"‚úÖ No significant bias detected")
            
            return bias_metrics
        
        except Exception as e:
            logger.error(f"‚ùå Bias detection failed: {e}")
            return {"error": str(e)}
    
    def validate_high_risk_inference(
        self,
        inference: AIInference,
        require_conformity: bool = True
    ) -> Tuple[bool, List[str]]:
        """
        Validate high-risk AI inference per EU AI Act
        
        Requirements:
        - Conformity assessment completed
        - Explainability provided
        - Human oversight enabled
        - Bias metrics acceptable
        
        Args:
            inference: AI inference to validate
            require_conformity: Require conformity assessment
        
        Returns:
            (is_valid, violations)
        """
        violations = []
        
        # Check risk class
        if inference.risk_class != AIRiskClass.HIGH:
            return True, []  # Only validate high-risk AI
        
        # Check conformity assessment
        if require_conformity:
            if inference.model_id not in self.conformity_assessments:
                violations.append(
                    "EU AI Act Art. 6: High-risk AI requires conformity assessment"
                )
            else:
                assessment = self.conformity_assessments[inference.model_id]
                if assessment.conformity_status != "CONFORMANT":
                    violations.append(
                        f"EU AI Act Art. 6: Model conformity status is {assessment.conformity_status}"
                    )
        
        # Check explainability
        if not inference.explainability:
            violations.append(
                "EU AI Act Art. 13: High-risk AI requires explainability"
            )
        
        # Check human oversight
        if not inference.human_oversight:
            violations.append(
                "EU AI Act Art. 14: High-risk AI requires human oversight"
            )
        
        # Check confidence threshold
        if inference.confidence_score < 0.7:
            violations.append(
                "FDA CDS Software: Low confidence score requires human review"
            )
        
        # Check bias metrics
        if inference.bias_metrics:
            if inference.bias_metrics.get("overall_bias_detected"):
                violations.append(
                    "ISO 42001: Bias detected in AI inference"
                )
        
        is_valid = len(violations) == 0
        
        if not is_valid:
            logger.warning(f"‚ö†Ô∏è High-risk AI validation failed: {len(violations)} violations")
        
        return is_valid, violations
    
    def create_conformity_assessment(
        self,
        model_id: str,
        risk_class: AIRiskClass,
        technical_documentation: Dict,
        assessor: str = "Internal"
    ) -> ConformityAssessment:
        """
        Create EU AI Act conformity assessment
        
        Args:
            model_id: Model identifier
            risk_class: AI risk classification
            technical_documentation: Technical docs
            assessor: Assessment authority
        
        Returns:
            ConformityAssessment
        """
        assessment = ConformityAssessment(
            assessment_id=f"CA-{model_id}-{datetime.utcnow().strftime('%Y%m%d')}",
            model_id=model_id,
            risk_class=risk_class,
            assessment_date=datetime.utcnow().isoformat(),
            assessor=assessor,
            technical_documentation=technical_documentation,
            quality_management={"iso_9001_compliant": True},
            risk_management={"iso_14971_compliant": True},
            data_governance={"data_quality_verified": True},
            transparency_obligations={"documentation_complete": True},
            human_oversight_measures={"oversight_enabled": True},
            accuracy_metrics=technical_documentation.get("accuracy", {}),
            robustness_metrics=technical_documentation.get("robustness", {}),
            cybersecurity_measures={"penetration_tested": True},
            conformity_status="CONFORMANT"
        )
        
        self.conformity_assessments[model_id] = assessment
        
        logger.info(f"‚úÖ Conformity assessment created: {assessment.assessment_id}")
        return assessment
    
    def log_post_market_performance(
        self,
        model_id: str,
        inference: AIInference,
        ground_truth: Optional[Any] = None
    ):
        """
        Log post-market performance for continuous monitoring
        
        Complies with:
        - EU AI Act Art. 61 (Post-market monitoring)
        - IMDRF AI-SaMD Real-World Performance
        """
        performance_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "model_id": model_id,
            "prediction": inference.prediction,
            "confidence": inference.confidence_score,
            "ground_truth": ground_truth,
            "correct": inference.prediction == ground_truth if ground_truth else None
        }
        
        self.performance_log.append(performance_entry)
        
        # Calculate rolling accuracy (last 100 predictions)
        recent_entries = [e for e in self.performance_log[-100:] if e["correct"] is not None]
        if recent_entries:
            accuracy = sum(e["correct"] for e in recent_entries) / len(recent_entries)
            
            if accuracy < 0.8:
                logger.warning(f"‚ö†Ô∏è Model {model_id} accuracy degraded: {accuracy:.2%}")
    
    def generate_transparency_report(
        self,
        model_id: str,
        reporting_period: str = "Q1 2025"
    ) -> Dict[str, Any]:
        """
        Generate AI transparency report
        
        Complies with:
        - EU AI Act Art. 13
        - ISO 42001 Clause 9.1
        """
        # Filter performance log for model
        model_entries = [e for e in self.performance_log if e["model_id"] == model_id]
        
        # Calculate metrics
        total_inferences = len(model_entries)
        correct_predictions = sum(e["correct"] for e in model_entries if e["correct"] is not None)
        accuracy = correct_predictions / total_inferences if total_inferences > 0 else 0
        
        avg_confidence = np.mean([e["confidence"] for e in model_entries]) if model_entries else 0
        
        report = {
            "model_id": model_id,
            "reporting_period": reporting_period,
            "generated_at": datetime.utcnow().isoformat(),
            "total_inferences": total_inferences,
            "accuracy": accuracy,
            "average_confidence": float(avg_confidence),
            "conformity_status": self.conformity_assessments.get(model_id, {}).conformity_status if model_id in self.conformity_assessments else "UNKNOWN",
            "compliance_frameworks": [
                "EU AI Act (Regulation 2024/1689)",
                "FDA CDS Software Guidance",
                "ISO/IEC 42001"
            ]
        }
        
        logger.info(f"üìä Transparency report generated: {model_id}")
        return report


# Example usage
if __name__ == "__main__":
    # Initialize AI Governance
    governance = AIGovernance(
        enable_shap=True,
        enable_bias_detection=True
    )
    
    # Classify AI risk
    risk_class = governance.classify_ai_risk(
        use_case="Cholera outbreak prediction",
        domain="health",
        impact="high"
    )
    print(f"Risk Classification: {risk_class.value}")
    
    # Create conformity assessment
    assessment = governance.create_conformity_assessment(
        model_id="FRENASA-ECF-v1",
        risk_class=risk_class,
        technical_documentation={
            "accuracy": {"train": 0.95, "test": 0.92},
            "robustness": {"adversarial_tested": True}
        }
    )
    print(f"Conformity Assessment: {assessment.assessment_id}")
    
    # Simulate AI inference with explainability
    # (In production, this would use actual model)
    inference = AIInference(
        model_id="FRENASA-ECF-v1",
        input_features={"fever": 1, "diarrhea": 1, "vomiting": 1},
        prediction="cholera",
        confidence_score=0.92,
        risk_class=risk_class,
        explainability={
            "method": "SHAP",
            "top_3_features": ["diarrhea", "vomiting", "fever"]
        },
        human_oversight=True
    )
    
    # Validate high-risk inference
    is_valid, violations = governance.validate_high_risk_inference(inference)
    print(f"\nValidation Result: {'‚úÖ VALID' if is_valid else '‚ùå INVALID'}")
    if violations:
        for v in violations:
            print(f"  - {v}")
    
    # Generate transparency report
    report = governance.generate_transparency_report("FRENASA-ECF-v1")
    print(f"\nTransparency Report:")
    print(json.dumps(report, indent=2))
