"""
AI Governance Module
EU AI Act (Regulation 2024/1689) Compliance Engine

Implements:
- High-risk AI classification (Â§6)
- Conformity assessment (Â§43)
- Transparency obligations (Â§13)
- Human oversight (Â§14)
- Post-market monitoring (Â§61)
- Explainability (SHAP/LIME)
"""

import logging
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from enum import Enum
from dataclasses import dataclass
import json

logger = logging.getLogger(__name__)


class AIRiskLevel(Enum):
    """AI risk classification per EU AI Act Â§6"""
    UNACCEPTABLE = "unacceptable"  # Prohibited
    HIGH = "high"  # Requires conformity assessment
    LIMITED = "limited"  # Transparency obligations
    MINIMAL = "minimal"  # No specific requirements


class AISystemType(Enum):
    """Types of AI systems in health context"""
    CLINICAL_DECISION_SUPPORT = "clinical_decision_support"
    OUTBREAK_PREDICTION = "outbreak_prediction"
    DIAGNOSTIC_ASSISTANCE = "diagnostic_assistance"
    TREATMENT_RECOMMENDATION = "treatment_recommendation"
    RESOURCE_ALLOCATION = "resource_allocation"
    SURVEILLANCE = "surveillance"
    TRIAGE = "triage"


@dataclass
class AISystemMetadata:
    """Metadata for AI system registration"""
    system_id: str
    system_type: AISystemType
    risk_level: AIRiskLevel
    intended_purpose: str
    target_population: str
    deployment_date: datetime
    version: str
    training_data_description: str
    performance_metrics: Dict
    known_limitations: List[str]


@dataclass
class ConformityAssessment:
    """EU AI Act Â§43 conformity assessment"""
    assessment_id: str
    system_id: str
    assessment_date: datetime
    assessor: str
    risk_management_adequate: bool
    data_governance_adequate: bool
    technical_documentation_complete: bool
    transparency_requirements_met: bool
    human_oversight_implemented: bool
    accuracy_robustness_verified: bool
    cybersecurity_adequate: bool
    conformity_status: str  # CONFORMANT | NON_CONFORMANT | PENDING
    findings: List[str]
    recommendations: List[str]


@dataclass
class ExplainabilityReport:
    """Explainability report for high-risk AI decisions"""
    report_id: str
    system_id: str
    decision_id: str
    timestamp: datetime
    input_features: Dict
    prediction: any
    confidence_score: float
    explanation_method: str  # SHAP | LIME | Feature_Importance
    feature_contributions: Dict
    decision_rationale: str
    evidence_chain: List[str]
    bias_assessment: Dict
    human_review_required: bool


class AIGovernanceEngine:
    """
    Central engine for AI governance and EU AI Act compliance.
    
    Capabilities:
    - Risk classification
    - Conformity assessment
    - Explainability generation
    - Post-market monitoring
    - Human oversight enforcement
    """
    
    def __init__(self, enable_strict_mode: bool = True):
        self.enable_strict_mode = enable_strict_mode
        self.registered_systems: Dict[str, AISystemMetadata] = {}
        self.conformity_assessments: Dict[str, ConformityAssessment] = {}
        self.explainability_reports: List[ExplainabilityReport] = []
        
        logger.info("ðŸ¤– AI Governance Engine initialized (EU AI Act compliance)")
    
    def classify_risk(
        self,
        system_type: AISystemType,
        intended_purpose: str,
        affects_health: bool = True,
        affects_safety: bool = False,
        affects_fundamental_rights: bool = False
    ) -> AIRiskLevel:
        """
        Classify AI system risk level per EU AI Act Â§6.
        
        Health AI systems are typically HIGH risk.
        """
        # Unacceptable risk (prohibited)
        if "social_scoring" in intended_purpose.lower():
            return AIRiskLevel.UNACCEPTABLE
        
        # High risk (requires conformity assessment)
        if affects_health or affects_safety or affects_fundamental_rights:
            return AIRiskLevel.HIGH
        
        # Clinical decision support is always high risk
        if system_type in [
            AISystemType.CLINICAL_DECISION_SUPPORT,
            AISystemType.DIAGNOSTIC_ASSISTANCE,
            AISystemType.TREATMENT_RECOMMENDATION
        ]:
            return AIRiskLevel.HIGH
        
        # Outbreak prediction with public health impact
        if system_type == AISystemType.OUTBREAK_PREDICTION:
            return AIRiskLevel.HIGH
        
        # Limited risk (transparency obligations)
        if system_type == AISystemType.SURVEILLANCE:
            return AIRiskLevel.LIMITED
        
        # Minimal risk
        return AIRiskLevel.MINIMAL
    
    def register_ai_system(
        self,
        metadata: AISystemMetadata
    ) -> Dict:
        """
        Register an AI system for governance tracking.
        
        Required for all high-risk AI systems per EU AI Act Â§16.
        """
        self.registered_systems[metadata.system_id] = metadata
        
        logger.info(f"âœ… AI System registered: {metadata.system_id} (Risk: {metadata.risk_level.value})")
        
        # Trigger conformity assessment for high-risk systems
        if metadata.risk_level == AIRiskLevel.HIGH:
            logger.warning(f"âš ï¸ High-risk AI system - Conformity assessment required: {metadata.system_id}")
        
        return {
            "system_id": metadata.system_id,
            "risk_level": metadata.risk_level.value,
            "conformity_assessment_required": metadata.risk_level == AIRiskLevel.HIGH,
            "registration_date": datetime.utcnow().isoformat()
        }
    
    def perform_conformity_assessment(
        self,
        system_id: str,
        assessor: str
    ) -> ConformityAssessment:
        """
        Perform EU AI Act Â§43 conformity assessment for high-risk AI.
        
        Checks:
        - Risk management system
        - Data governance
        - Technical documentation
        - Transparency
        - Human oversight
        - Accuracy and robustness
        - Cybersecurity
        """
        if system_id not in self.registered_systems:
            raise ValueError(f"AI system not registered: {system_id}")
        
        metadata = self.registered_systems[system_id]
        
        if metadata.risk_level != AIRiskLevel.HIGH:
            raise ValueError(f"Conformity assessment only required for high-risk AI: {system_id}")
        
        # Perform assessment checks
        assessment = ConformityAssessment(
            assessment_id=f"CA-{system_id}-{datetime.utcnow().strftime('%Y%m%d')}",
            system_id=system_id,
            assessment_date=datetime.utcnow(),
            assessor=assessor,
            risk_management_adequate=True,  # Would check actual risk management
            data_governance_adequate=True,  # Would check data quality/governance
            technical_documentation_complete=True,  # Would verify documentation
            transparency_requirements_met=True,  # Would check transparency
            human_oversight_implemented=True,  # Would verify human oversight
            accuracy_robustness_verified=True,  # Would check performance metrics
            cybersecurity_adequate=True,  # Would verify security measures
            conformity_status="CONFORMANT",
            findings=[],
            recommendations=[]
        )
        
        # Check if all requirements are met
        if not all([
            assessment.risk_management_adequate,
            assessment.data_governance_adequate,
            assessment.technical_documentation_complete,
            assessment.transparency_requirements_met,
            assessment.human_oversight_implemented,
            assessment.accuracy_robustness_verified,
            assessment.cybersecurity_adequate
        ]):
            assessment.conformity_status = "NON_CONFORMANT"
        
        self.conformity_assessments[assessment.assessment_id] = assessment
        
        logger.info(f"ðŸ“‹ Conformity assessment complete: {assessment.assessment_id} - {assessment.conformity_status}")
        
        return assessment
    
    def generate_explainability_report(
        self,
        system_id: str,
        decision_id: str,
        input_features: Dict,
        prediction: any,
        confidence_score: float,
        explanation_method: str = "SHAP"
    ) -> ExplainabilityReport:
        """
        Generate explainability report for high-risk AI decision.
        
        Required per EU AI Act Â§13 (Transparency) and Â§14 (Human Oversight).
        """
        if system_id not in self.registered_systems:
            raise ValueError(f"AI system not registered: {system_id}")
        
        metadata = self.registered_systems[system_id]
        
        # High-risk AI requires explainability
        if metadata.risk_level == AIRiskLevel.HIGH and confidence_score > 0.7:
            human_review_required = True
        else:
            human_review_required = False
        
        # Generate feature contributions (would use actual SHAP/LIME)
        feature_contributions = self._calculate_feature_contributions(
            input_features,
            explanation_method
        )
        
        # Generate decision rationale
        decision_rationale = self._generate_decision_rationale(
            prediction,
            feature_contributions,
            confidence_score
        )
        
        # Bias assessment
        bias_assessment = self._assess_bias(input_features, prediction)
        
        report = ExplainabilityReport(
            report_id=f"EXP-{system_id}-{decision_id}",
            system_id=system_id,
            decision_id=decision_id,
            timestamp=datetime.utcnow(),
            input_features=input_features,
            prediction=prediction,
            confidence_score=confidence_score,
            explanation_method=explanation_method,
            feature_contributions=feature_contributions,
            decision_rationale=decision_rationale,
            evidence_chain=self._build_evidence_chain(input_features, prediction),
            bias_assessment=bias_assessment,
            human_review_required=human_review_required
        )
        
        self.explainability_reports.append(report)
        
        logger.info(f"ðŸ“Š Explainability report generated: {report.report_id}")
        
        if human_review_required:
            logger.warning(f"ðŸ‘¤ Human review required for decision: {decision_id}")
        
        return report
    
    def _calculate_feature_contributions(
        self,
        input_features: Dict,
        method: str
    ) -> Dict:
        """Calculate feature contributions using SHAP/LIME"""
        # Simplified - would use actual SHAP/LIME library
        contributions = {}
        for feature, value in input_features.items():
            # Mock contribution calculation
            contributions[feature] = 0.1  # Would be actual SHAP value
        return contributions
    
    def _generate_decision_rationale(
        self,
        prediction: any,
        feature_contributions: Dict,
        confidence_score: float
    ) -> str:
        """Generate human-readable decision rationale"""
        top_features = sorted(
            feature_contributions.items(),
            key=lambda x: abs(x[1]),
            reverse=True
        )[:3]
        
        rationale = f"Prediction: {prediction} (confidence: {confidence_score:.2%}). "
        rationale += f"Top contributing factors: {', '.join([f[0] for f in top_features])}"
        
        return rationale
    
    def _build_evidence_chain(
        self,
        input_features: Dict,
        prediction: any
    ) -> List[str]:
        """Build evidence chain for decision"""
        chain = []
        for feature, value in input_features.items():
            chain.append(f"{feature}={value}")
        chain.append(f"prediction={prediction}")
        return chain
    
    def _assess_bias(
        self,
        input_features: Dict,
        prediction: any
    ) -> Dict:
        """Assess potential bias in decision"""
        # Simplified bias assessment
        return {
            "demographic_parity": 0.95,  # Would calculate actual metrics
            "equal_opportunity": 0.93,
            "bias_detected": False
        }
    
    def post_market_monitoring(
        self,
        system_id: str,
        performance_data: Dict
    ) -> Dict:
        """
        Post-market monitoring per EU AI Act Â§61.
        
        Tracks real-world performance and detects drift.
        """
        if system_id not in self.registered_systems:
            raise ValueError(f"AI system not registered: {system_id}")
        
        metadata = self.registered_systems[system_id]
        
        # Compare real-world performance to training metrics
        training_accuracy = metadata.performance_metrics.get("accuracy", 0.0)
        real_world_accuracy = performance_data.get("accuracy", 0.0)
        
        drift_detected = abs(training_accuracy - real_world_accuracy) > 0.05
        
        monitoring_report = {
            "system_id": system_id,
            "monitoring_date": datetime.utcnow().isoformat(),
            "training_accuracy": training_accuracy,
            "real_world_accuracy": real_world_accuracy,
            "drift_detected": drift_detected,
            "action_required": drift_detected
        }
        
        if drift_detected:
            logger.warning(f"âš ï¸ Performance drift detected: {system_id}")
            logger.warning(f"   Training: {training_accuracy:.2%}, Real-world: {real_world_accuracy:.2%}")
        
        return monitoring_report
    
    def enforce_human_oversight(
        self,
        system_id: str,
        decision_id: str,
        confidence_score: float
    ) -> Dict:
        """
        Enforce human oversight per EU AI Act Â§14.
        
        High-risk decisions require human review.
        """
        if system_id not in self.registered_systems:
            raise ValueError(f"AI system not registered: {system_id}")
        
        metadata = self.registered_systems[system_id]
        
        # High-risk AI with high confidence requires human oversight
        requires_oversight = (
            metadata.risk_level == AIRiskLevel.HIGH and
            confidence_score > 0.7
        )
        
        return {
            "system_id": system_id,
            "decision_id": decision_id,
            "requires_human_oversight": requires_oversight,
            "reason": "High-risk AI decision with high confidence" if requires_oversight else "No oversight required"
        }
    
    def generate_transparency_report(
        self,
        system_id: str
    ) -> Dict:
        """
        Generate transparency report per EU AI Act Â§13.
        
        Required for users to understand AI system capabilities and limitations.
        """
        if system_id not in self.registered_systems:
            raise ValueError(f"AI system not registered: {system_id}")
        
        metadata = self.registered_systems[system_id]
        
        return {
            "system_id": system_id,
            "system_type": metadata.system_type.value,
            "risk_level": metadata.risk_level.value,
            "intended_purpose": metadata.intended_purpose,
            "target_population": metadata.target_population,
            "performance_metrics": metadata.performance_metrics,
            "known_limitations": metadata.known_limitations,
            "training_data": metadata.training_data_description,
            "version": metadata.version,
            "deployment_date": metadata.deployment_date.isoformat(),
            "conformity_status": self._get_conformity_status(system_id)
        }
    
    def _get_conformity_status(self, system_id: str) -> str:
        """Get latest conformity status for system"""
        assessments = [
            a for a in self.conformity_assessments.values()
            if a.system_id == system_id
        ]
        if not assessments:
            return "NOT_ASSESSED"
        latest = max(assessments, key=lambda a: a.assessment_date)
        return latest.conformity_status


# Example usage
if __name__ == "__main__":
    engine = AIGovernanceEngine(enable_strict_mode=True)
    
    # Register outbreak prediction AI
    metadata = AISystemMetadata(
        system_id="FRENASA-OUTBREAK-PRED-v1",
        system_type=AISystemType.OUTBREAK_PREDICTION,
        risk_level=AIRiskLevel.HIGH,
        intended_purpose="Predict cholera outbreaks in refugee camps",
        target_population="Dadaab refugee camp residents",
        deployment_date=datetime.utcnow(),
        version="1.0.0",
        training_data_description="Historical outbreak data 2015-2024, CBS reports, EMR records",
        performance_metrics={"accuracy": 0.92, "precision": 0.89, "recall": 0.94},
        known_limitations=["Limited data for rare diseases", "Requires internet for cloud inference"]
    )
    
    registration = engine.register_ai_system(metadata)
    print(f"âœ… System registered: {registration}")
    
    # Perform conformity assessment
    assessment = engine.perform_conformity_assessment(
        system_id="FRENASA-OUTBREAK-PRED-v1",
        assessor="Dr. Jane Smith, AI Safety Officer"
    )
    print(f"\nðŸ“‹ Conformity Assessment: {assessment.conformity_status}")
    
    # Generate explainability report for a prediction
    report = engine.generate_explainability_report(
        system_id="FRENASA-OUTBREAK-PRED-v1",
        decision_id="PRED-20251223-001",
        input_features={
            "diarrhea_cases": 15,
            "vomiting_cases": 12,
            "water_quality_score": 0.3,
            "population_density": 0.8
        },
        prediction="HIGH_RISK_OUTBREAK",
        confidence_score=0.87,
        explanation_method="SHAP"
    )
    print(f"\nðŸ“Š Explainability Report: {report.report_id}")
    print(f"   Decision: {report.prediction}")
    print(f"   Confidence: {report.confidence_score:.2%}")
    print(f"   Rationale: {report.decision_rationale}")
    print(f"   Human Review Required: {report.human_review_required}")
