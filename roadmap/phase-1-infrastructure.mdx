---
title: 'Phase 1: Multi-Cloud Liberation'
description: 'Decoupling from GCP for sovereign resilience'
icon: 'cloud'
---

## Objective

Transcend vendor lock-in by replacing GCP-specific code with portable, containerized logic. This ensures iLuminara can run on AWS, Azure, or bare metal servers in a bunker.

<Card
  title="Strategic imperative"
  icon="flag"
>
  "A sovereign health system cannot depend on a single cloud provider. True sovereignty requires the ability to deploy anywhere, instantly."
</Card>

## Current architecture (GCP-locked)

```
┌─────────────────────────────────────┐
│         GOOGLE CLOUD PLATFORM       │
│  ┌──────────────────────────────┐  │
│  │  Cloud Run (API)             │  │
│  │  Vertex AI (Forecasting)     │  │
│  │  BigQuery (Analytics)        │  │
│  │  Cloud Spanner (Audit)       │  │
│  │  Cloud KMS (Crypto)          │  │
│  └──────────────────────────────┘  │
└─────────────────────────────────────┘
```

**Problems:**
- ❌ Cannot deploy to AWS/Azure without rewriting code
- ❌ GCP outage = complete system failure
- ❌ Pricing changes create financial risk
- ❌ Data sovereignty violations if GCP changes regions

## Target architecture (Cloud-agnostic)

```
┌─────────────────────────────────────────────────────────────┐
│              TERRAFORM INFRASTRUCTURE LAYER                 │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐     │
│  │   AWS        │  │   AZURE      │  │   GCP        │     │
│  │   EKS        │  │   AKS        │  │   GKE        │     │
│  └──────────────┘  └──────────────┘  └──────────────┘     │
└─────────────────────────────────────────────────────────────┘
              │                │                │
              └────────────────┼────────────────┘
                               ▼
                    ┌──────────────────────┐
                    │   KUBERNETES PODS    │
                    │  - FRENASA Engine    │
                    │  - AI Agents         │
                    │  - Governance Kernel │
                    └──────────────────────┘
```

**Benefits:**
- ✅ Deploy to any cloud in <24 hours
- ✅ Multi-cloud redundancy (no single point of failure)
- ✅ Negotiate pricing across providers
- ✅ Data sovereignty by region selection

## Architecture upgrades

### 1. Infrastructure-as-Code (Terraform)

Replace manual GCP console clicks with declarative infrastructure definitions.

**Before (manual):**
```bash
gcloud run deploy frenasa-api --image gcr.io/iluminara/frenasa:latest
gcloud sql instances create audit-db --tier db-n1-standard-1
```

**After (Terraform):**
```hcl
# terraform/main.tf

module "kubernetes_cluster" {
  source = "./modules/k8s"
  
  cloud_provider = var.cloud_provider  # "aws" | "azure" | "gcp"
  region         = var.region
  node_count     = var.node_count
}

module "storage" {
  source = "./modules/storage"
  
  provider = var.cloud_provider
  # Automatically maps to S3, Azure Blob, or GCS
}

module "database" {
  source = "./modules/database"
  
  provider = var.cloud_provider
  # Automatically maps to RDS, Azure SQL, or Cloud SQL
}
```

**Deploy to AWS:**
```bash
terraform apply -var="cloud_provider=aws" -var="region=af-south-1"
```

**Deploy to Azure:**
```bash
terraform apply -var="cloud_provider=azure" -var="region=southafricawest"
```

### 2. Kubernetes migration

Containerize all services for portability across clouds.

**Helm chart structure:**
```
iluminara-core/
├── Chart.yaml
├── values.yaml
├── templates/
│   ├── frenasa-deployment.yaml
│   ├── ai-agents-deployment.yaml
│   ├── governance-kernel-deployment.yaml
│   ├── api-service.yaml
│   └── ingress.yaml
```

**Deploy to any Kubernetes cluster:**
```bash
# AWS EKS
helm install iluminara ./iluminara-core --set cloud=aws

# Azure AKS
helm install iluminara ./iluminara-core --set cloud=azure

# GCP GKE
helm install iluminara ./iluminara-core --set cloud=gcp
```

### 3. Storage abstraction layer

Unified interface for S3, Azure Blob, and Google Cloud Storage.

**Before (GCP-specific):**
```python
from google.cloud import storage

client = storage.Client()
bucket = client.bucket('iluminara-data')
blob = bucket.blob('patient_records.json')
blob.upload_from_string(data)
```

**After (cloud-agnostic):**
```python
from iluminara.storage import CloudStorage

storage = CloudStorage(provider=os.getenv('CLOUD_PROVIDER'))
storage.upload('patient_records.json', data)

# Automatically routes to:
# - S3 if provider=aws
# - Azure Blob if provider=azure
# - GCS if provider=gcp
```

**Implementation:**
```python
# iluminara/storage.py

class CloudStorage:
    def __init__(self, provider: str):
        if provider == 'aws':
            self.client = boto3.client('s3')
        elif provider == 'azure':
            self.client = BlobServiceClient()
        elif provider == 'gcp':
            self.client = storage.Client()
    
    def upload(self, key: str, data: bytes):
        if isinstance(self.client, boto3.client):
            self.client.put_object(Bucket=BUCKET, Key=key, Body=data)
        elif isinstance(self.client, BlobServiceClient):
            blob_client = self.client.get_blob_client(CONTAINER, key)
            blob_client.upload_blob(data)
        elif isinstance(self.client, storage.Client):
            bucket = self.client.bucket(BUCKET)
            blob = bucket.blob(key)
            blob.upload_from_string(data)
```

### 4. Database abstraction

Unified interface for RDS, Azure SQL, and Cloud SQL.

**Before (GCP-specific):**
```python
from google.cloud import spanner

client = spanner.Client()
instance = client.instance('iluminara-audit')
database = instance.database('audit-trail')
```

**After (cloud-agnostic):**
```python
from iluminara.database import Database

db = Database(provider=os.getenv('CLOUD_PROVIDER'))
db.execute("INSERT INTO audit_trail VALUES (...)")

# Automatically routes to:
# - RDS PostgreSQL if provider=aws
# - Azure SQL if provider=azure
# - Cloud Spanner if provider=gcp
```

### 5. MLflow for model portability

Replace Vertex AI with MLflow for cloud-agnostic model training.

**Before (Vertex AI):**
```python
from google.cloud import aiplatform

aiplatform.init(project='iluminara', location='us-central1')

job = aiplatform.CustomTrainingJob(
    display_name='cholera-forecast',
    script_path='train.py'
)
job.run()
```

**After (MLflow):**
```python
import mlflow

mlflow.set_tracking_uri(os.getenv('MLFLOW_TRACKING_URI'))

with mlflow.start_run():
    model = train_cholera_forecast()
    mlflow.sklearn.log_model(model, 'cholera-forecast')

# Deploy to any cloud:
# - AWS SageMaker
# - Azure ML
# - Vertex AI
```

## Auto-scaling configuration

Horizontal Pod Autoscaling based on real-time inference load.

```yaml
# kubernetes/hpa.yaml

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: frenasa-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: frenasa-engine
  minReplicas: 2
  maxReplicas: 100
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: inference_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
```

**Behavior:**
- Normal load: 2 pods
- Outbreak detected: Auto-scales to 100 pods in <60 seconds
- Cost optimization: Scales down when load decreases

## Migration strategy

<Steps>
  <Step title="Week 1-2: Terraform setup">
    Create IaC modules for AWS, Azure, and GCP
  </Step>
  <Step title="Week 3-4: Containerization">
    Package all services as Docker containers with Helm charts
  </Step>
  <Step title="Week 5-6: Storage abstraction">
    Implement CloudStorage and Database abstraction layers
  </Step>
  <Step title="Week 7-8: MLflow migration">
    Replace Vertex AI with MLflow for model training
  </Step>
  <Step title="Week 9-10: Testing">
    Deploy to AWS and Azure test environments
  </Step>
  <Step title="Week 11-12: Production cutover">
    Migrate production traffic to multi-cloud architecture
  </Step>
</Steps>

## Cost comparison

| Provider | Monthly Cost (10K users) | Sovereignty | Latency (Africa) |
|----------|--------------------------|-------------|------------------|
| **GCP only** | $4,200 | ❌ Single vendor | 120ms |
| **AWS only** | $3,800 | ❌ Single vendor | 95ms |
| **Azure only** | $4,500 | ❌ Single vendor | 110ms |
| **Multi-cloud** | $4,000 | ✅ Sovereign | 85ms (optimized routing) |

**Multi-cloud advantages:**
- 5% cost savings through competitive pricing
- 30% latency reduction through regional optimization
- 99.99% uptime through redundancy
- Zero vendor lock-in risk

## Success metrics

<AccordionGroup>
  <Accordion title="Deployment velocity">
    - ✅ Deploy to AWS in <24 hours
    - ✅ Deploy to Azure in <24 hours
    - ✅ Deploy to bare metal in <48 hours
  </Accordion>
  <Accordion title="Performance">
    - ✅ <5% performance degradation vs. GCP-native
    - ✅ <100ms API latency (p95)
    - ✅ Auto-scale from 2 to 100 pods in <60 seconds
  </Accordion>
  <Accordion title="Cost efficiency">
    - ✅ 5-10% cost reduction through multi-cloud arbitrage
    - ✅ Zero egress fees through regional optimization
  </Accordion>
  <Accordion title="Sovereignty">
    - ✅ Data residency in sovereign territory
    - ✅ Zero code changes for regional deployment
    - ✅ Instant failover to backup cloud provider
  </Accordion>
</AccordionGroup>

## Next steps

<CardGroup cols={2}>
  <Card
    title="Phase 2: Resilience"
    icon="network-wired"
    href="/roadmap/phase-2-resilience"
  >
    Event-driven architecture for fault tolerance
  </Card>
  <Card
    title="Terraform modules"
    icon="code"
    href="/deployment/terraform"
  >
    Infrastructure-as-Code implementation guide
  </Card>
  <Card
    title="Kubernetes setup"
    icon="dharmachakra"
    href="/deployment/kubernetes"
  >
    Container orchestration configuration
  </Card>
  <Card
    title="MLflow integration"
    icon="flask"
    href="/ai-agents/mlflow"
  >
    Cloud-agnostic model training
  </Card>
</CardGroup>
