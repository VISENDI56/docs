---
title: Vertex AI + SHAP integration
description: Explainable AI with SHAP analysis for regulatory compliance
---

## Overview

The Vertex AI + SHAP integration provides **Right to Explanation** capabilities required by EU AI Act §6, GDPR Art. 22, and NIST AI RMF. Every high-risk AI inference includes SHAP (SHapley Additive exPlanations) analysis for complete transparency.

<Card
  title="Philosophy"
  icon="lightbulb"
>
  "Every clinical decision with consequence must be explicable."
</Card>

## Architecture

```
┌────────────────────────────────────────────────────────────────┐
│                   VERTEX AI + SHAP INTEGRATION                  │
└────────────────────────────────────────────────────────────────┘
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
   ┌────▼────┐      ┌──────▼──────┐    ┌──────▼──────┐
   │ VERTEX  │      │    SHAP     │    │  GOVERNANCE │
   │   AI    │──────│  EXPLAINER  │────│   KERNEL    │
   │ (Model) │      │  (Analysis) │    │ (Validation)│
   └─────────┘      └─────────────┘    └─────────────┘
```

## Key features

<CardGroup cols={2}>\n  <Card title=\"SHAP analysis\" icon=\"chart-line\">\n    Feature importance attribution for all predictions\n  </Card>\n  <Card title=\"Regulatory compliance\" icon=\"scale-balanced\">\n    EU AI Act §6, GDPR Art. 22, NIST AI RMF\n  </Card>\n  <Card title=\"Audit trail\" icon=\"file-contract\">\n    Tamper-proof explanation logging\n  </Card>\n  <Card title=\"Real-time\" icon=\"bolt\">\n    <5 second explanation generation\n  </Card>\n</CardGroup>

## Setup

### Prerequisites

```bash\n# Install dependencies\npip install google-cloud-aiplatform shap numpy pandas\n\n# Set environment variables\nexport GOOGLE_CLOUD_PROJECT=\"your-project-id\"\nexport VERTEX_AI_REGION=\"us-central1\"\n```

### Initialize explainer

```python\nfrom governance_kernel.humanitarian_constraints import VertexAIExplainableAI\n\nexplainer = VertexAIExplainableAI(\n    project_id=\"iluminara-core\",\n    region=\"us-central1\"\n)\n```

## Basic usage

### Explain a prediction

```python\n# Generate explanation for outbreak prediction\nexplanation = explainer.explain_prediction(\n    model_id=\"outbreak-predictor-v1\",\n    input_data={\n        \"cbs_signals\": 45,\n        \"z_score\": 3.8,\n        \"location\": \"Dadaab\",\n        \"population_density\": 12500,\n        \"healthcare_capacity\": 0.3\n    },\n    prediction=\"OUTBREAK_LIKELY\",\n    feature_names=[\"cbs_signals\", \"z_score\", \"location\", \"population_density\", \"healthcare_capacity\"]\n)\n\n# Get top contributing factors\ntop_factors = explanation.get_top_contributors(n=3)\nprint(f\"Top 3 factors: {top_factors}\")\n# Output: [(\"z_score\", 0.42), (\"cbs_signals\", 0.31), (\"healthcare_capacity\", 0.18)]\n\n# Get full SHAP values\nshap_values = explanation.shap_values\nprint(f\"SHAP values: {shap_values}\")\n```

### Validate with SovereignGuardrail

```python\nfrom governance_kernel.vector_ledger import SovereignGuardrail\n\nguardrail = SovereignGuardrail()\n\n# Validate high-risk inference\nguardrail.validate_action(\n    action_type=\"High_Risk_Inference\",\n    payload={\n        \"explanation\": explanation.to_dict(),\n        \"confidence_score\": 0.87,\n        \"evidence_chain\": [\"z_score_spike\", \"cbs_cluster\", \"low_capacity\"],\n        \"consent_token\": \"EMERGENCY_OVERRIDE\",\n        \"consent_scope\": \"public_health_surveillance\"\n    },\n    jurisdiction=\"EU_AI_ACT\"\n)\n\nprint(\"✅ High-risk inference validated with explainability\")\n```

## SHAP explanation structure

```python\nclass SHAPExplanation:\n    \"\"\"\n    SHAP explanation for a single prediction.\n    \n    Attributes:\n        model_id: Vertex AI model identifier\n        prediction: Model prediction value\n        shap_values: SHAP values for each feature\n        base_value: Expected value (baseline)\n        feature_names: Names of input features\n        feature_values: Actual feature values\n        timestamp: When explanation was generated\n    \"\"\"\n    \n    def get_top_contributors(self, n: int = 5) -> List[Tuple[str, float]]:\n        \"\"\"Get top N contributing features by absolute SHAP value.\"\"\"\n        pass\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for audit logging.\"\"\"\n        pass\n    \n    def visualize(self, output_path: str = None):\n        \"\"\"Generate SHAP waterfall plot.\"\"\"\n        pass\n```

## Integration with Golden Thread

```python\nfrom edge_node.sync_protocol.golden_thread import GoldenThread\n\ngt = GoldenThread()\nexplainer = VertexAIExplainableAI()\n\n# Fuse data streams\nfused_record = gt.fuse_data_streams(\n    cbs_signal={\"location\": \"Nairobi\", \"symptom\": \"fever\"},\n    emr_record={\"location\": \"Nairobi\", \"diagnosis\": \"malaria\"},\n    patient_id=\"PAT-12345\"\n)\n\n# Generate risk prediction\nrisk_score = model.predict(fused_record)\n\n# Explain the prediction\nexplanation = explainer.explain_prediction(\n    model_id=\"risk-model-v2\",\n    input_data=fused_record,\n    prediction=risk_score,\n    feature_names=list(fused_record.keys())\n)\n\n# Validate with governance\nguardrail.validate_action(\n    action_type=\"High_Risk_Inference\",\n    payload={\n        \"explanation\": explanation.to_dict(),\n        \"confidence_score\": risk_score,\n        \"evidence_chain\": explanation.get_top_contributors(n=3)\n    },\n    jurisdiction=\"GDPR_EU\"\n)\n```

## Compliance requirements

### EU AI Act §6 (High-Risk AI)

```python\n# High-risk AI systems require:\n# 1. Risk pyramid classification\n# 2. Conformity assessment\n# 3. Post-market monitoring\n# 4. Explainability\n\nexplanation = explainer.explain_prediction(\n    model_id=\"high-risk-model\",\n    input_data=input_data,\n    prediction=prediction,\n    feature_names=feature_names,\n    metadata={\n        \"risk_classification\": \"HIGH_RISK\",\n        \"conformity_assessment\": \"PASSED\",\n        \"post_market_monitoring\": \"ENABLED\"\n    }\n)\n```

### GDPR Art. 22 (Right to Explanation)

```python\n# Automated decisions require:\n# 1. Meaningful information about the logic\n# 2. Significance and consequences\n# 3. Right to human intervention\n\nexplanation = explainer.explain_prediction(\n    model_id=\"automated-decision-model\",\n    input_data=input_data,\n    prediction=prediction,\n    feature_names=feature_names,\n    metadata={\n        \"logic_description\": \"Gradient boosting with SHAP\",\n        \"consequences\": \"Resource allocation decision\",\n        \"human_review_available\": True\n    }\n)\n```

### NIST AI RMF (Trustworthiness)

```python\n# AI systems must be:\n# 1. Valid and Reliable\n# 2. Safe\n# 3. Secure and Resilient\n# 4. Accountable and Transparent\n# 5. Explainable and Interpretable\n# 6. Privacy-Enhanced\n# 7. Fair with Harmful Bias Managed\n\nexplanation = explainer.explain_prediction(\n    model_id=\"trustworthy-model\",\n    input_data=input_data,\n    prediction=prediction,\n    feature_names=feature_names,\n    metadata={\n        \"trustworthiness_characteristics\": [\n            \"valid\", \"reliable\", \"safe\", \"secure\",\n            \"accountable\", \"transparent\", \"explainable\",\n            \"privacy_enhanced\", \"fair\"\n        ]\n    }\n)\n```

## Visualization

### SHAP waterfall plot

```python\nimport shap\nimport matplotlib.pyplot as plt\n\n# Generate waterfall plot\nexplanation.visualize(output_path=\"shap_waterfall.png\")\n\n# Or manually:\nshap.waterfall_plot(\n    shap.Explanation(\n        values=explanation.shap_values,\n        base_values=explanation.base_value,\n        data=explanation.feature_values,\n        feature_names=explanation.feature_names\n    )\n)\nplt.savefig(\"shap_waterfall.png\")\n```

### SHAP force plot

```python\n# Generate force plot\nshap.force_plot(\n    base_value=explanation.base_value,\n    shap_values=explanation.shap_values,\n    features=explanation.feature_values,\n    feature_names=explanation.feature_names,\n    matplotlib=True\n)\nplt.savefig(\"shap_force.png\")\n```

## Caching and audit

```python\n# Explanations are automatically cached for audit\nexplanation_id = explanation.explanation_id\n\n# Retrieve cached explanation\ncached = explainer.get_explanation(explanation_id)\n\n# Generate audit report\naudit_report = cached.to_dict()\nprint(f\"Audit report: {audit_report}\")\n```

## Performance

- **Explanation generation**: <5 seconds\n- **SHAP calculation**: ~2 seconds for 10 features\n- **Caching**: Redis-backed for <100ms retrieval\n- **Audit logging**: Async to Bigtable\n\n## Error handling

```python\ntry:\n    explanation = explainer.explain_prediction(\n        model_id=\"model-v1\",\n        input_data=input_data,\n        prediction=prediction,\n        feature_names=feature_names\n    )\nexcept ModelNotFoundError:\n    print(\"Model not found in Vertex AI\")\nexcept ExplanationFailedError as e:\n    print(f\"SHAP calculation failed: {e}\")\nexcept ComplianceViolationError as e:\n    print(f\"Explanation does not meet compliance requirements: {e}\")\n```

## Next steps

<CardGroup cols={2}>\n  <Card\n    title=\"Governance kernel\"\n    icon=\"shield-check\"\n    href=\"/governance/overview\"\n  >\n    Integrate with SovereignGuardrail\n  </Card>\n  <Card\n    title=\"AI agents\"\n    icon=\"brain-circuit\"\n    href=\"/ai-agents/overview\"\n  >\n    Deploy explainable AI agents\n  </Card>\n  <Card\n    title=\"Humanitarian constraints\"\n    icon=\"heart\"\n    href=\"/governance/humanitarian\"\n  >\n    Apply ethical constraints\n  </Card>\n  <Card\n    title=\"Deployment\"\n    icon=\"rocket\"\n    href=\"/deployment/gcp\"\n  >\n    Deploy to Vertex AI\n  </Card>\n</CardGroup>\n