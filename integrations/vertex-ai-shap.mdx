---
title: Vertex AI + SHAP integration
description: Right to Explanation for high-risk clinical AI with SHAP explainability
---

## Overview

iLuminara integrates **Vertex AI** for model training and inference with **SHAP (SHapley Additive exPlanations)** for explainability, ensuring compliance with the **EU AI Act §6** and **GDPR Art. 22** (Right to Explanation).

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk clinical inference automatically triggers SHAP analysis
</Card>

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    VERTEX AI MODEL                          │
│  - AutoML Time-Series Forecasting                          │
│  - Custom Training (TensorFlow, PyTorch)                   │
│  - Batch/Online Prediction                                 │
└─────────────────────────────────────────────────────────────┘
                            │
                            │ Prediction
                            ▼
┌─────────────────────────────────────────────────────────────┐
│                 SHAP EXPLAINER                              │
│  - TreeExplainer (XGBoost, LightGBM, CatBoost)            │
│  - DeepExplainer (Neural Networks)                         │
│  - KernelExplainer (Model-agnostic)                        │
└─────────────────────────────────────────────────────────────┘
                            │
                            │ Explanation
                            ▼
┌─────────────────────────────────────────────────────────────┐
│              SOVEREIGN GUARDRAIL                            │
│  - Validates explanation completeness                       │
│  - Enforces EU AI Act §6 requirements                      │
│  - Logs to tamper-proof audit trail                        │
└─────────────────────────────────────────────────────────────┘
```

## High-risk AI classification

According to **EU AI Act §6**, iLuminara classifies the following as high-risk:

| Use Case | Risk Level | Explanation Required |
|----------|------------|----------------------|
| **Cholera outbreak prediction** | High | ✅ Yes |
| **Patient triage (Emergency/Priority)** | High | ✅ Yes |
| **Resource allocation (ventilators, beds)** | High | ✅ Yes |
| **Epidemic forecasting (R0 > 2.0)** | High | ✅ Yes |
| **Symptom severity scoring (>7/10)** | High | ✅ Yes |
| **Routine data aggregation** | Low | ❌ No |
| **Dashboard visualization** | Minimal | ❌ No |

## Vertex AI setup

### 1. Enable Vertex AI API

```bash
gcloud services enable aiplatform.googleapis.com
```

### 2. Create training dataset

```python
from google.cloud import aiplatform

aiplatform.init(project="iluminara-core", location="africa-south1")

# Create dataset for outbreak forecasting
dataset = aiplatform.TimeSeriesDataset.create(\n    display_name="cholera_outbreak_forecast",\n    gcs_source="gs://iluminara-data/training/cholera_cases.csv",\n    bq_source="bq://iluminara-core.health_data.outbreak_cases"\n)
```

### 3. Train AutoML model

```python
# Train time-series forecasting model
job = aiplatform.AutoMLForecastingTrainingJob(\n    display_name="cholera_forecast_model",\n    optimization_objective="minimize-rmse",\n    column_specs={\n        "date": "timestamp",\n        "cases": "numeric",\n        "location": "categorical",\n        "temperature": "numeric",\n        "rainfall": "numeric"\n    }\n)

model = job.run(\n    dataset=dataset,\n    target_column="cases",\n    time_column="date",\n    time_series_identifier_column="location",\n    forecast_horizon=72,  # 72-hour forecast\n    data_granularity_unit="hour",\n    data_granularity_count=1,\n    budget_milli_node_hours=1000\n)
```

### 4. Deploy model

```python
# Deploy to endpoint
endpoint = model.deploy(\n    machine_type="n1-standard-4",\n    min_replica_count=1,\n    max_replica_count=10,\n    accelerator_type="NVIDIA_TESLA_T4",\n    accelerator_count=1\n)
```

## SHAP integration

### 1. Install SHAP

```bash
pip install shap
```

### 2. Create SHAP explainer

```python
import shap
import numpy as np
from google.cloud import aiplatform

# Load model from Vertex AI
model = aiplatform.Model("projects/123/locations/africa-south1/models/456")

# Create prediction function
def predict_fn(X):
    instances = [{"features": row.tolist()} for row in X]
    predictions = endpoint.predict(instances=instances)
    return np.array([p["value"] for p in predictions.predictions])

# Create SHAP explainer
explainer = shap.KernelExplainer(predict_fn, background_data)
```

### 3. Generate explanations

```python
# Get SHAP values for prediction
shap_values = explainer.shap_values(input_data)

# Visualize
shap.force_plot(\n    explainer.expected_value,\n    shap_values[0],\n    input_data.iloc[0],\n    matplotlib=True\n)
```

## Governance integration

### Validate high-risk inference

```python
from governance_kernel.vector_ledger import SovereignGuardrail
import shap

guardrail = SovereignGuardrail(enable_tamper_proof_audit=True)

# Make prediction
prediction = endpoint.predict(instances=[input_data])
confidence_score = prediction.predictions[0]["confidence"]

# Generate SHAP explanation
shap_values = explainer.shap_values(input_data)
feature_importance = dict(zip(feature_names, shap_values[0]))

# Validate with SovereignGuardrail
try:
    guardrail.validate_action(\n        action_type='High_Risk_Inference',\n        payload={\n            'model_id': model.resource_name,\n            'prediction': prediction.predictions[0]["value"],\n            'confidence_score': confidence_score,\n            'explanation': {\n                'method': 'SHAP',\n                'base_value': explainer.expected_value,\n                'shap_values': shap_values[0].tolist(),\n                'feature_importance': feature_importance\n            },\n            'evidence_chain': [\n                f\"Feature: {k}, Contribution: {v:.4f}\" \n                for k, v in sorted(feature_importance.items(), \n                                   key=lambda x: abs(x[1]), \n                                   reverse=True)[:5]\n            ],\n            'consent_token': 'EMERGENCY_OUTBREAK_RESPONSE',\n            'consent_scope': 'public_health_surveillance'\n        },\n        jurisdiction='EU_AI_ACT'\n    )
    \n    print(\"✅ High-risk inference validated with explanation\")\n    \nexcept SovereigntyViolationError as e:\n    print(f\"❌ Validation failed: {e}\")\n    # Block inference - explanation insufficient\n```

## Example: Cholera outbreak prediction

```python
import pandas as pd
import shap
from google.cloud import aiplatform

# Initialize
aiplatform.init(project="iluminara-core", location="africa-south1")
endpoint = aiplatform.Endpoint("projects/123/locations/africa-south1/endpoints/789")

# Input data
input_data = pd.DataFrame({\n    'location': ['Dadaab'],\n    'current_cases': [45],\n    'temperature': [32.5],\n    'rainfall': [12.3],\n    'population_density': [850],\n    'water_quality_index': [0.65],\n    'sanitation_coverage': [0.42]\n})

# Predict
prediction = endpoint.predict(instances=[input_data.to_dict('records')[0]])
forecast_cases = prediction.predictions[0]['value']
confidence = prediction.predictions[0]['confidence']

print(f\"Forecast: {forecast_cases} cases in 72 hours (confidence: {confidence:.2%})\")\n\n# Generate SHAP explanation
explainer = shap.KernelExplainer(predict_fn, background_data)
shap_values = explainer.shap_values(input_data)

# Top contributing features
feature_importance = dict(zip(input_data.columns, shap_values[0]))
top_features = sorted(feature_importance.items(), key=lambda x: abs(x[1]), reverse=True)[:3]

print(\"\\nTop contributing factors:\")\nfor feature, contribution in top_features:
    print(f\"  - {feature}: {contribution:+.4f}\")\n\n# Validate with governance
guardrail = SovereignGuardrail()\nresult = guardrail.validate_action(\n    action_type='High_Risk_Inference',\n    payload={\n        'model_id': 'cholera_forecast_v1',\n        'prediction': forecast_cases,\n        'confidence_score': confidence,\n        'explanation': {\n            'method': 'SHAP',\n            'shap_values': shap_values[0].tolist(),\n            'feature_importance': feature_importance\n        },\n        'evidence_chain': [f\"{k}: {v:+.4f}\" for k, v in top_features]\n    },\n    jurisdiction='EU_AI_ACT'\n)

print(f\"\\n✅ Governance validation: {result['status']}\")\n```

## Explanation requirements

Per **EU AI Act §13**, high-risk AI systems must provide:

<Steps>
  <Step title=\"Confidence score\">
    Numerical confidence/probability of the prediction
  </Step>
  <Step title=\"Evidence chain\">
    Top contributing features with their impact values
  </Step>
  <Step title=\"Feature contributions\">
    SHAP values showing how each feature influenced the prediction
  </Step>
  <Step title=\"Decision rationale\">
    Human-readable explanation of why the model made this prediction
  </Step>
  <Step title=\"Alternative outcomes\">
    What would change the prediction (counterfactual explanations)
  </Step>
</Steps>

## SHAP visualization

### Force plot

```python
import shap

# Single prediction explanation
shap.force_plot(\n    explainer.expected_value,\n    shap_values[0],\n    input_data.iloc[0],\n    matplotlib=True\n)
```

### Waterfall plot

```python
# Waterfall showing feature contributions
shap.waterfall_plot(\n    shap.Explanation(\n        values=shap_values[0],\n        base_values=explainer.expected_value,\n        data=input_data.iloc[0],\n        feature_names=input_data.columns.tolist()\n    )\n)
```

### Summary plot

```python
# Global feature importance
shap.summary_plot(shap_values, input_data)
```

## Compliance checklist

<AccordionGroup>
  <Accordion title=\"EU AI Act §6 (High-Risk Classification)\">
    ✅ Cholera outbreak prediction classified as high-risk  
    ✅ Patient triage classified as high-risk  
    ✅ Resource allocation classified as high-risk
  </Accordion>
  <Accordion title=\"EU AI Act §12 (Record-keeping)\">
    ✅ All predictions logged to tamper-proof audit trail  
    ✅ SHAP explanations stored with predictions  
    ✅ Audit retention: 7 years (HIPAA requirement)
  </Accordion>
  <Accordion title=\"EU AI Act §13 (Transparency)\">
    ✅ Confidence scores provided  
    ✅ Feature importance disclosed  
    ✅ Evidence chain documented
  </Accordion>
  <Accordion title=\"EU AI Act §14 (Human Oversight)\">
    ✅ Human review option available  
    ✅ Override capability implemented  
    ✅ Escalation to clinical staff
  </Accordion>
  <Accordion title=\"GDPR Art. 22 (Automated Decision-Making)\">
    ✅ Right to explanation enforced  
    ✅ Right to human intervention  
    ✅ Right to contest decision
  </Accordion>
</AccordionGroup>

## Performance considerations

- **SHAP computation time**: ~100-500ms per prediction (KernelExplainer)
- **TreeExplainer**: ~10-50ms (for tree-based models)
- **DeepExplainer**: ~50-200ms (for neural networks)
- **Caching**: Background data cached for 1 hour
- **Batch processing**: Recommended for >100 predictions

## Next steps

<CardGroup cols={2}>
  <Card
    title=\"Governance kernel\"
    icon=\"shield-check\"
    href=\"/governance/overview\"
  >
    Understand compliance enforcement
  </Card>
  <Card
    title=\"AI agents\"
    icon=\"brain-circuit\"
    href=\"/ai-agents/overview\"
  >
    Deploy autonomous surveillance
  </Card>
  <Card
    title=\"API reference\"
    icon=\"terminal\"
    href=\"/api-reference/overview\"
  >
    Integrate with REST API
  </Card>
  <Card
    title=\"Deployment\"
    icon=\"rocket\"
    href=\"/deployment/overview\"
  >
    Deploy to production
  </Card>
</CardGroup>
