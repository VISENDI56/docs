---
title: Vertex AI + SHAP explainability
description: Right to Explanation for high-risk clinical AI with SHAP values
---

## Overview

iLuminara-Core integrates **Vertex AI** for model training and **SHAP (SHapley Additive exPlanations)** for explainability, ensuring compliance with the **EU AI Act ¬ß6** and **GDPR Art. 22** (Right to Explanation).

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk clinical inference requires explainability with SHAP values, feature importance, and evidence chains.
</Card>

## Why explainability matters

<Steps>
  <Step title="Legal requirement">
    EU AI Act ¬ß6 mandates explainability for high-risk AI systems (medical diagnosis, treatment recommendations)
  </Step>
  <Step title="Clinical trust">
    Healthcare workers need to understand AI recommendations to make informed decisions
  </Step>
  <Step title="Audit trail">
    Explainability provides forensic evidence for regulatory audits and malpractice investigations
  </Step>
  <Step title="Bias detection">
    SHAP values reveal when models rely on protected attributes (race, gender, age)
  </Step>
</Steps>

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    VERTEX AI                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  AutoML Time-Series Forecasting                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - 72-hour outbreak prediction                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Hierarchical spatial forecasting              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Custom Training                                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Disease classification                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Risk stratification                           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SHAP EXPLAINER                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  TreeExplainer (for tree-based models)           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  KernelExplainer (for neural networks)           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  DeepExplainer (for deep learning)               ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              SOVEREIGNGUARDRAIL VALIDATION              ‚îÇ
‚îÇ  - Confidence score ‚â• 0.7 requires explanation          ‚îÇ
‚îÇ  - Evidence chain must be complete                      ‚îÇ
‚îÇ  - Feature contributions must be provided               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Training models on Vertex AI

### Setup

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project="your-project-id",
    location="africa-south1",  # Kenya/South Africa region
    staging_bucket="gs://iluminara-models"
)
```

### Train AutoML time-series model

```python
from google.cloud import aiplatform

# Create dataset
dataset = aiplatform.TimeSeriesDataset.create(
    display_name="cholera_outbreak_forecast",
    gcs_source="gs://iluminara-data/cholera_cases.csv",
    time_column="timestamp",
    time_series_identifier_column="location",
    target_column="case_count"
)

# Train model
job = aiplatform.AutoMLForecastingTrainingJob(
    display_name="cholera_72h_forecast",
    optimization_objective="minimize-rmse",
    column_transformations=[
        {"numeric": {"column_name": "temperature"}},
        {"numeric": {"column_name": "rainfall"}},
        {"numeric": {"column_name": "population_density"}},
    ],
)

model = job.run(
    dataset=dataset,
    target_column="case_count",
    time_column="timestamp",
    time_series_identifier_column="location",
    forecast_horizon=72,  # 72 hours
    data_granularity_unit="hour",
    data_granularity_count=1,
    budget_milli_node_hours=1000,
)

print(f"‚úÖ Model trained: {model.resource_name}")
```

### Train custom classification model

```python
from google.cloud import aiplatform

# Create dataset
dataset = aiplatform.TabularDataset.create(
    display_name="disease_classification",
    gcs_source="gs://iluminara-data/symptoms_diagnosis.csv"
)

# Train model
job = aiplatform.AutoMLTabularTrainingJob(
    display_name="disease_classifier",
    optimization_prediction_type="classification",
    optimization_objective="maximize-au-prc",
)

model = job.run(
    dataset=dataset,
    target_column="diagnosis",
    budget_milli_node_hours=1000,
    model_display_name="disease_classifier_v1",
)

print(f"‚úÖ Model trained: {model.resource_name}")
```

## SHAP explainability

### Install SHAP

```bash
pip install shap
```

### Explain predictions

```python
import shap
import numpy as np
from google.cloud import aiplatform

# Load model
model = aiplatform.Model("projects/123/locations/africa-south1/models/456")

# Get predictions
endpoint = model.deploy(
    machine_type="n1-standard-4",
    min_replica_count=1,
    max_replica_count=3,
)

# Sample data
X_test = np.array([
    [38.5, 95, 1, 1, 0],  # fever, heart_rate, diarrhea, vomiting, cough
    [37.2, 80, 0, 0, 1],
    [39.1, 105, 1, 1, 1],
])

# Get predictions
predictions = endpoint.predict(instances=X_test.tolist())

# Create SHAP explainer
explainer = shap.KernelExplainer(
    model=lambda x: endpoint.predict(instances=x.tolist()).predictions,
    data=shap.sample(X_test, 100)
)

# Calculate SHAP values
shap_values = explainer.shap_values(X_test)

# Visualize
shap.summary_plot(shap_values, X_test, feature_names=[
    "temperature", "heart_rate", "diarrhea", "vomiting", "cough"
])
```

### Feature importance

```python
import shap
import matplotlib.pyplot as plt

# Feature importance
feature_importance = np.abs(shap_values).mean(axis=0)

# Sort by importance
feature_names = ["temperature", "heart_rate", "diarrhea", "vomiting", "cough"]
sorted_idx = np.argsort(feature_importance)[::-1]

print("Feature Importance:")
for idx in sorted_idx:
    print(f"  {feature_names[idx]}: {feature_importance[idx]:.4f}")

# Visualize
plt.barh(range(len(feature_importance)), feature_importance[sorted_idx])
plt.yticks(range(len(feature_importance)), [feature_names[i] for i in sorted_idx])
plt.xlabel("Mean |SHAP value|")
plt.title("Feature Importance")
plt.show()
```

## Integration with SovereignGuardrail

Every high-risk inference must pass through the SovereignGuardrail:

```python
from governance_kernel.vector_ledger import SovereignGuardrail, SovereigntyViolationError
import shap
import numpy as np

guardrail = SovereignGuardrail()

# Make prediction
prediction = model.predict(patient_data)
confidence_score = prediction["confidence"]

# Calculate SHAP values
explainer = shap.KernelExplainer(model.predict, background_data)
shap_values = explainer.shap_values(patient_data)

# Build evidence chain
evidence_chain = [
    f"fever: {patient_data['temperature']}¬∞C",
    f"heart_rate: {patient_data['heart_rate']} bpm",
    f"symptoms: {', '.join(patient_data['symptoms'])}"
]

# Validate with SovereignGuardrail
try:
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': prediction['diagnosis'],
            'explanation': shap_values.tolist(),
            'confidence_score': confidence_score,
            'evidence_chain': evidence_chain,
            'feature_importance': dict(zip(feature_names, np.abs(shap_values).mean(axis=0))),
            'consent_token': patient_data['consent_token'],
            'consent_scope': 'diagnosis'
        },
        jurisdiction='EU_AI_ACT'
    )
    
    print(f"‚úÖ Inference approved: {prediction['diagnosis']}")
    print(f"   Confidence: {confidence_score:.2%}")
    print(f"   Top features: {sorted_idx[:3]}")
    
except SovereigntyViolationError as e:
    print(f"‚ùå Inference blocked: {e}")
```

## Explainability dashboard

Create a Streamlit dashboard for explainability:

```python
import streamlit as st
import shap
import matplotlib.pyplot as plt

st.title("üîç iLuminara Explainability Dashboard")

# Load model and data
model = load_model()
patient_data = st.file_uploader("Upload patient data", type="csv")

if patient_data:
    # Make prediction
    prediction = model.predict(patient_data)
    
    # Calculate SHAP values
    explainer = shap.KernelExplainer(model.predict, background_data)
    shap_values = explainer.shap_values(patient_data)
    
    # Display prediction
    st.metric("Diagnosis", prediction['diagnosis'])
    st.metric("Confidence", f"{prediction['confidence']:.1%}")
    
    # SHAP waterfall plot
    st.subheader("Feature Contributions")
    fig, ax = plt.subplots()
    shap.waterfall_plot(shap.Explanation(
        values=shap_values[0],
        base_values=explainer.expected_value,
        data=patient_data.iloc[0],
        feature_names=feature_names
    ))
    st.pyplot(fig)
    
    # Feature importance
    st.subheader("Feature Importance")
    feature_importance = np.abs(shap_values).mean(axis=0)
    st.bar_chart(dict(zip(feature_names, feature_importance)))
    
    # Evidence chain
    st.subheader("Evidence Chain")
    for evidence in prediction['evidence_chain']:
        st.write(f"- {evidence}")
    
    # Compliance status
    st.subheader("Compliance Status")
    if prediction['confidence'] >= 0.7:
        st.success("‚úÖ Explainability required (EU AI Act ¬ß6)")
        st.info(f"SHAP values provided: {len(shap_values[0])} features")
    else:
        st.info("‚ÑπÔ∏è Low-risk inference (no explanation required)")
```

## Compliance requirements

### EU AI Act ¬ß6 (High-Risk AI)

<AccordionGroup>
  <Accordion title="Article 6: Classification rules">
    AI systems used for medical diagnosis, treatment, or risk assessment are classified as high-risk and require:
    - Explainability of decisions
    - Human oversight
    - Accuracy and robustness testing
    - Logging of operations
  </Accordion>
  <Accordion title="Article 8: Transparency">
    High-risk AI systems must provide:
    - Clear information about capabilities and limitations
    - Explanation of decision-making logic
    - Instructions for human oversight
  </Accordion>
  <Accordion title="Article 12: Record keeping">
    Automatic logging of:
    - Input data
    - Output decisions
    - Explanation values (SHAP)
    - Confidence scores
  </Accordion>
</AccordionGroup>

### GDPR Art. 22 (Right to Explanation)

Individuals have the right to:
- Not be subject to automated decision-making
- Obtain human intervention
- Express their point of view
- Contest the decision
- **Obtain an explanation** of the decision

## Best practices

<CardGroup cols={2}>
  <Card title="Use appropriate explainers" icon="brain">
    TreeExplainer for tree-based models, KernelExplainer for black-box models
  </Card>
  <Card title="Validate explanations" icon="check">
    Ensure SHAP values sum to prediction - base_value
  </Card>
  <Card title="Store explanations" icon="database">
    Log SHAP values to audit trail for regulatory compliance
  </Card>
  <Card title="Human oversight" icon="user">
    Always require human review for high-risk decisions
  </Card>
</CardGroup>

## Example: Cholera outbreak prediction

```python
import shap
import numpy as np
from google.cloud import aiplatform

# Load trained model
model = aiplatform.Model("projects/iluminara/locations/africa-south1/models/cholera_forecast")
endpoint = model.deploy(machine_type="n1-standard-4")

# Input features
features = {
    "temperature": 32.5,      # ¬∞C
    "rainfall": 45.2,         # mm
    "population_density": 850, # per km¬≤
    "water_quality": 0.65,    # 0-1 score
    "sanitation_coverage": 0.42, # 0-1 score
    "previous_cases": 12      # last 7 days
}

# Predict
prediction = endpoint.predict(instances=[list(features.values())])
forecast = prediction.predictions[0]

print(f"üìä 72-hour forecast: {forecast['case_count']:.0f} cases")
print(f"   Confidence interval: [{forecast['lower_bound']:.0f}, {forecast['upper_bound']:.0f}]")

# Explain with SHAP
explainer = shap.KernelExplainer(
    model=lambda x: endpoint.predict(instances=x.tolist()).predictions,
    data=background_data
)

shap_values = explainer.shap_values(np.array([list(features.values())]))

# Feature contributions
print("\nüîç Feature Contributions:")
for feature, value in sorted(
    zip(features.keys(), shap_values[0]),
    key=lambda x: abs(x[1]),
    reverse=True
):
    direction = "‚Üë" if value > 0 else "‚Üì"
    print(f"   {direction} {feature}: {value:+.3f}")

# Validate with SovereignGuardrail
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'cholera_outbreak_forecast',
        'explanation': shap_values.tolist(),
        'confidence_score': forecast['confidence'],
        'evidence_chain': [
            f"High temperature: {features['temperature']}¬∞C",
            f"Recent rainfall: {features['rainfall']}mm",
            f"Low sanitation: {features['sanitation_coverage']:.0%}"
        ],
        'feature_importance': dict(zip(features.keys(), np.abs(shap_values[0]))),
    },
    jurisdiction='EU_AI_ACT'
)
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="Deploy to Vertex AI"
    icon="google"
    href="/deployment/gcp"
  >
    Deploy models to production
  </Card>
  <Card
    title="SovereignGuardrail"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Integrate with autonomous agents
  </Card>
  <Card
    title="Audit trail"
    icon="file-contract"
    href="/governance/audit"
  >
    Log explanations for compliance
  </Card>
</CardGroup>
