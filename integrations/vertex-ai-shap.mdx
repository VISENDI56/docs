---
title: Vertex AI + SHAP integration
description: Right to Explanation with explainable AI for high-risk clinical inferences
---

## Overview

iLuminara-Core integrates Google Cloud Vertex AI with SHAP (SHapley Additive exPlanations) to provide the **Right to Explanation** for every high-risk clinical inference, as required by EU AI Act §6 and GDPR Art. 22.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk clinical inference requires explainability (SHAP values, feature importance)
</Card>

## Architecture

```
┌─────────────────────────────────────────────────────────┐
│              VERTEX AI MODEL                            │
│  ┌──────────────────────────────────────────────────┐  │
│  │  AutoML Time-Series Forecasting                  │  │
│  │  - 72-hour outbreak prediction                   │  │
│  │  - Hierarchical spatial forecasting              │  │
│  │  - Multi-disease modeling                        │  │
│  └──────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────┘
                         │
                         │ Prediction
                         ▼
┌─────────────────────────────────────────────────────────┐
│              SHAP EXPLAINER                             │
│  ┌──────────────────────────────────────────────────┐  │
│  │  TreeExplainer / KernelExplainer                 │  │
│  │  - Feature importance                            │  │
│  │  - SHAP values per prediction                    │  │
│  │  - Waterfall plots                               │  │
│  └──────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────┘
                         │
                         │ Explanation
                         ▼
┌─────────────────────────────────────────────────────────┐
│              SOVEREIGN GUARDRAIL                        │
│  ┌──────────────────────────────────────────────────┐  │
│  │  Validates High-Risk Inference                   │  │
│  │  - Confidence score check                        │  │
│  │  - Explanation completeness                      │  │
│  │  - Evidence chain validation                     │  │
│  └──────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────┘
```

## High-risk inference validation

The SovereignGuardrail enforces explainability for all high-risk clinical inferences:

```python
from governance_kernel.vector_ledger import SovereignGuardrail, SovereigntyViolationError
import shap
import numpy as np

guardrail = SovereignGuardrail()

# Make prediction with Vertex AI model
prediction = vertex_ai_model.predict(patient_features)
confidence_score = prediction['confidence']

# Generate SHAP explanation
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(patient_features)

# Validate with SovereignGuardrail
try:
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'cholera_diagnosis',
            'confidence_score': confidence_score,
            'explanation': {
                'shap_values': shap_values.tolist(),
                'feature_names': feature_names,
                'base_value': explainer.expected_value
            },
            'evidence_chain': [
                'symptom: diarrhea (SHAP: +0.45)',
                'symptom: vomiting (SHAP: +0.32)',
                'location: Dadaab (SHAP: +0.18)',
                'recent_outbreak: True (SHAP: +0.25)'
            ],
            'consent_token': 'VALID_CONSENT_TOKEN',
            'consent_scope': 'diagnosis'
        },
        jurisdiction='EU_AI_ACT'
    )
    
    print("✅ High-risk inference approved with explanation")
    
except SovereigntyViolationError as e:
    print(f"❌ Inference blocked: {e}")
```

## Vertex AI model training

### 1. Prepare training data

```python
from google.cloud import bigquery
import pandas as pd

# Query historical outbreak data
client = bigquery.Client()

query = """
SELECT
    date,
    location,
    disease,
    case_count,
    temperature,
    rainfall,
    population_density,
    sanitation_score
FROM `iluminara.outbreak_data.historical_cases`
WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR)
ORDER BY date
"""

df = client.query(query).to_dataframe()

# Feature engineering
df['day_of_week'] = pd.to_datetime(df['date']).dt.dayofweek
df['month'] = pd.to_datetime(df['date']).dt.month
df['lag_7d'] = df.groupby('location')['case_count'].shift(7)
df['lag_14d'] = df.groupby('location')['case_count'].shift(14)

# Save to BigQuery for Vertex AI
df.to_gbq(
    'iluminara.outbreak_data.training_features',
    project_id='iluminara-core',
    if_exists='replace'
)
```

### 2. Train Vertex AI model

```python
from google.cloud import aiplatform

aiplatform.init(project='iluminara-core', location='us-central1')

# Create AutoML time-series dataset
dataset = aiplatform.TimeSeriesDataset.create(
    display_name='outbreak_forecasting_dataset',
    bq_source='bq://iluminara-core.outbreak_data.training_features',
    time_column='date',
    time_series_identifier_column='location',
    target_column='case_count'
)

# Train AutoML model
job = aiplatform.AutoMLForecastingTrainingJob(
    display_name='outbreak_forecasting_model',
    optimization_objective='minimize-rmse',
    column_specs={
        'temperature': 'numeric',
        'rainfall': 'numeric',
        'population_density': 'numeric',
        'sanitation_score': 'numeric',
        'day_of_week': 'categorical',
        'month': 'categorical'
    }
)

model = job.run(
    dataset=dataset,
    target_column='case_count',
    time_column='date',
    time_series_identifier_column='location',
    forecast_horizon=72,  # 72-hour forecast
    data_granularity_unit='hour',
    data_granularity_count=1,
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1
)

print(f"✅ Model trained: {model.resource_name}")
```

### 3. Deploy model endpoint

```python
# Deploy to endpoint
endpoint = model.deploy(
    machine_type='n1-standard-4',
    min_replica_count=1,
    max_replica_count=10,
    accelerator_type='NVIDIA_TESLA_T4',
    accelerator_count=1
)

print(f"✅ Model deployed: {endpoint.resource_name}")
```

## SHAP explanation generation

### TreeExplainer (for tree-based models)

```python
import shap
import numpy as np

# Load model from Vertex AI
model = aiplatform.Model('projects/123/locations/us-central1/models/456')

# Create SHAP explainer
explainer = shap.TreeExplainer(model)

# Generate explanations for prediction
patient_features = np.array([[
    38.5,  # temperature
    120,   # rainfall
    5000,  # population_density
    0.6,   # sanitation_score
    3,     # day_of_week
    6      # month
]])

shap_values = explainer.shap_values(patient_features)

# Visualize
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values[0],
        base_values=explainer.expected_value,
        data=patient_features[0],
        feature_names=['temperature', 'rainfall', 'population_density', 
                      'sanitation_score', 'day_of_week', 'month']
    )
)
```

### KernelExplainer (model-agnostic)

```python
import shap

# Create model-agnostic explainer
def model_predict(data):
    instances = [{'features': row.tolist()} for row in data]
    predictions = endpoint.predict(instances=instances)
    return np.array([p['value'] for p in predictions.predictions])

# Use a background dataset
background = shap.sample(training_data, 100)

explainer = shap.KernelExplainer(model_predict, background)

# Generate explanations
shap_values = explainer.shap_values(patient_features)

# Force plot
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    patient_features[0],
    feature_names=feature_names
)
```

## Integration with API

The API automatically generates SHAP explanations for high-risk predictions:

```python
from flask import Flask, request, jsonify
from google.cloud import aiplatform
import shap
import numpy as np

app = Flask(__name__)

# Load model and explainer
endpoint = aiplatform.Endpoint('projects/123/locations/us-central1/endpoints/789')
explainer = shap.TreeExplainer(model)

@app.route('/predict-outbreak', methods=['POST'])
def predict_outbreak():
    data = request.json
    
    # Extract features
    features = np.array([[
        data['temperature'],
        data['rainfall'],
        data['population_density'],
        data['sanitation_score'],
        data['day_of_week'],
        data['month']
    ]])
    
    # Make prediction
    prediction = endpoint.predict(instances=[{'features': features[0].tolist()}])
    confidence_score = prediction.predictions[0]['confidence']
    
    # Generate SHAP explanation
    shap_values = explainer.shap_values(features)
    
    # Build evidence chain
    evidence_chain = []
    for i, (name, value, shap_val) in enumerate(zip(
        feature_names, features[0], shap_values[0]
    )):
        evidence_chain.append(f"{name}: {value:.2f} (SHAP: {shap_val:+.2f})")
    
    # Validate with SovereignGuardrail
    guardrail = SovereignGuardrail()
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'outbreak_prediction',
            'confidence_score': confidence_score,
            'explanation': {
                'shap_values': shap_values[0].tolist(),
                'feature_names': feature_names,
                'base_value': explainer.expected_value
            },
            'evidence_chain': evidence_chain,
            'consent_token': data.get('consent_token'),
            'consent_scope': 'public_health_surveillance'
        },
        jurisdiction='EU_AI_ACT'
    )
    
    return jsonify({
        'prediction': prediction.predictions[0]['value'],
        'confidence': confidence_score,
        'explanation': {
            'shap_values': shap_values[0].tolist(),
            'evidence_chain': evidence_chain,
            'base_value': float(explainer.expected_value)
        }
    })
```

## Compliance requirements

### EU AI Act §6 (High-Risk AI)

- ✅ Risk assessment conducted
- ✅ Explainability provided (SHAP)
- ✅ Human oversight enabled
- ✅ Accuracy metrics tracked
- ✅ Robustness testing performed

### GDPR Art. 22 (Right to Explanation)

- ✅ Automated decision-making disclosed
- ✅ Meaningful information provided
- ✅ Logic of processing explained
- ✅ Significance and consequences communicated

## Visualization examples

### Waterfall plot

Shows how each feature contributes to the prediction:

```python
shap.plots.waterfall(shap_values[0])
```

### Force plot

Interactive visualization of feature contributions:

```python
shap.plots.force(explainer.expected_value, shap_values[0], features[0])
```

### Summary plot

Global feature importance across all predictions:

```python
shap.summary_plot(shap_values, features, feature_names=feature_names)
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="Deploy to Vertex AI"
    icon="google"
    href="/deployment/gcp"
  >
    Deploy models to production
  </Card>
  <Card
    title="SovereignGuardrail"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="API integration"
    icon="plug"
    href="/api-reference/overview"
  >
    Integrate with REST API
  </Card>
  <Card
    title="Monitoring"
    icon="chart-line"
    href="/deployment/monitoring"
  >
    Monitor model performance
  </Card>
</CardGroup>
