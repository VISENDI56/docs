---
title: Vertex AI + SHAP explainability
description: Right to Explanation for high-risk AI systems (EU AI Act §6, GDPR Art. 22)
---

## Overview

iLuminara integrates Vertex AI with SHAP (SHapley Additive exPlanations) to provide transparent, explainable AI predictions that satisfy the EU AI Act requirement for "Right to Explanation."

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk clinical inference requires explainability (SHAP values, feature importance, evidence chain)
</Card>

## Why explainability matters

### Legal requirements

- **EU AI Act §6** - High-risk AI systems must provide explanations
- **GDPR Art. 22** - Right to explanation for automated decision-making
- **HIPAA** - Clinical decisions require documentation
- **WHO IHR** - Public health decisions must be transparent

### Use cases

<CardGroup cols={2}>
  <Card title="Outbreak prediction" icon="chart-line">
    Explain which factors (population density, humidity, etc.) drove the alert
  </Card>
  <Card title="Resource allocation" icon="hand-holding-medical">
    Show why specific regions were prioritized for vaccine distribution
  </Card>
  <Card title="Risk scoring" icon="gauge-high">
    Explain patient risk scores for cholera, malaria, etc.
  </Card>
  <Card title="Diagnosis support" icon="stethoscope">
    Provide evidence chain for AI-assisted diagnoses
  </Card>
</CardGroup>

## Architecture

```
┌─────────────────────────────────────────────────────────┐
│                  VERTEX AI MODEL                        │
│  - AutoML Time-Series Forecasting                      │
│  - Custom TensorFlow/PyTorch Models                    │
│  - Ensemble Predictions                                │
└─────────────────────────────────────────────────────────┘
                        │
                        │ Prediction
                        ▼
┌─────────────────────────────────────────────────────────┐
│                  SHAP EXPLAINER                         │
│  - TreeExplainer (for tree-based models)               │
│  - DeepExplainer (for neural networks)                 │
│  - KernelExplainer (model-agnostic)                    │
└─────────────────────────────────────────────────────────┘
                        │
                        │ SHAP Values
                        ▼
┌─────────────────────────────────────────────────────────┐
│              SOVEREIGNGUARDRAIL                         │
│  - Validates explainability completeness               │
│  - Enforces EU AI Act §6 requirements                  │
│  - Logs explanation in audit trail                     │
└─────────────────────────────────────────────────────────┘
```

## Implementation

### Step 1: Deploy model to Vertex AI

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project="iluminara-core",
    location="africa-south1"
)

# Deploy model
model = aiplatform.Model.upload(
    display_name="cholera-outbreak-forecaster",
    artifact_uri="gs://iluminara-models/cholera-v1",
    serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-8:latest"
)

endpoint = model.deploy(
    machine_type="n1-standard-4",
    min_replica_count=1,
    max_replica_count=10
)
```

### Step 2: Generate SHAP explanations

```python
import shap
import numpy as np
from google.cloud import aiplatform

class VertexAIExplainer:
    def __init__(self, endpoint_name: str):
        self.endpoint = aiplatform.Endpoint(endpoint_name)
    
    def explain_prediction(
        self,
        input_data: Dict,
        feature_names: List[str]
    ) -> Dict:
        """
        Generate SHAP explanation for Vertex AI prediction.
        
        Returns:
            {
                'prediction': float,
                'confidence': float,
                'shap_values': List[float],
                'feature_importance': Dict[str, float],
                'explanation_text': str,
                'compliance': {
                    'eu_ai_act': True,
                    'gdpr_art_22': True
                }
            }
        """
        # Get prediction from Vertex AI
        prediction = self.endpoint.predict(instances=[input_data])
        
        # Create SHAP explainer
        explainer = shap.KernelExplainer(
            model=lambda x: self.endpoint.predict(instances=x.tolist()).predictions,
            data=shap.sample(input_data, 100)
        )
        
        # Calculate SHAP values
        shap_values = explainer.shap_values(np.array([list(input_data.values())]))
        
        # Generate feature importance
        feature_importance = dict(zip(feature_names, shap_values[0]))
        
        # Sort by absolute importance
        sorted_features = sorted(
            feature_importance.items(),
            key=lambda x: abs(x[1]),
            reverse=True
        )
        
        # Generate human-readable explanation
        explanation_text = self._generate_explanation(sorted_features, prediction)
        
        return {
            'prediction': float(prediction.predictions[0]),
            'confidence': float(prediction.predictions[0]),
            'shap_values': shap_values[0].tolist(),
            'feature_importance': feature_importance,
            'top_features': sorted_features[:5],
            'explanation_text': explanation_text,
            'compliance': {
                'eu_ai_act_s6': True,
                'gdpr_art_22': True,
                'hipaa_documentation': True
            }
        }
    
    def _generate_explanation(
        self,
        sorted_features: List[Tuple[str, float]],
        prediction: object
    ) -> str:
        """Generate human-readable explanation"""
        top_features = sorted_features[:3]
        
        explanation = f"Prediction: {prediction.predictions[0]:.2f}\\n\\n"
        explanation += "Top contributing factors:\\n"
        
        for i, (feature, value) in enumerate(top_features, 1):
            direction = "increases" if value > 0 else "decreases"
            explanation += f"{i}. {feature}: {direction} risk by {abs(value):.2f}\\n"
        
        return explanation


# Example usage
explainer = VertexAIExplainer(
    endpoint_name="projects/123/locations/africa-south1/endpoints/456"
)

explanation = explainer.explain_prediction(
    input_data={
        'population_density': 5000,
        'humidity': 0.85,
        'temperature': 32,
        'water_quality': 0.3,
        'recent_cases': 45
    },
    feature_names=['population_density', 'humidity', 'temperature', 'water_quality', 'recent_cases']
)

print(explanation['explanation_text'])
```

### Step 3: Integrate with SovereignGuardrail

```python
from middleware.sovereign_guardrail_middleware import require_sovereignty_compliance, ComplianceFramework

@app.route('/api/predict', methods=['POST'])
@require_sovereignty_compliance([ComplianceFramework.EU_AI_ACT, ComplianceFramework.GDPR])
def predict_outbreak():
    """High-risk AI endpoint with explainability"""
    data = request.get_json()
    
    # Get prediction with explanation
    explainer = VertexAIExplainer(endpoint_name=VERTEX_ENDPOINT)
    result = explainer.explain_prediction(
        input_data=data['features'],
        feature_names=data['feature_names']
    )
    
    # SovereignGuardrail validates:
    # - Explanation is present
    # - Confidence score is provided
    # - Evidence chain is complete
    # - Complies with EU AI Act §6
    
    return jsonify({
        'status': 'success',
        'prediction': result['prediction'],
        'confidence': result['confidence'],
        'explanation': result['explanation_text'],
        'shap_values': result['shap_values'],
        'feature_importance': result['feature_importance'],
        'compliance': result['compliance']
    })
```

## SHAP visualization

### Feature importance plot

```python
import shap
import matplotlib.pyplot as plt

# Create SHAP summary plot
shap.summary_plot(
    shap_values,
    features=input_data,
    feature_names=feature_names,
    show=False
)

plt.title("Cholera Outbreak Risk - Feature Importance")
plt.tight_layout()
plt.savefig("shap_summary.png")
```

### Waterfall plot

```python
# Create waterfall plot for single prediction
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values[0],
        base_values=explainer.expected_value,
        data=input_data,
        feature_names=feature_names
    )
)
```

### Force plot

```python
# Create force plot
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    input_data,
    feature_names=feature_names,
    matplotlib=True
)
```

## Compliance validation

The SovereignGuardrail automatically validates explainability:

```python
# From sovereign_guardrail_middleware.py

def _validate_explainability(self, payload: Dict, request: Request) -> None:
    """
    Validate explainability requirements for high-risk AI.
    
    Frameworks: EU AI Act §6, GDPR Art. 22
    """
    if payload.get('action_type') == 'High_Risk_Inference':
        confidence_score = payload.get('confidence_score', 0.0)
        
        if confidence_score >= 0.7:  # High-risk threshold
            # Require explanation
            if not payload.get('explanation'):
                raise SovereigntyLockout(
                    violation_type=ViolationType.EXPLAINABILITY_REQUIRED,
                    frameworks=[ComplianceFramework.EU_AI_ACT, ComplianceFramework.GDPR],
                    details="High-risk AI inference requires explanation (SHAP, LIME, etc.)"
                )
            
            # Validate explanation completeness
            required_fields = ['confidence_score', 'evidence_chain', 'feature_contributions']
            missing_fields = [f for f in required_fields if f not in payload]
            
            if missing_fields:
                raise SovereigntyLockout(
                    violation_type=ViolationType.EXPLAINABILITY_REQUIRED,
                    frameworks=[ComplianceFramework.EU_AI_ACT],
                    details=f"Incomplete explanation. Missing: {missing_fields}"
                )
```

## Model types

### AutoML Time-Series

```python
from google.cloud import aiplatform

# Train AutoML model
dataset = aiplatform.TimeSeriesDataset.create(
    display_name="cholera-cases-dataset",
    gcs_source="gs://iluminara-data/cholera-cases.csv"
)

job = aiplatform.AutoMLForecastingTrainingJob(
    display_name="cholera-forecasting-job",
    optimization_objective="minimize-rmse"
)

model = job.run(
    dataset=dataset,
    target_column="cases",
    time_column="date",
    time_series_identifier_column="location",
    forecast_horizon=14,
    data_granularity_unit="day",
    data_granularity_count=1
)
```

### Custom TensorFlow model

```python
import tensorflow as tf

# Define custom model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Train model
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.fit(X_train, y_train, epochs=50, validation_split=0.2)

# Save for Vertex AI
model.save('gs://iluminara-models/custom-cholera-v1')
```

## Performance

- **Prediction latency**: <100ms (Vertex AI)
- **SHAP calculation**: ~500ms (depends on model complexity)
- **Total latency**: <1 second for explained predictions
- **Throughput**: 100+ predictions/second

## Next steps

<CardGroup cols={2}>
  <Card
    title="Deploy to Vertex AI"
    icon="google"
    href="/deployment/gcp"
  >
    Deploy models to production
  </Card>
  <Card
    title="SovereignGuardrail"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="API integration"
    icon="plug"
    href="/api-reference/overview"
  >
    Integrate with REST API
  </Card>
  <Card
    title="Monitoring"
    icon="chart-line"
    href="/deployment/monitoring"
  >
    Monitor model performance
  </Card>
</CardGroup>
