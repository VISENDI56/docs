---
title: Vertex AI + SHAP explainability
description: Right to Explanation for high-risk clinical AI with SHAP values and EU AI Act compliance
---

## Overview

iLuminara integrates **Google Cloud Vertex AI** with **SHAP (SHapley Additive exPlanations)** to provide explainability for every high-risk clinical inference, ensuring compliance with EU AI Act §6 and GDPR Art. 22.

<Card
  title="Philosophy"
  icon="lightbulb"
>
  "Every high-risk clinical inference requires explainability. No black boxes in healthcare."
</Card>

## The Right to Explanation

### Legal requirements

| Framework | Article | Requirement |
|-----------|---------|-------------|
| **EU AI Act** | §6 | High-risk AI systems must provide explanations |
| **GDPR** | Art. 22 | Right not to be subject to automated decision-making |
| **GDPR** | Art. 13-14 | Right to meaningful information about logic involved |
| **ISO 27001** | A.18.1.4 | Privacy and protection of personally identifiable information |

### What qualifies as "high-risk"?

<AccordionGroup>
  <Accordion title="Clinical diagnosis">
    AI-generated diagnoses with confidence >70% require SHAP explainability
  </Accordion>
  <Accordion title="Treatment recommendations">
    Automated treatment suggestions must show evidence chain
  </Accordion>
  <Accordion title="Outbreak predictions">
    Forecasts triggering emergency response need feature importance
  </Accordion>
  <Accordion title="Resource allocation">
    AI-driven triage decisions require transparent reasoning
  </Accordion>
</AccordionGroup>

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    VERTEX AI + SHAP                         │
│                                                             │
│  ┌──────────────┐         ┌──────────────┐                │
│  │  Input Data  │  ──────>│  Vertex AI   │                │
│  │  (Symptoms,  │         │  Model       │                │
│  │   Vitals)    │         │  (AutoML)    │                │
│  └──────────────┘         └──────┬───────┘                │
│                                   │                         │
│                                   │ Prediction              │
│                                   ▼                         │
│                          ┌─────────────────┐               │
│                          │  SHAP Explainer │               │
│                          │  (TreeExplainer)│               │
│                          └────────┬────────┘               │
│                                   │                         │
│                                   │ Feature Contributions   │
│                                   ▼                         │
│                          ┌─────────────────┐               │
│                          │ SovereignGuard  │               │
│                          │ Validation      │               │
│                          └────────┬────────┘               │
│                                   │                         │
│                                   ▼                         │
│                          ┌─────────────────┐               │
│                          │  Explainable    │               │
│                          │  Prediction     │               │
│                          │  + Evidence     │               │
│                          └─────────────────┘               │
└─────────────────────────────────────────────────────────────┘
```

## Setup

### Install dependencies

```bash
pip install google-cloud-aiplatform shap scikit-learn pandas numpy
```

### Configure Vertex AI

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project="your-project-id",
    location="africa-south1",  # Sovereignty-compliant region
    staging_bucket="gs://your-bucket"
)
```

### Train AutoML model

```python
from google.cloud import aiplatform

# Create dataset
dataset = aiplatform.TabularDataset.create(
    display_name="cholera_outbreak_dataset",
    gcs_source="gs://your-bucket/cholera_data.csv"
)

# Train AutoML model
job = aiplatform.AutoMLTabularTrainingJob(
    display_name="cholera_prediction_model",
    optimization_prediction_type="classification",
    optimization_objective="maximize-au-prc"
)

model = job.run(
    dataset=dataset,
    target_column="outbreak",
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
    budget_milli_node_hours=1000,
    model_display_name="cholera_predictor_v1"
)
```

## SHAP integration

### Basic explainability

```python
import shap
import pandas as pd
from google.cloud import aiplatform

# Load trained model
model = aiplatform.Model("projects/123/locations/africa-south1/models/456")

# Get predictions
endpoint = model.deploy(machine_type="n1-standard-4")

# Prepare data
X = pd.DataFrame({
    'fever': [38.5],
    'diarrhea': [1],
    'vomiting': [1],
    'dehydration': [0.7],
    'location_risk': [0.8]
})

# Get prediction
prediction = endpoint.predict(instances=X.values.tolist())

# Generate SHAP values
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

# Visualize
shap.summary_plot(shap_values, X)
```

### Integration with SovereignGuardrail

```python
from governance_kernel.vector_ledger import SovereignGuardrail
import shap

guardrail = SovereignGuardrail()

# Get prediction and SHAP values
prediction = model.predict(X)
shap_values = explainer.shap_values(X)

# Validate high-risk inference
try:
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'cholera_diagnosis',
            'confidence_score': prediction['confidence'],
            'explanation': {
                'shap_values': shap_values.tolist(),
                'feature_names': X.columns.tolist(),
                'base_value': explainer.expected_value
            },
            'evidence_chain': [
                f"fever: {X['fever'].values[0]}°C",
                f"diarrhea: {'yes' if X['diarrhea'].values[0] else 'no'}",
                f"location_risk: {X['location_risk'].values[0]:.2%}"
            ],
            'consent_token': 'VALID_TOKEN'
        },
        jurisdiction='EU_AI_ACT'
    )
    
    print("✅ High-risk inference validated with explainability")
    
except SovereigntyViolationError as e:
    print(f"❌ Validation failed: {e}")
```

## Explainability API

Create a REST API endpoint for explainable predictions:

```python
from flask import Flask, request, jsonify
from google.cloud import aiplatform
import shap
import pandas as pd

app = Flask(__name__)

# Load model and explainer
model = aiplatform.Model("projects/123/locations/africa-south1/models/456")
endpoint = model.deploy(machine_type="n1-standard-4")
explainer = shap.TreeExplainer(model)

@app.route('/predict-explainable', methods=['POST'])
def predict_explainable():
    """
    Explainable prediction endpoint
    
    Request:
    {
        "features": {
            "fever": 38.5,
            "diarrhea": 1,
            "vomiting": 1,
            "dehydration": 0.7,
            "location_risk": 0.8
        },
        "patient_id": "PAT_001",
        "consent_token": "VALID_TOKEN"
    }
    
    Response:
    {
        "prediction": "cholera",
        "confidence": 0.92,
        "explanation": {
            "shap_values": [...],
            "feature_contributions": {
                "fever": 0.15,
                "diarrhea": 0.35,
                "vomiting": 0.25,
                "dehydration": 0.10,
                "location_risk": 0.07
            },
            "evidence_chain": [...]
        },
        "compliance": {
            "framework": "EU_AI_ACT",
            "validated": true
        }
    }
    """
    data = request.json
    
    # Prepare features
    X = pd.DataFrame([data['features']])
    
    # Get prediction
    prediction = endpoint.predict(instances=X.values.tolist())
    
    # Generate SHAP values
    shap_values = explainer.shap_values(X)
    
    # Calculate feature contributions
    feature_contributions = dict(zip(
        X.columns,
        shap_values[0].tolist()
    ))
    
    # Build evidence chain
    evidence_chain = [
        f"{feature}: {value}" 
        for feature, value in data['features'].items()
        if abs(feature_contributions[feature]) > 0.05
    ]
    
    # Validate with SovereignGuardrail
    guardrail = SovereignGuardrail()
    
    try:
        guardrail.validate_action(
            action_type='High_Risk_Inference',
            payload={
                'inference': prediction['prediction'],
                'confidence_score': prediction['confidence'],
                'explanation': {
                    'shap_values': shap_values.tolist(),
                    'feature_contributions': feature_contributions
                },
                'evidence_chain': evidence_chain,
                'consent_token': data['consent_token']
            },
            jurisdiction='EU_AI_ACT'
        )
        
        validated = True
        
    except SovereigntyViolationError:
        validated = False
    
    return jsonify({
        'prediction': prediction['prediction'],
        'confidence': prediction['confidence'],
        'explanation': {
            'shap_values': shap_values.tolist(),
            'feature_contributions': feature_contributions,
            'evidence_chain': evidence_chain
        },
        'compliance': {
            'framework': 'EU_AI_ACT',
            'validated': validated
        }
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
```

## Visualization

### SHAP summary plot

```python
import shap
import matplotlib.pyplot as plt

# Generate SHAP values for dataset
shap_values = explainer.shap_values(X_test)

# Summary plot (feature importance)
shap.summary_plot(shap_values, X_test, plot_type="bar")
plt.savefig('shap_summary.png')
```

### SHAP waterfall plot

```python
# Waterfall plot for single prediction
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values[0],
        base_values=explainer.expected_value,
        data=X_test.iloc[0],
        feature_names=X_test.columns.tolist()
    )
)
plt.savefig('shap_waterfall.png')
```

### SHAP force plot

```python
# Force plot for single prediction
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    X_test.iloc[0],
    matplotlib=True
)
plt.savefig('shap_force.png')
```

## Model types

### AutoML Tabular

```python
from google.cloud import aiplatform

# Train AutoML model
job = aiplatform.AutoMLTabularTrainingJob(
    display_name="disease_classifier",
    optimization_prediction_type="classification"
)

model = job.run(dataset=dataset, target_column="disease")

# SHAP explainer
explainer = shap.TreeExplainer(model)
```

### Custom TensorFlow

```python
import tensorflow as tf
import shap

# Load custom model
model = tf.keras.models.load_model('gs://bucket/model')

# SHAP explainer for deep learning
explainer = shap.DeepExplainer(model, X_train[:100])
shap_values = explainer.shap_values(X_test)
```

### Scikit-learn

```python
from sklearn.ensemble import RandomForestClassifier
import shap

# Train scikit-learn model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# SHAP explainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)
```

## Compliance checklist

<Steps>
  <Step title="High-risk threshold">
    Set confidence threshold (default: 0.7) for requiring explainability
  </Step>
  <Step title="SHAP integration">
    Generate SHAP values for all high-risk predictions
  </Step>
  <Step title="SovereignGuardrail validation">
    Validate every high-risk inference with explainability
  </Step>
  <Step title="Evidence chain">
    Build human-readable evidence chain from SHAP values
  </Step>
  <Step title="Audit trail">
    Log all explanations to tamper-proof audit ledger
  </Step>
  <Step title="Patient consent">
    Verify consent token before processing
  </Step>
</Steps>

## Best practices

<AccordionGroup>
  <Accordion title="Use TreeExplainer for tree-based models">
    TreeExplainer is exact and fast for Random Forests, XGBoost, LightGBM
  </Accordion>
  <Accordion title="Use DeepExplainer for neural networks">
    DeepExplainer provides approximate SHAP values for deep learning models
  </Accordion>
  <Accordion title="Cache explainers">
    Initialize explainers once and reuse for multiple predictions
  </Accordion>
  <Accordion title="Set background dataset">
    Use representative sample (100-1000 instances) for background distribution
  </Accordion>
  <Accordion title="Visualize top features">
    Show only top 5-10 features in evidence chain for clarity
  </Accordion>
</AccordionGroup>

## Performance optimization

### Batch predictions

```python
# Batch predict with explainability
predictions = []
explanations = []

for batch in batches(X_test, batch_size=100):
    pred = endpoint.predict(instances=batch.values.tolist())
    shap_vals = explainer.shap_values(batch)
    
    predictions.extend(pred)
    explanations.extend(shap_vals)
```

### Caching

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_explanation(features_tuple):
    """Cache SHAP explanations for repeated queries"""
    X = pd.DataFrame([dict(features_tuple)])
    shap_values = explainer.shap_values(X)
    return shap_values
```

## Troubleshooting

<AccordionGroup>
  <Accordion title="SHAP values don't sum to prediction">
    Ensure you're using the correct explainer type (Tree vs Deep vs Kernel)
  </Accordion>
  <Accordion title="Explainer initialization slow">
    Use smaller background dataset (100-1000 samples)
  </Accordion>
  <Accordion title="Memory errors">
    Process data in batches and clear cache regularly
  </Accordion>
  <Accordion title="Validation fails">
    Check that confidence_score, explanation, and evidence_chain are all provided
  </Accordion>
</AccordionGroup>

## Next steps

<CardGroup cols={2}>
  <Card
    title="SovereignGuardrail"
    icon="shield-check"
    href="/security/sovereign-guardrail"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="Bio-Interface API"
    icon="mobile"
    href="/api-reference/bio-interface"
  >
    Mobile health app integration
  </Card>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Deploy autonomous surveillance
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/overview"
  >
    Deploy to Vertex AI
  </Card>
</CardGroup>
