---
title: Vertex AI + SHAP integration
description: Right to Explanation for high-risk clinical AI with SHAP explainability
---

## Overview

iLuminara-Core integrates Google Cloud Vertex AI with SHAP (SHapley Additive exPlanations) to provide **Right to Explanation** for every high-risk clinical inference, ensuring compliance with EU AI Act ¬ß6 and GDPR Art. 22.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk inference automatically triggers SHAP analysis to comply with EU AI Act ¬ß6 (High-Risk AI) and GDPR Art. 22 (Right to Explanation).
</Card>

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  VERTEX AI MODEL                        ‚îÇ
‚îÇ  - AutoML Time-Series Forecasting                      ‚îÇ
‚îÇ  - Custom Training (TensorFlow, PyTorch)               ‚îÇ
‚îÇ  - Model Registry & Versioning                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              HIGH-RISK INFERENCE DETECTOR               ‚îÇ
‚îÇ  - Confidence Score > 0.7                              ‚îÇ
‚îÇ  - Clinical Decision (diagnosis, treatment)            ‚îÇ
‚îÇ  - Resource Allocation                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 SHAP EXPLAINER                          ‚îÇ
‚îÇ  - TreeExplainer (for tree-based models)               ‚îÇ
‚îÇ  - DeepExplainer (for neural networks)                 ‚îÇ
‚îÇ  - KernelExplainer (model-agnostic)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              SOVEREIGNGUARDRAIL VALIDATION              ‚îÇ
‚îÇ  - Verify explanation completeness                     ‚îÇ
‚îÇ  - Enforce EU AI Act ¬ß6 requirements                   ‚îÇ
‚îÇ  - Log to tamper-proof audit trail                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## High-risk inference detection

The system automatically detects high-risk inferences that require explanation:

```python
from integrations.vertex_ai_shap import VertexAIExplainer
from governance_kernel.vector_ledger import SovereignGuardrail

explainer = VertexAIExplainer(
    project_id="iluminara-core",
    location="us-central1",
    model_name="cholera-forecaster"
)

# Make prediction
prediction = explainer.predict(features)

# Check if high-risk
if prediction['confidence'] > 0.7 and prediction['type'] == 'clinical_decision':
    # Automatically generate SHAP explanation
    explanation = explainer.explain(features)
    
    # Validate with SovereignGuardrail
    guardrail = SovereignGuardrail()
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': prediction['result'],
            'explanation': explanation['shap_values'],
            'confidence_score': prediction['confidence'],
            'evidence_chain': explanation['feature_importance']
        },
        jurisdiction='EU_AI_ACT'
    )
```

## SHAP integration

### TreeExplainer (for tree-based models)

```python
import shap
from google.cloud import aiplatform

# Load Vertex AI model
aiplatform.init(project="iluminara-core", location="us-central1")
model = aiplatform.Model("projects/123/locations/us-central1/models/456")

# Create SHAP explainer
explainer = shap.TreeExplainer(model)

# Generate explanations
shap_values = explainer.shap_values(X_test)

# Visualize
shap.summary_plot(shap_values, X_test, feature_names=feature_names)
```

### DeepExplainer (for neural networks)

```python
import shap
import tensorflow as tf

# Load model
model = tf.keras.models.load_model('cholera_forecaster.h5')

# Create SHAP explainer
explainer = shap.DeepExplainer(model, X_train[:100])

# Generate explanations
shap_values = explainer.shap_values(X_test)

# Visualize
shap.force_plot(explainer.expected_value, shap_values[0], X_test[0])
```

### KernelExplainer (model-agnostic)

```python
import shap

# Define prediction function
def predict_fn(X):
    return model.predict(X)

# Create SHAP explainer
explainer = shap.KernelExplainer(predict_fn, X_train[:100])

# Generate explanations
shap_values = explainer.shap_values(X_test)
```

## Explanation requirements

Per EU AI Act ¬ß6 and GDPR Art. 22, every high-risk inference must include:

<Steps>
  <Step title="Confidence score">
    Probability or confidence level of the prediction
  </Step>
  <Step title="Evidence chain">
    Input features that contributed to the decision
  </Step>
  <Step title="Feature contributions">
    SHAP values showing positive/negative impact of each feature
  </Step>
  <Step title="Decision rationale">
    Human-readable explanation of why the model made this prediction
  </Step>
</Steps>

## Example: Cholera outbreak prediction

```python
from integrations.vertex_ai_shap import CholeraForecaster

forecaster = CholeraForecaster()

# Input features
features = {
    'rainfall_mm': 45.2,
    'temperature_c': 28.5,
    'population_density': 1200,
    'water_quality_index': 0.65,
    'previous_cases': 12,
    'sanitation_coverage': 0.45
}

# Make prediction
prediction = forecaster.predict(features)

print(f"Outbreak Risk: {prediction['risk_score']:.2%}")
print(f"Confidence: {prediction['confidence']:.2%}")

# Get SHAP explanation
explanation = forecaster.explain(features)

print("\nFeature Contributions:")
for feature, contribution in explanation['feature_importance'].items():
    direction = "‚Üë" if contribution > 0 else "‚Üì"
    print(f"  {feature}: {contribution:+.3f} {direction}")

# Output:
# Outbreak Risk: 78.5%
# Confidence: 92.3%
# 
# Feature Contributions:
#   rainfall_mm: +0.245 ‚Üë
#   water_quality_index: +0.189 ‚Üë
#   previous_cases: +0.156 ‚Üë
#   sanitation_coverage: -0.134 ‚Üì
#   temperature_c: +0.089 ‚Üë
#   population_density: +0.067 ‚Üë
```

## Vertex AI AutoML integration

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project="iluminara-core",
    location="us-central1",
    staging_bucket="gs://iluminara-ml-staging"
)

# Create AutoML time-series dataset
dataset = aiplatform.TimeSeriesDataset.create(
    display_name="cholera-cases-dadaab",
    gcs_source="gs://iluminara-data/cholera_cases.csv",
    time_column="timestamp",
    time_series_identifier_column="location",
    target_column="case_count"
)

# Train AutoML model
job = aiplatform.AutoMLForecastingTrainingJob(
    display_name="cholera-forecaster",
    optimization_objective="minimize-rmse",
    column_transformations=[
        {"numeric": {"column_name": "rainfall_mm"}},
        {"numeric": {"column_name": "temperature_c"}},
        {"numeric": {"column_name": "water_quality_index"}},
    ],
)

model = job.run(
    dataset=dataset,
    target_column="case_count",
    time_column="timestamp",
    time_series_identifier_column="location",
    forecast_horizon=72,  # 72 hours
    data_granularity_unit="hour",
    data_granularity_count=1,
    budget_milli_node_hours=1000,
)

# Deploy model
endpoint = model.deploy(
    machine_type="n1-standard-4",
    min_replica_count=1,
    max_replica_count=10,
)
```

## Explainability dashboard

iLuminara provides a Streamlit dashboard for visualizing SHAP explanations:

```python
import streamlit as st
import shap
import matplotlib.pyplot as plt

st.title("üîç AI Explainability Dashboard")

# Load model and data
model = load_model()
X_test = load_test_data()

# Generate SHAP values
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Summary plot
st.subheader("Feature Importance")
fig, ax = plt.subplots()
shap.summary_plot(shap_values, X_test, show=False)
st.pyplot(fig)

# Force plot for individual prediction
st.subheader("Individual Prediction Explanation")
prediction_idx = st.slider("Select prediction", 0, len(X_test)-1, 0)
fig = shap.force_plot(
    explainer.expected_value,
    shap_values[prediction_idx],
    X_test.iloc[prediction_idx],
    matplotlib=True,
    show=False
)
st.pyplot(fig)
```

## Compliance validation

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# Validate high-risk inference
try:
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'actor': 'ml_system',
            'resource': 'patient_diagnosis',
            'explanation': shap_values.tolist(),
            'confidence_score': 0.92,
            'evidence_chain': feature_importance,
            'consent_token': 'valid_token',
            'consent_scope': 'diagnosis'
        },
        jurisdiction='EU_AI_ACT'
    )
    print("‚úÖ Compliant with EU AI Act ¬ß6")
except SovereigntyViolationError as e:
    print(f"‚ùå Compliance violation: {e}")
```

## Model registry

Track all models with explainability metadata:

```python
from google.cloud import aiplatform

# Register model with explainability metadata
model = aiplatform.Model.upload(
    display_name="cholera-forecaster-v2",
    artifact_uri="gs://iluminara-models/cholera-v2",
    serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-8:latest",
    explanation_metadata={
        "inputs": {
            "rainfall_mm": {"input_tensor_name": "rainfall"},
            "temperature_c": {"input_tensor_name": "temperature"},
        },
        "outputs": {
            "outbreak_risk": {"output_tensor_name": "risk"}
        }
    },
    explanation_parameters={
        "sampled_shapley_attribution": {
            "path_count": 10
        }
    }
)
```

## Performance considerations

- **SHAP computation time**: ~100-500ms per prediction
- **Caching**: Cache SHAP explainers for repeated use
- **Batch processing**: Generate explanations in batches for efficiency
- **Model complexity**: Simpler models = faster explanations

## Next steps

<CardGroup cols={2}>
  <Card
    title="Deploy to Vertex AI"
    icon="google"
    href="/deployment/gcp"
  >
    Deploy models to Google Cloud Platform
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Understand compliance enforcement
  </Card>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Integrate with autonomous agents
  </Card>
  <Card
    title="API reference"
    icon="terminal"
    href="/api-reference/overview"
  >
    Use prediction endpoints
  </Card>
</CardGroup>
