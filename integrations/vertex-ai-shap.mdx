---
title: Vertex AI + SHAP integration
description: Right to Explanation with SHAP explainability for high-risk AI inferences
---

## Overview

iLuminara-Core integrates Google Cloud Vertex AI with SHAP (SHapley Additive exPlanations) to provide the **Right to Explanation** for every high-risk clinical inference, ensuring compliance with EU AI Act §6 and GDPR Art. 22.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk inference automatically triggers SHAP analysis
</Card>

## Architecture

```
┌─────────────────────────────────────┐
│         VERTEX AI MODEL             │
│  - AutoML Time-Series               │
│  - Custom Training                  │
│  - Batch/Online Prediction          │
└─────────────────────────────────────┘
              │
              │ Prediction
              ▼
┌─────────────────────────────────────┐
│      SHAP EXPLAINER                 │
│  - Feature importance               │
│  - SHAP values                      │
│  - Decision rationale               │
└─────────────────────────────────────┘
              │
              │ Explanation
              ▼
┌─────────────────────────────────────┐
│    SOVEREIGN GUARDRAIL              │
│  - Validate explainability          │
│  - Enforce EU AI Act §6             │
│  - Audit trail                      │
└─────────────────────────────────────┘
```

## High-risk AI systems

According to EU AI Act §6, these systems require explainability:

<AccordionGroup>
  <Accordion title="Clinical diagnosis">
    AI systems that diagnose diseases or recommend treatments
  </Accordion>
  <Accordion title="Outbreak prediction">
    Models that predict disease outbreaks and trigger emergency responses
  </Accordion>
  <Accordion title="Resource allocation">
    Systems that allocate medical resources during emergencies
  </Accordion>
  <Accordion title="Patient triage">
    AI that prioritizes patients for treatment
  </Accordion>
</AccordionGroup>

## Implementation

### Train model with Vertex AI

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project="iluminara-health",
    location="africa-south1"
)

# Create AutoML time-series dataset
dataset = aiplatform.TimeSeriesDataset.create(
    display_name="cholera_outbreak_forecast",
    gcs_source="gs://iluminara-data/training/cholera_cases.csv",
    time_column="timestamp",
    time_series_identifier_column="location",
    target_column="case_count"
)

# Train AutoML model
model = aiplatform.AutoMLForecastingTrainingJob(
    display_name="cholera_forecast_model",
    optimization_objective="minimize-rmse",
    column_transformations=[
        {"numeric": {"column_name": "temperature"}},
        {"numeric": {"column_name": "rainfall"}},
        {"numeric": {"column_name": "population_density"}}
    ]
)

model.run(
    dataset=dataset,
    target_column="case_count",
    time_column="timestamp",
    time_series_identifier_column="location",
    forecast_horizon=72,  # 72 hours
    data_granularity_unit="hour",
    data_granularity_count=1
)
```

### Generate predictions with SHAP

```python
import shap
import numpy as np
from google.cloud import aiplatform

# Load trained model
model = aiplatform.Model("projects/123/locations/africa-south1/models/456")

# Prepare input data
input_data = {
    "location": "Dadaab",
    "temperature": 32.5,
    "rainfall": 15.2,
    "population_density": 850,
    "historical_cases": [12, 15, 18, 22, 27]
}

# Get prediction
prediction = model.predict(instances=[input_data])

# Generate SHAP explanation
explainer = shap.Explainer(model.predict, input_data)
shap_values = explainer(input_data)

# Extract explanation
explanation = {
    "prediction": prediction.predictions[0],
    "confidence_score": prediction.confidence,
    "shap_values": shap_values.values.tolist(),
    "feature_importance": {
        "temperature": shap_values.values[0],
        "rainfall": shap_values.values[1],
        "population_density": shap_values.values[2],
        "historical_cases": shap_values.values[3]
    },
    "decision_rationale": generate_rationale(shap_values)
}
```

### Validate with SovereignGuardrail

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# Validate high-risk inference
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'outbreak_prediction',
        'explanation': explanation,
        'confidence_score': prediction.confidence,
        'evidence_chain': [
            'temperature_anomaly',
            'rainfall_increase',
            'population_density_high',
            'historical_trend_upward'
        ],
        'consent_token': 'PUBLIC_HEALTH_SURVEILLANCE',
        'consent_scope': 'outbreak_response'
    },
    jurisdiction='EU_AI_ACT'
)
```

## SHAP visualization

### Feature importance waterfall

```python
import matplotlib.pyplot as plt

# Create SHAP waterfall plot
shap.plots.waterfall(shap_values[0])
plt.title("Feature Contributions to Outbreak Prediction")
plt.savefig("shap_waterfall.png")
```

### Force plot

```python
# Create SHAP force plot
shap.plots.force(
    explainer.expected_value,
    shap_values.values[0],
    input_data,
    matplotlib=True
)
plt.savefig("shap_force.png")
```

## Explanation generation

```python
def generate_rationale(shap_values):
    """
    Generate human-readable explanation from SHAP values.
    """
    features = [
        ("temperature", shap_values.values[0]),
        ("rainfall", shap_values.values[1]),
        ("population_density", shap_values.values[2]),
        ("historical_cases", shap_values.values[3])
    ]
    
    # Sort by absolute contribution
    features.sort(key=lambda x: abs(x[1]), reverse=True)
    
    rationale = []
    
    for feature, contribution in features[:3]:  # Top 3 features
        if contribution > 0:
            rationale.append(
                f"Higher {feature} increases outbreak risk by {abs(contribution):.2f}"
            )
        else:
            rationale.append(
                f"Lower {feature} decreases outbreak risk by {abs(contribution):.2f}"
            )
    
    return " | ".join(rationale)
```

## API integration

```python
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/predict-with-explanation', methods=['POST'])
def predict_with_explanation():
    """
    Predict outbreak with SHAP explanation.
    """
    data = request.json
    
    # Get prediction
    prediction = model.predict(instances=[data])
    
    # Generate SHAP explanation
    explainer = shap.Explainer(model.predict, data)
    shap_values = explainer(data)
    
    # Validate with SovereignGuardrail
    explanation = {
        "prediction": prediction.predictions[0],
        "confidence_score": prediction.confidence,
        "shap_values": shap_values.values.tolist(),
        "feature_importance": extract_feature_importance(shap_values),
        "decision_rationale": generate_rationale(shap_values)
    }
    
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'outbreak_prediction',
            'explanation': explanation,
            'confidence_score': prediction.confidence,
            'evidence_chain': extract_evidence_chain(shap_values)
        },
        jurisdiction='EU_AI_ACT'
    )
    
    return jsonify({
        "status": "success",
        "prediction": prediction.predictions[0],
        "explanation": explanation,
        "compliance": {
            "eu_ai_act": "COMPLIANT",
            "gdpr_art_22": "COMPLIANT",
            "explainability_provided": True
        }
    })
```

## Compliance requirements

### EU AI Act §6

<Steps>
  <Step title="Risk assessment">
    Classify AI system as high-risk based on use case
  </Step>
  <Step title="Explainability">
    Provide SHAP values and feature importance for every prediction
  </Step>
  <Step title="Human oversight">
    Enable human review of high-confidence predictions
  </Step>
  <Step title="Documentation">
    Maintain audit trail of all explanations
  </Step>
</Steps>

### GDPR Art. 22

- **Right to explanation** - Users can request explanation of automated decisions
- **Right to human intervention** - Users can challenge AI decisions
- **Right to contest** - Users can dispute automated decisions

## Performance optimization

### Batch SHAP computation

```python
# Compute SHAP values in batches
batch_size = 100
shap_values_batch = []

for i in range(0, len(predictions), batch_size):
    batch = predictions[i:i+batch_size]
    shap_values = explainer(batch)
    shap_values_batch.append(shap_values)

# Combine results
all_shap_values = np.concatenate(shap_values_batch)
```

### Caching explanations

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_cached_explanation(input_hash):
    """
    Cache SHAP explanations for identical inputs.
    """
    return compute_shap_explanation(input_hash)
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="Bio-Interface"
    icon="mobile"
    href="/integrations/bio-interface"
  >
    Mobile health app integration
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Compliance enforcement
  </Card>
  <Card
    title="API reference"
    icon="terminal"
    href="/api-reference/overview"
  >
    REST API endpoints
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/overview"
  >
    Deploy to production
  </Card>
</CardGroup>
