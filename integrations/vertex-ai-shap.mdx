---
title: Vertex AI + SHAP integration
description: Right to Explanation with SHAP explainability for high-risk AI inferences
---

## Overview

iLuminara integrates Vertex AI with SHAP (SHapley Additive exPlanations) to provide explainability for every high-risk clinical inference, enforcing the EU AI Act §6 and GDPR Art. 22 Right to Explanation.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  EU AI Act §6 (High-Risk AI), GDPR Art. 22 (Right to Explanation)
</Card>

## Architecture

```
┌──────────────────────────────────────────────────────────────┐
│                    VERTEX AI MODEL                            │
│  - AutoML Forecasting                                         │
│  - Custom Training                                            │
│  - Prediction Service                                         │
└──────────────────────────────────────────────────────────────┘
                            │
                            │ Prediction
                            ▼
┌──────────────────────────────────────────────────────────────┐
│                    SHAP EXPLAINER                             │
│  - Feature Importance                                         │
│  - SHAP Values                                                │
│  - Waterfall Plots                                            │
└──────────────────────────────────────────────────────────────┘
                            │
                            │ Explanation
                            ▼
┌──────────────────────────────────────────────────────────────┐
│                 SOVEREIGN GUARDRAIL                           │
│  - Validate Explanation                                       │
│  - Enforce Right to Explanation                               │
│  - Audit Trail                                                │
└──────────────────────────────────────────────────────────────┘
```

## Basic usage

### Train Vertex AI model

```python
from google.cloud import aiplatform
from cloud_oracle.vertex_ai_integration import VertexAIIntegration

# Initialize Vertex AI
aiplatform.init(
    project='iluminara-core',
    location='us-central1'
)

# Create integration
vertex_integration = VertexAIIntegration(
    project_id='iluminara-core',
    location='us-central1'
)

# Train forecasting model
model = vertex_integration.train_forecasting_model(
    training_data=historical_cases,
    target_column='cases',
    time_column='timestamp',
    forecast_horizon=72
)

print(f"Model trained: {model.resource_name}")
```

### Make prediction with SHAP explanation

```python
from cloud_oracle.vertex_explainability import VertexExplainer

explainer = VertexExplainer(
    model_resource_name=model.resource_name,
    enable_shap=True
)

# Make prediction
prediction = explainer.predict_with_explanation(
    instances=[{
        'location': 'Dadaab',
        'symptoms': ['diarrhea', 'vomiting'],
        'population': 200000,
        'historical_cases': [10, 15, 20, 25, 30]
    }]
)

# Extract SHAP values
shap_values = prediction['explanation']['shap_values']
feature_importance = prediction['explanation']['feature_importance']

print(f"Prediction: {prediction['predicted_cases']}")
print(f"SHAP values: {shap_values}")
print(f"Feature importance: {feature_importance}")
```

## SHAP explainability

### Feature importance

SHAP values show how much each feature contributed to the prediction:

```python
import shap
import numpy as np

# Get SHAP explainer
explainer = shap.Explainer(model)

# Calculate SHAP values
shap_values = explainer(X_test)

# Plot feature importance
shap.summary_plot(shap_values, X_test, plot_type="bar")
```

**Example output:**
```
Feature Importance (SHAP):
1. historical_cases_trend:  0.45
2. population_density:      0.28
3. symptom_severity:        0.15
4. seasonal_factor:         0.08
5. geographic_proximity:    0.04
```

### Waterfall plot

Shows how each feature pushes the prediction from the base value:

```python
# Plot waterfall for single prediction
shap.waterfall_plot(shap_values[0])
```

**Interpretation:**
```
Base value: 50 cases
+ historical_cases_trend: +30
+ population_density: +15
+ symptom_severity: +8
- seasonal_factor: -3
= Final prediction: 100 cases
```

### Force plot

Interactive visualization of feature contributions:

```python
# Generate force plot
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    X_test.iloc[0]
)
```

## Sovereignty compliance

### Validate high-risk inference

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# Validate prediction with explanation
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'outbreak_prediction',
        'explanation': {
            'shap_values': shap_values.tolist(),
            'feature_importance': feature_importance,
            'confidence_score': 0.92
        },
        'evidence_chain': [
            'historical_cases_trend',
            'population_density',
            'symptom_severity'
        ],
        'consent_token': 'PUBLIC_HEALTH_SURVEILLANCE',
        'consent_scope': 'outbreak_forecasting'
    },
    jurisdiction='EU_AI_ACT'
)
```

### Explanation requirements

The SovereignGuardrail enforces these requirements for high-risk AI:

<Steps>
  <Step title="Confidence score">
    Must be provided for all predictions
  </Step>
  <Step title="Evidence chain">
    List of features that contributed to the decision
  </Step>
  <Step title="Feature contributions">
    SHAP values or similar explainability metrics
  </Step>
  <Step title="Decision rationale">
    Human-readable explanation of the prediction
  </Step>
</Steps>

## Integration with API

### Explainable prediction endpoint

```python
# api_service.py
from flask import Flask, request, jsonify
from cloud_oracle.vertex_explainability import VertexExplainer

app = Flask(__name__)
explainer = VertexExplainer(model_resource_name='projects/...')

@app.route('/predict-explainable', methods=['POST'])
def predict_explainable():
    data = request.get_json()
    
    # Make prediction with explanation
    result = explainer.predict_with_explanation(
        instances=[data]
    )
    
    # Validate with SovereignGuardrail
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'outbreak_prediction',
            'explanation': result['explanation'],
            'confidence_score': result['confidence']
        },
        jurisdiction='EU_AI_ACT'
    )
    
    return jsonify(result)
```

### Request example

```bash
curl -X POST http://localhost:8080/predict-explainable \
  -H "Content-Type: application/json" \
  -d '{
    "location": "Dadaab",
    "symptoms": ["diarrhea", "vomiting"],
    "population": 200000
  }'
```

### Response with explanation

```json
{
  "status": "success",
  "predicted_cases": 11776,
  "confidence": 0.92,
  "explanation": {
    "shap_values": {
      "historical_cases_trend": 0.45,
      "population_density": 0.28,
      "symptom_severity": 0.15,
      "seasonal_factor": 0.08,
      "geographic_proximity": 0.04
    },
    "feature_importance": [
      {"feature": "historical_cases_trend", "importance": 0.45},
      {"feature": "population_density", "importance": 0.28},
      {"feature": "symptom_severity", "importance": 0.15}
    ],
    "decision_rationale": "High outbreak risk due to rapid increase in historical cases (45% contribution) and high population density (28% contribution). Symptom severity indicates cholera-like illness.",
    "evidence_chain": [
      "Historical cases show 300% increase over 7 days",
      "Population density of 200,000 in confined area",
      "Classic cholera symptoms (diarrhea + vomiting + dehydration)"
    ]
  },
  "compliance": {
    "framework": "EU_AI_ACT",
    "article": "§6 (High-Risk AI)",
    "explanation_provided": true,
    "audit_trail_id": "audit_12345"
  }
}
```

## Vertex AI Explainable AI

### Enable Explainable AI

```python
from google.cloud import aiplatform

# Create model with explainability
model = aiplatform.Model.upload(
    display_name='cholera_forecast_explainable',
    artifact_uri='gs://iluminara-models/cholera_forecast',
    serving_container_image_uri='gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-8:latest',
    explanation_metadata={
        'inputs': {
            'historical_cases': {'input_tensor_name': 'historical_cases'},
            'population': {'input_tensor_name': 'population'},
            'symptoms': {'input_tensor_name': 'symptoms'}
        },
        'outputs': {
            'predicted_cases': {'output_tensor_name': 'predicted_cases'}
        }
    },
    explanation_parameters={
        'sampled_shapley_attribution': {
            'path_count': 10
        }
    }
)
```

### Get explanations

```python
# Make prediction with explanation
endpoint = model.deploy(
    machine_type='n1-standard-4',
    min_replica_count=1,
    max_replica_count=10
)

prediction = endpoint.explain(
    instances=[{
        'historical_cases': [10, 15, 20, 25, 30],
        'population': 200000,
        'symptoms': ['diarrhea', 'vomiting']
    }]
)

# Extract attributions
attributions = prediction.explanations[0].attributions
for attribution in attributions:
    print(f"{attribution.feature_name}: {attribution.attribution}")
```

## Audit trail

All explainable predictions are logged to the tamper-proof audit trail:

```python
from governance_kernel.audit_trail import AuditTrail

audit = AuditTrail(enable_tamper_proof=True)

# Log explainable prediction
audit.log_event(
    event_type='HIGH_RISK_INFERENCE',
    actor='vertex_ai_model',
    resource='outbreak_prediction',
    details={
        'model_id': model.resource_name,
        'prediction': predicted_cases,
        'confidence': confidence_score,
        'shap_values': shap_values,
        'explanation_provided': True,
        'compliance_framework': 'EU_AI_ACT'
    }
)
```

## Performance

| Metric | Value | Target |
|--------|-------|--------|
| Prediction latency | 120ms | <200ms |
| Explanation latency | 80ms | <100ms |
| Total latency | 200ms | <300ms |
| Explanation accuracy | 95% | >90% |

## Next steps

<CardGroup cols={2}>
  <Card
    title="Bio-Interface API"
    icon="mobile"
    href="/integrations/bio-interface"
  >
    Mobile health app integration
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Understand compliance enforcement
  </Card>
  <Card
    title="Cloud Oracle"
    icon="cloud"
    href="/architecture/cloud-oracle"
  >
    Deep dive into forecasting
  </Card>
  <Card
    title="Deploy to GCP"
    icon="rocket"
    href="/deployment/gcp"
  >
    Production deployment
  </Card>
</CardGroup>
