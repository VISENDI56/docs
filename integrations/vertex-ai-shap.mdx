---
title: Vertex AI + SHAP integration
description: Explainable AI with SHAP analysis for regulatory compliance
---

## Overview

The Vertex AI + SHAP integration provides **Right to Explanation** capabilities required by EU AI Act §6 and GDPR Art. 22. Every high-risk AI inference includes SHAP (SHapley Additive exPlanations) analysis for complete transparency.

<Card
  title=\"Philosophy\"
  icon=\"brain-circuit\"
>
  \"Every AI decision affecting human welfare must be transparent, auditable, and explainable.\"
</Card>

## Architecture

```
┌────────────────────────────────────────────────────────────────┐
│                   VERTEX AI + SHAP INTEGRATION                  │
└────────────────────────────────────────────────────────────────┘
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
   ┌────▼────┐      ┌──────▼──────┐    ┌──────▼──────┐
   │ VERTEX  │      │    SHAP     │    │  GOVERNANCE │
   │   AI    │──────│  EXPLAINER  │────│   KERNEL    │
   │ (Model) │      │  (Analysis) │    │ (Validation)│
   └─────────┘      └─────────────┘    └─────────────┘
```

## Key features

<CardGroup cols={2}>\n  <Card title=\"SHAP analysis\" icon=\"chart-line\">\n    Feature importance attribution for all predictions\n  </Card>\n  <Card title=\"Regulatory compliance\" icon=\"scale-balanced\">\n    EU AI Act §6, GDPR Art. 22, NIST AI RMF\n  </Card>\n  <Card title=\"Audit trail\" icon=\"file-contract\">\n    Tamper-proof logging of all explanations\n  </Card>\n  <Card title=\"Real-time\" icon=\"bolt\">\n    <5 second latency for explanation generation\n  </Card>\n</CardGroup>\n\n## Installation\n\n```bash\npip install google-cloud-aiplatform shap numpy pandas\n```\n\n## Basic usage\n\n```python\nfrom governance_kernel.humanitarian_constraints import VertexAIExplainableAI\n\n# Initialize explainer\nexplainer = VertexAIExplainableAI(\n    project_id=\"iluminara-core\",\n    location=\"us-central1\"\n)\n\n# Generate explanation for a prediction\nexplanation = explainer.explain_prediction(\n    model_id=\"outbreak-predictor-v1\",\n    input_data={\n        \"cbs_signals\": 45,\n        \"z_score\": 3.8,\n        \"location\": \"Dadaab\",\n        \"population\": 200000,\n        \"healthcare_capacity\": 0.5\n    },\n    prediction=\"OUTBREAK_LIKELY\",\n    feature_names=[\"cbs_signals\", \"z_score\", \"location\", \"population\", \"healthcare_capacity\"]\n)\n\n# Get top contributing factors\ntop_factors = explanation.get_top_contributors(n=3)\nprint(f\"Top 3 factors: {top_factors}\")\n\n# Get SHAP values\nshap_values = explanation.shap_values\nprint(f\"SHAP values: {shap_values}\")\n\n# Export for audit\naudit_report = explanation.to_dict()\n```\n\n## SHAP explanation structure\n\n```python\n{\n  \"decision_id\": \"DEC-2025-001\",\n  \"timestamp\": \"2025-12-28T22:00:00Z\",\n  \"model_id\": \"outbreak-predictor-v1\",\n  \"prediction\": \"OUTBREAK_LIKELY\",\n  \"confidence_score\": 0.92,\n  \"shap_values\": {\n    \"cbs_signals\": 0.45,\n    \"z_score\": 0.38,\n    \"location\": 0.09,\n    \"population\": 0.05,\n    \"healthcare_capacity\": -0.15\n  },\n  \"feature_importance\": [\n    {\"feature\": \"cbs_signals\", \"importance\": 0.45, \"direction\": \"positive\"},\n    {\"feature\": \"z_score\", \"importance\": 0.38, \"direction\": \"positive\"},\n    {\"feature\": \"healthcare_capacity\", \"importance\": 0.15, \"direction\": \"negative\"}\n  ],\n  \"top_contributors\": [\n    \"cbs_signals (45 reports)\",\n    \"z_score (3.8 standard deviations)\",\n    \"healthcare_capacity (50% capacity)\"\n  ],\n  \"explanation_text\": \"The model predicts an outbreak is likely based primarily on the high number of community-based surveillance signals (45) and elevated Z-score (3.8). Low healthcare capacity (50%) increases vulnerability.\",\n  \"compliance\": {\n    \"eu_ai_act_s6\": \"COMPLIANT\",\n    \"gdpr_art22\": \"COMPLIANT\",\n    \"nist_ai_rmf\": \"COMPLIANT\"\n  }\n}\n```\n\n## Integration with SovereignGuardrail\n\n```python\nfrom governance_kernel.vector_ledger import SovereignGuardrail\nfrom governance_kernel.humanitarian_constraints import VertexAIExplainableAI\n\n# Initialize systems\nguardrail = SovereignGuardrail()\nexplainer = VertexAIExplainableAI()\n\n# Generate SHAP explanation\nexplanation = explainer.explain_prediction(\n    model_id=\"risk-model\",\n    input_data={\"feature1\": 10, \"feature2\": 20},\n    prediction=\"HIGH_RISK\",\n    feature_names=[\"feature1\", \"feature2\"]\n)\n\n# Validate with SovereignGuardrail\nguardrail.validate_action(\n    action_type=\"High_Risk_Inference\",\n    payload={\n        \"explanation\": explanation.to_dict(),\n        \"confidence_score\": 0.87,\n        \"evidence_chain\": explanation.get_top_contributors(n=3),\n        \"consent_token\": \"TOKEN-123\",\n        \"consent_scope\": \"diagnosis\"\n    },\n    jurisdiction=\"EU_AI_ACT\"\n)\n```\n\n## Vertex AI model deployment\n\n### Deploy a model to Vertex AI\n\n```python\nfrom google.cloud import aiplatform\n\n# Initialize Vertex AI\naiplatform.init(\n    project=\"iluminara-core\",\n    location=\"us-central1\"\n)\n\n# Upload model\nmodel = aiplatform.Model.upload(\n    display_name=\"outbreak-predictor-v1\",\n    artifact_uri=\"gs://iluminara-models/outbreak-predictor\",\n    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n)\n\n# Deploy to endpoint\nendpoint = model.deploy(\n    machine_type=\"n1-standard-4\",\n    min_replica_count=1,\n    max_replica_count=10\n)\n```\n\n### Generate predictions with explanations\n\n```python\n# Make prediction\ninstances = [{\n    \"cbs_signals\": 45,\n    \"z_score\": 3.8,\n    \"location\": \"Dadaab\"\n}]\n\npredictions = endpoint.predict(instances=instances)\n\n# Generate SHAP explanation\nexplanation = explainer.explain_prediction(\n    model_id=model.resource_name,\n    input_data=instances[0],\n    prediction=predictions.predictions[0],\n    feature_names=list(instances[0].keys())\n)\n```\n\n## SHAP visualization\n\n```python\nimport shap\nimport matplotlib.pyplot as plt\n\n# Create SHAP explainer\nshap_explainer = shap.Explainer(model)\n\n# Generate SHAP values\nshap_values = shap_explainer(X_test)\n\n# Waterfall plot (single prediction)\nshap.plots.waterfall(shap_values[0])\n\n# Summary plot (all predictions)\nshap.plots.summary(shap_values, X_test)\n\n# Force plot (interactive)\nshap.plots.force(shap_values[0])\n```\n\n## Compliance validation\n\n### EU AI Act §6 (High-Risk AI Systems)\n\n```python\ndef validate_eu_ai_act_compliance(explanation):\n    \"\"\"Validate compliance with EU AI Act §6\"\"\"\n    \n    # Check 1: Explanation exists\n    assert explanation is not None, \"No explanation provided\"\n    \n    # Check 2: SHAP values present\n    assert explanation.shap_values, \"SHAP values missing\"\n    \n    # Check 3: Feature importance documented\n    assert explanation.feature_importance, \"Feature importance missing\"\n    \n    # Check 4: Human-readable explanation\n    assert explanation.explanation_text, \"Human-readable explanation missing\"\n    \n    # Check 5: Confidence score disclosed\n    assert explanation.confidence_score is not None, \"Confidence score missing\"\n    \n    return True\n```\n\n### GDPR Art. 22 (Right to Explanation)\n\n```python\ndef validate_gdpr_art22_compliance(explanation):\n    \"\"\"Validate compliance with GDPR Art. 22\"\"\"\n    \n    # Check 1: Logic of decision explained\n    assert explanation.explanation_text, \"Decision logic not explained\"\n    \n    # Check 2: Significance and consequences disclosed\n    assert explanation.top_contributors, \"Significance not disclosed\"\n    \n    # Check 3: Meaningful information provided\n    assert len(explanation.feature_importance) > 0, \"No meaningful information\"\n    \n    return True\n```\n\n## Performance optimization\n\n### Caching explanations\n\n```python\nfrom functools import lru_cache\nimport hashlib\nimport json\n\nclass CachedExplainer:\n    def __init__(self, explainer):\n        self.explainer = explainer\n        self.cache = {}\n    \n    def _cache_key(self, input_data):\n        \"\"\"Generate cache key from input data\"\"\"\n        data_str = json.dumps(input_data, sort_keys=True)\n        return hashlib.sha256(data_str.encode()).hexdigest()\n    \n    def explain_prediction(self, model_id, input_data, prediction, feature_names):\n        \"\"\"Explain prediction with caching\"\"\"\n        cache_key = self._cache_key(input_data)\n        \n        if cache_key in self.cache:\n            return self.cache[cache_key]\n        \n        explanation = self.explainer.explain_prediction(\n            model_id, input_data, prediction, feature_names\n        )\n        \n        self.cache[cache_key] = explanation\n        return explanation\n```\n\n### Batch explanations\n\n```python\ndef batch_explain(explainer, model_id, instances, predictions, feature_names):\n    \"\"\"Generate explanations for multiple predictions\"\"\"\n    explanations = []\n    \n    for instance, prediction in zip(instances, predictions):\n        explanation = explainer.explain_prediction(\n            model_id=model_id,\n            input_data=instance,\n            prediction=prediction,\n            feature_names=feature_names\n        )\n        explanations.append(explanation)\n    \n    return explanations\n```\n\n## Audit trail integration\n\n```python\nfrom governance_kernel.vector_ledger import SovereignGuardrail\n\n# Initialize with tamper-proof audit\nguardrail = SovereignGuardrail(enable_tamper_proof_audit=True)\n\n# Generate explanation\nexplanation = explainer.explain_prediction(\n    model_id=\"outbreak-predictor-v1\",\n    input_data={\"cbs_signals\": 45},\n    prediction=\"OUTBREAK_LIKELY\",\n    feature_names=[\"cbs_signals\"]\n)\n\n# Log to tamper-proof audit trail\nguardrail.validate_action(\n    action_type=\"High_Risk_Inference\",\n    payload={\n        \"decision_id\": explanation.decision_id,\n        \"explanation\": explanation.to_dict(),\n        \"confidence_score\": explanation.confidence_score,\n        \"evidence_chain\": explanation.top_contributors\n    },\n    jurisdiction=\"EU_AI_ACT\"\n)\n\n# Retrieve audit history\naudit_history = guardrail.get_tamper_proof_audit_history(limit=100)\n```\n\n## Testing\n\n```python\nimport pytest\nfrom governance_kernel.humanitarian_constraints import VertexAIExplainableAI\n\ndef test_shap_explanation():\n    explainer = VertexAIExplainableAI()\n    \n    explanation = explainer.explain_prediction(\n        model_id=\"test-model\",\n        input_data={\"feature1\": 10},\n        prediction=\"HIGH_RISK\",\n        feature_names=[\"feature1\"]\n    )\n    \n    assert explanation is not None\n    assert explanation.shap_values\n    assert explanation.feature_importance\n    assert explanation.explanation_text\n\ndef test_eu_ai_act_compliance():\n    explainer = VertexAIExplainableAI()\n    \n    explanation = explainer.explain_prediction(\n        model_id=\"test-model\",\n        input_data={\"feature1\": 10},\n        prediction=\"HIGH_RISK\",\n        feature_names=[\"feature1\"]\n    )\n    \n    assert validate_eu_ai_act_compliance(explanation)\n    assert validate_gdpr_art22_compliance(explanation)\n```\n\n## Next steps\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Governance kernel\"\n    icon=\"shield-check\"\n    href=\"/governance/overview\"\n  >\n    Integrate with compliance enforcement\n  </Card>\n  <Card\n    title=\"AI agents\"\n    icon=\"brain-circuit\"\n    href=\"/ai-agents/overview\"\n  >\n    Deploy autonomous surveillance agents\n  </Card>\n  <Card\n    title=\"Bio-Interface API\"\n    icon=\"dna\"\n    href=\"/integrations/bio-interface\"\n  >\n    Mobile health app integration\n  </Card>\n  <Card\n    title=\"Security stack\"\n    icon=\"shield-halved\"\n    href=\"/security/overview\"\n  >\n    Sovereign Health Fortress security\n  </Card>\n</CardGroup>\n