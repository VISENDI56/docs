---
title: Vertex AI + SHAP integration
description: Right to Explanation for high-risk AI inferences with SHAP explainability
---

## Overview

Every high-risk clinical inference in iLuminara-Core requires explainability. The Vertex AI + SHAP integration provides transparent, auditable AI decision-making that complies with the EU AI Act Â§6 and GDPR Art. 22.

<Card
  title="Compliance mandate"
  icon="scale-balanced"
>
  "Every high-risk clinical inference requires explainability (SHAP values, feature importance)." â€” EU AI Act Â§6
</Card>

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Mobile App / CHV Device         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Vertex AI Endpoint              â”‚
â”‚  - Cholera outbreak prediction          â”‚
â”‚  - Malaria risk assessment              â”‚
â”‚  - Disease classification               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Explainability Threshold Check       â”‚
â”‚    Confidence > 0.7 â†’ Trigger SHAP      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SHAP Explainer                  â”‚
â”‚  - Feature contributions                â”‚
â”‚  - Evidence chain                       â”‚
â”‚  - Decision rationale                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Tamper-Proof Audit Trail           â”‚
â”‚  BigQuery â†’ Cloud Spanner â†’ Cloud KMS   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Risk levels

| Level | Confidence Range | Explainability Required | Compliance |
|-------|------------------|-------------------------|------------|
| **LOW** | < 0.5 | âŒ No | Routine monitoring |
| **MEDIUM** | 0.5 - 0.7 | âŒ No | Increased surveillance |
| **HIGH** | 0.7 - 0.9 | âœ… Yes | EU AI Act Â§6, GDPR Art. 22 |
| **CRITICAL** | > 0.9 | âœ… Yes | Mandatory audit trail |

## Basic usage

```python
from cloud_oracle.vertex_ai_shap import VertexAIExplainer

# Initialize explainer
explainer = VertexAIExplainer(
    project_id="iluminara-core",
    location="us-central1",
    model_name="cholera-outbreak-predictor",
    explainability_threshold=0.7,
    enable_audit=True
)

# Prepare input features
instances = [{
    "fever": 1,
    "diarrhea": 1,
    "vomiting": 1,
    "dehydration": 1,
    "location_risk": 0.8,
    "population_density": 0.9,
    "water_quality": 0.3
}]

feature_names = [
    "fever", "diarrhea", "vomiting", "dehydration",
    "location_risk", "population_density", "water_quality"
]

# Make prediction with explanation
result = explainer.predict_with_explanation(
    instances=instances,
    feature_names=feature_names,
    patient_id="PAT_12345",
    jurisdiction="GDPR_EU"
)
```

## Response format

### High-risk prediction (confidence > 0.7)

```json
{
  "predictions": [[0.95, 0.05]],
  "confidence": 0.95,
  "risk_level": "CRITICAL",
  "timestamp": "2025-12-20T10:00:00Z",
  "model_name": "cholera-outbreak-predictor",
  "patient_id": "PAT_12345",
  "jurisdiction": "GDPR_EU",
  "explanation": {
    "method": "SHAP",
    "feature_contributions": {
      "fever": 0.35,
      "diarrhea": 0.28,
      "location_risk": 0.22,
      "dehydration": 0.10,
      "vomiting": 0.05
    },
    "evidence_chain": [
      "Primary factor: fever (35% contribution)",
      "Secondary factor: diarrhea (28% contribution)",
      "Contributing factor: location_risk (22% contribution)"
    ],
    "base_value": 0.15,
    "prediction_value": 0.95
  },
  "compliance": {
    "explainability_required": true,
    "explainability_method": "SHAP",
    "frameworks": [
      "EU AI Act Â§6 (High-Risk AI Systems)",
      "GDPR Art. 22 (Right to Explanation)",
      "HIPAA Â§164.524 (Right of Access)"
    ],
    "audit_trail": true
  }
}
```

### Low-risk prediction (confidence < 0.7)

```json
{
  "predictions": [[0.45, 0.55]],
  "confidence": 0.45,
  "risk_level": "LOW",
  "explanation": null,
  "compliance": {
    "explainability_required": false,
    "reason": "Confidence 45% below threshold 70%"
  }
}
```

## SHAP explanation

SHAP (SHapley Additive exPlanations) provides feature-level contributions to predictions:

### Feature contributions

Each feature's contribution to the final prediction:

```python
{
  "fever": 0.35,           # 35% contribution (positive)
  "diarrhea": 0.28,        # 28% contribution (positive)
  "location_risk": 0.22,   # 22% contribution (positive)
  "dehydration": 0.10,     # 10% contribution (positive)
  "vomiting": 0.05,        # 5% contribution (positive)
  "population_density": 0.03,
  "water_quality": -0.03   # Negative contribution (protective factor)
}
```

### Evidence chain

Human-readable explanation of the decision:

```
1. Primary factor: fever (35% contribution)
2. Secondary factor: diarrhea (28% contribution)
3. Contributing factor: location_risk (22% contribution)
4. Contributing factor: dehydration (10% contribution)
5. Contributing factor: vomiting (5% contribution)
```

## Compliance validation

Validate that predictions meet compliance requirements:

```python
# Validate compliance
is_compliant = explainer.validate_compliance(result)

if not is_compliant:
    print("âŒ Compliance violation detected")
    # Block action or escalate
else:
    print("âœ… Compliance validation passed")
```

**Validation checks:**
- High-risk predictions have explanations
- Explanation includes required fields (method, feature_contributions, evidence_chain)
- Audit trail is enabled
- Tamper-proof logging is active

## Batch predictions

Process multiple predictions with explanations:

```python
# Batch prediction
results = explainer.batch_predict_with_explanations(
    instances=[instance1, instance2, instance3],
    feature_names=feature_names,
    patient_ids=["PAT_001", "PAT_002", "PAT_003"]
)

# Process results
for result in results:
    if result["risk_level"] in ["HIGH", "CRITICAL"]:
        print(f"ğŸš¨ High-risk case: {result['patient_id']}")
        print(f"   Evidence: {result['explanation']['evidence_chain'][0]}")
```

## Audit trail

All high-risk inferences are logged to BigQuery for tamper-proof audit:

```sql
-- Query audit trail
SELECT
  timestamp,
  patient_id,
  confidence,
  risk_level,
  explainability_method,
  feature_contributions,
  evidence_chain
FROM `iluminara_audit.ai_explanations`
WHERE risk_level IN ('HIGH', 'CRITICAL')
ORDER BY timestamp DESC
LIMIT 100
```

## Integration with SovereignGuardrail

AI inferences are validated against sovereignty constraints:

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# Validate high-risk inference
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'cholera_diagnosis',
        'explanation': result['explanation'],
        'confidence_score': result['confidence'],
        'evidence_chain': result['explanation']['evidence_chain'],
        'consent_token': 'VALID_TOKEN',
        'consent_scope': 'diagnosis'
    },
    jurisdiction='GDPR_EU'
)
```

## Model deployment

### Deploy to Vertex AI

```bash
# Deploy model to Vertex AI
gcloud ai models upload \
  --region=us-central1 \
  --display-name=cholera-outbreak-predictor \
  --container-image-uri=gcr.io/iluminara-core/cholera-model:latest

# Create endpoint
gcloud ai endpoints create \
  --region=us-central1 \
  --display-name=cholera-outbreak-predictor

# Deploy model to endpoint
gcloud ai endpoints deploy-model ENDPOINT_ID \
  --region=us-central1 \
  --model=MODEL_ID \
  --display-name=cholera-v1
```

### Configure explainability

```python
# Configure Vertex AI Explainability
from google.cloud import aiplatform

aiplatform.init(project="iluminara-core", location="us-central1")

model = aiplatform.Model.upload(
    display_name="cholera-outbreak-predictor",
    artifact_uri="gs://iluminara-models/cholera",
    serving_container_image_uri="gcr.io/iluminara-core/cholera-model:latest",
    explanation_metadata={
        "inputs": {
            "fever": {"input_tensor_name": "fever"},
            "diarrhea": {"input_tensor_name": "diarrhea"},
            # ... other features
        },
        "outputs": {
            "prediction": {"output_tensor_name": "prediction"}
        }
    },
    explanation_parameters={
        "sampled_shapley_attribution": {
            "path_count": 10
        }
    }
)
```

## Performance considerations

- **Explainability overhead**: ~50-100ms per prediction
- **Batch processing**: Recommended for >10 predictions
- **Caching**: Cache SHAP explainer for repeated predictions
- **Async processing**: Use async for non-blocking explanations

## Compliance frameworks

<AccordionGroup>
  <Accordion title="EU AI Act Â§6 (High-Risk AI Systems)">
    Requires transparency and explainability for high-risk AI systems used in healthcare
  </Accordion>
  <Accordion title="GDPR Art. 22 (Right to Explanation)">
    Individuals have the right to obtain an explanation of automated decisions
  </Accordion>
  <Accordion title="HIPAA Â§164.524 (Right of Access)">
    Patients have the right to access their health information and understand how decisions are made
  </Accordion>
  <Accordion title="ISO 27001 A.18.1.4">
    Privacy and protection of personally identifiable information requires transparency
  </Accordion>
</AccordionGroup>

## Next steps

<CardGroup cols={2}>
  <Card
    title="Bio-Interface API"
    icon="mobile"
    href="/integrations/bio-interface"
  >
    Integrate mobile health apps with Golden Thread
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure sovereignty constraints
  </Card>
  <Card
    title="Audit trail"
    icon="file-contract"
    href="/governance/audit"
  >
    Set up tamper-proof logging
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/gcp"
  >
    Deploy to Google Cloud Platform
  </Card>
</CardGroup>
