---
title: Vertex AI + SHAP integration
description: Right to Explanation for high-risk clinical AI inferences
---

## Overview

Every high-risk clinical inference in iLuminara-Core requires explainability through SHAP (SHapley Additive exPlanations) analysis, ensuring compliance with EU AI Act Â§6 and GDPR Art. 22.

<Card
  title="Compliance mandate"
  icon="scale-balanced"
>
  "Every high-risk clinical inference requires explainability (SHAP values, feature importance). Enforces EU AI Act Â§6, GDPR Art. 22."
</Card>

## High-risk thresholds

Inferences above these confidence thresholds automatically trigger SHAP analysis:

| Use Case | Threshold | Rationale |
|----------|-----------|-----------|
| Clinical Diagnosis | 0.7 | Direct patient care decision |
| Outbreak Prediction | 0.8 | Public health emergency response |
| Resource Allocation | 0.6 | Impacts access to care |
| Triage Decision | 0.75 | Life-or-death prioritization |

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      MOBILE APP / CHV DEVICE        â”‚
â”‚  - Symptom collection               â”‚
â”‚  - Vital signs                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      VERTEX AI ENDPOINT             â”‚
â”‚  - Model prediction                 â”‚
â”‚  - Confidence scoring               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼ (if confidence >= threshold)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      SHAP EXPLAINER                 â”‚
â”‚  - Feature importance               â”‚
â”‚  - SHAP values                      â”‚
â”‚  - Evidence chain                   â”‚
â”‚  - Decision rationale               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      SOVEREIGN GUARDRAIL            â”‚
â”‚  - Compliance validation            â”‚
â”‚  - Audit logging                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Basic usage

### Initialize explainer

```python
from cloud_oracle.vertex_ai_shap import VertexAIExplainer

explainer = VertexAIExplainer(
    project_id="iluminara-core",
    location="us-central1",
    enable_compliance_logging=True
)
```

### Make explainable prediction

```python
# Prepare input
instances = [{
    'fever': 1,
    'cough': 1,
    'fatigue': 1,
    'age': 35,
    'location_risk': 0.8
}]

feature_names = ['fever', 'cough', 'fatigue', 'age', 'location_risk']

# Predict with explanation
results = explainer.predict_with_explanation(
    endpoint_id="projects/123/locations/us-central1/endpoints/456",
    instances=instances,
    feature_names=feature_names,
    high_risk_threshold=0.7
)

# Access explanation
result = results[0]
print(f"Prediction: {result.prediction}")
print(f"Confidence: {result.confidence_score:.2%}")
print(f"Compliance: {result.compliance_status}")
print(f"\nSHAP Values:")
for feature, value in result.shap_values.items():
    print(f"  {feature}: {value:.3f}")
```

## Outbreak prediction example

```python
from cloud_oracle.vertex_ai_shap import OutbreakPredictor

predictor = OutbreakPredictor(
    project_id="iluminara-core",
    location="us-central1"
)

# Predict outbreak risk
result = predictor.predict_outbreak_risk(
    endpoint_id="projects/123/locations/us-central1/endpoints/456",
    location={'lat': 0.0512, 'lng': 40.3129},
    symptoms=['diarrhea', 'vomiting', 'fever'],
    environmental_factors={
        'temperature': 32.5,
        'rainfall': 15.2,
        'water_quality': 0.6
    },
    population_density=1500
)

print(f"âœ… Prediction: {result.prediction}")
print(f"ğŸ“Š Confidence: {result.confidence_score:.2%}")
print(f"ğŸ” Compliance: {result.compliance_status}")
print(f"\nğŸ“‹ Decision Rationale:\n{result.decision_rationale}")
```

## Explainable inference structure

```python
@dataclass
class ExplainableInference:
    prediction: str                    # Predicted class
    confidence_score: float            # Confidence (0-1)
    shap_values: Dict[str, float]      # SHAP values per feature
    feature_importance: Dict[str, float]  # Absolute importance
    evidence_chain: List[str]          # Human-readable evidence
    decision_rationale: str            # Full explanation
    timestamp: str                     # ISO 8601 timestamp
    model_version: str                 # Model identifier
    compliance_status: str             # Compliance status
```

## Evidence chain

The evidence chain provides human-readable explanations:

```
Evidence chain:
1. fever=1 (importance: 0.342)
2. location_risk=0.8 (importance: 0.289)
3. cough=1 (importance: 0.156)
4. age=35 (importance: 0.123)
5. fatigue=1 (importance: 0.090)
```

## Decision rationale

Full explanation generated for each high-risk inference:

```
Prediction: cholera_outbreak (confidence: 87.5%)

Key contributing factors:
1. fever=1 (importance: 0.342)
2. location_risk=0.8 (importance: 0.289)
3. cough=1 (importance: 0.156)
4. age=35 (importance: 0.123)
5. fatigue=1 (importance: 0.090)

This explanation satisfies EU AI Act Â§6 (High-Risk AI) and 
GDPR Art. 22 (Right to Explanation).
```

## Compliance logging

All high-risk inferences are automatically logged:

```
ğŸ“Š COMPLIANCE LOG - High-Risk Inference
   Prediction: cholera_outbreak
   Confidence: 87.5%
   Compliance: COMPLIANT_EU_AI_ACT_6
   Evidence Chain: 5 factors
   Timestamp: 2025-12-25T10:00:00Z
```

## Integration with SovereignGuardrail

SHAP explanations integrate with governance enforcement:

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# Validate high-risk inference
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': result.prediction,
        'explanation': result.shap_values,
        'confidence_score': result.confidence_score,
        'evidence_chain': result.evidence_chain
    },
    jurisdiction='EU_AI_ACT'
)
```

## SHAP explainer types

Choose the appropriate SHAP explainer based on your model:

<AccordionGroup>
  <Accordion title="TreeExplainer">
    For tree-based models (XGBoost, LightGBM, Random Forest)
    ```python
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)
    ```
  </Accordion>
  <Accordion title="DeepExplainer">
    For deep learning models (TensorFlow, PyTorch)
    ```python
    explainer = shap.DeepExplainer(model, background_data)
    shap_values = explainer.shap_values(X)
    ```
  </Accordion>
  <Accordion title="KernelExplainer">
    Model-agnostic (works with any model)
    ```python
    explainer = shap.KernelExplainer(model.predict, background_data)
    shap_values = explainer.shap_values(X)
    ```
  </Accordion>
  <Accordion title="LinearExplainer">
    For linear models (Logistic Regression, Linear SVM)
    ```python
    explainer = shap.LinearExplainer(model, background_data)
    shap_values = explainer.shap_values(X)
    ```
  </Accordion>
</AccordionGroup>

## Visualization

Generate SHAP visualizations for clinical review:

```python
import shap
import matplotlib.pyplot as plt

# Force plot (single prediction)
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    features[0],
    matplotlib=True
)
plt.savefig('shap_force_plot.png')

# Summary plot (multiple predictions)
shap.summary_plot(shap_values, features)
plt.savefig('shap_summary_plot.png')

# Waterfall plot (single prediction)
shap.waterfall_plot(shap.Explanation(
    values=shap_values[0],
    base_values=explainer.expected_value,
    data=features[0]
))
plt.savefig('shap_waterfall_plot.png')
```

## Compliance requirements

<Steps>
  <Step title="Mandatory explanation">
    All inferences above high-risk threshold MUST include SHAP analysis
  </Step>
  <Step title="Evidence chain">
    Minimum 3 contributing factors with importance scores
  </Step>
  <Step title="Decision rationale">
    Human-readable explanation of prediction logic
  </Step>
  <Step title="Audit logging">
    All high-risk inferences logged to tamper-proof audit trail
  </Step>
  <Step title="Patient access">
    Patients have right to access explanations (GDPR Art. 15)
  </Step>
</Steps>

## Performance considerations

- **SHAP computation time**: 50-500ms depending on model complexity
- **Caching**: Cache SHAP explainers for repeated predictions
- **Background data**: Use representative sample (100-1000 instances)
- **Batch processing**: Process multiple predictions in parallel

## Testing

Test SHAP integration:

```bash
python tests/test_vertex_ai_shap.py
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="Bio-Interface API"
    icon="mobile"
    href="/integrations/bio-interface"
  >
    Integrate mobile health apps
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="Deploy to Vertex AI"
    icon="google"
    href="/deployment/gcp"
  >
    Deploy models to production
  </Card>
  <Card
    title="Audit trail"
    icon="file-contract"
    href="/governance/audit"
  >
    Configure tamper-proof logging
  </Card>
</CardGroup>
