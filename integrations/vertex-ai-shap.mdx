---
title: Vertex AI + SHAP integration
description: Right to Explanation for high-risk AI inferences with SHAP explainability
---

## Overview

Every high-risk clinical inference in iLuminara-Core requires explainability. This integration combines Vertex AI models with SHAP (SHapley Additive exPlanations) to provide transparent, auditable AI decisions.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  EU AI Act §6, GDPR Art. 22, HIPAA §164.312(b)
</Card>

## Right to Explanation

The EU AI Act §6 and GDPR Art. 22 mandate that high-risk AI systems provide explanations for their decisions. iLuminara automatically generates SHAP explanations for any inference with confidence ≥ 70%.

<Steps>
  <Step title="Prediction">
    Vertex AI model makes clinical prediction
  </Step>
  <Step title="Risk assessment">
    System determines if explanation is required (confidence ≥ 70%)
  </Step>
  <Step title="SHAP analysis">
    Generate feature contributions and evidence chain
  </Step>
  <Step title="Audit logging">
    Store explanation in tamper-proof audit trail
  </Step>
</Steps>

## Basic usage

```python
from integrations.vertex_ai_shap import VertexAIExplainer

# Initialize explainer
explainer = VertexAIExplainer(
    project_id="iluminara-health",
    location="us-central1",
    high_risk_threshold=0.7,
    enable_audit=True
)

# Make prediction with automatic explanation
result = explainer.predict_with_explanation(
    endpoint_id="projects/123/locations/us-central1/endpoints/456",
    instances=[{
        "fever": 1,
        "diarrhea": 1,
        "vomiting": 1,
        "dehydration": 1,
        "location_risk": 0.8,
        "recent_cases": 15,
        "water_quality": 0.3
    }],
    feature_names=[
        "fever", "diarrhea", "vomiting", "dehydration",
        "location_risk", "recent_cases", "water_quality"
    ],
    patient_id="PAT_12345",
    jurisdiction="KDPA_KE"
)

print(f"Prediction: {result['prediction']}")
print(f"Confidence: {result['confidence']:.2%}")
print(f"Risk Level: {result['risk_level']}")

if result['compliance']['requires_explanation']:
    print(f"Rationale: {result['explanation']['decision_rationale']}")
    print(f"Explainability Score: {result['compliance']['explainability_score']}")
```

## Response structure

### High-risk inference (confidence ≥ 70%)

```json
{
  "prediction": {
    "label": "cholera",
    "confidence": 0.92
  },
  "confidence": 0.92,
  "risk_level": "critical",
  "timestamp": "2025-01-15T10:00:00Z",
  "patient_id": "PAT_12345",
  "jurisdiction": "KDPA_KE",
  "explanation": {
    "shap_values": [0.15, 0.22, 0.18, 0.25, 0.08, 0.07, 0.05],
    "feature_contributions": {
      "dehydration": 0.25,
      "diarrhea": 0.22,
      "vomiting": 0.18,
      "fever": 0.15,
      "location_risk": 0.08,
      "recent_cases": 0.07,
      "water_quality": 0.05
    },
    "evidence_chain": [
      {
        "feature": "dehydration",
        "contribution": 0.25,
        "impact": "positive"
      },
      {
        "feature": "diarrhea",
        "contribution": 0.22,
        "impact": "positive"
      },
      {
        "feature": "vomiting",
        "contribution": 0.18,
        "impact": "positive"
      }
    ],
    "decision_rationale": "The model predicts cholera with 92.0% confidence. The primary factor is 'dehydration' (contribution: 0.250). Additional contributing factors: 'diarrhea' (0.220), 'vomiting' (0.180)",
    "base_value": 0.15,
    "method": "SHAP (Kernel Explainer)"
  },
  "compliance": {
    "requires_explanation": true,
    "frameworks": ["EU AI Act §6", "GDPR Art. 22"],
    "explanation_method": "SHAP",
    "explainability_score": 0.875
  }
}
```

### Low-risk inference (confidence < 70%)

```json
{
  "prediction": {
    "label": "malaria",
    "confidence": 0.65
  },
  "confidence": 0.65,
  "risk_level": "medium",
  "timestamp": "2025-01-15T10:00:00Z",
  "patient_id": "PAT_12346",
  "jurisdiction": "KDPA_KE",
  "compliance": {
    "requires_explanation": false,
    "frameworks": [],
    "reason": "Confidence 65.0% below threshold 70.0%"
  }
}
```

## Risk levels

| Level | Confidence Range | Explanation Required | Use Case |
|-------|------------------|---------------------|----------|
| **CRITICAL** | ≥ 90% | ✅ Yes | Life-threatening diagnosis |
| **HIGH** | 70-89% | ✅ Yes | Serious conditions requiring intervention |
| **MEDIUM** | 50-69% | ❌ No | Moderate symptoms, routine monitoring |
| **LOW** | < 50% | ❌ No | Uncertain diagnosis, requires further testing |

## SHAP explanation components

### Feature contributions

Shows how each input feature contributed to the prediction:

```python
feature_contributions = {
    "dehydration": 0.25,      # Strongest positive contributor
    "diarrhea": 0.22,
    "vomiting": 0.18,
    "fever": 0.15,
    "location_risk": 0.08,
    "recent_cases": 0.07,
    "water_quality": 0.05     # Weakest contributor
}
```

### Evidence chain

Top 5 features ranked by absolute contribution:

```python
evidence_chain = [
    {"feature": "dehydration", "contribution": 0.25, "impact": "positive"},
    {"feature": "diarrhea", "contribution": 0.22, "impact": "positive"},
    {"feature": "vomiting", "contribution": 0.18, "impact": "positive"},
    {"feature": "fever", "contribution": 0.15, "impact": "positive"},
    {"feature": "location_risk", "contribution": 0.08, "impact": "positive"}
]
```

### Decision rationale

Human-readable explanation:

> "The model predicts cholera with 92.0% confidence. The primary factor is 'dehydration' (contribution: 0.250). Additional contributing factors: 'diarrhea' (0.220), 'vomiting' (0.180)"

## Explainability score

The system calculates an explainability score (0-1) based on:

- **Completeness** (40%) - All required fields present
- **Evidence quality** (30%) - Sufficient evidence chain depth
- **Diversity** (30%) - Decision not dominated by single feature

```python
explainability_score = (completeness * 0.4) + (evidence_quality * 0.3) + (diversity * 0.3)
```

**Interpretation:**
- **0.8-1.0** - Highly explainable decision
- **0.6-0.8** - Moderately explainable
- **0.4-0.6** - Weakly explainable
- **< 0.4** - Poorly explainable (requires review)

## Batch processing

Process multiple instances efficiently:

```python
instances = [
    {"fever": 1, "diarrhea": 1, "patient_id": "PAT_001"},
    {"fever": 1, "cough": 1, "patient_id": "PAT_002"},
    {"fever": 0, "rash": 1, "patient_id": "PAT_003"}
]

results = explainer.batch_explain(
    endpoint_id="projects/123/locations/us-central1/endpoints/456",
    instances=instances,
    feature_names=["fever", "diarrhea", "cough", "rash"],
    jurisdiction="KDPA_KE"
)

for result in results:
    print(f"Patient {result['patient_id']}: {result['risk_level']}")
```

## Audit trail

All high-risk inferences are automatically logged to BigQuery:

```sql
SELECT
  timestamp,
  patient_id,
  confidence,
  risk_level,
  explainability_score,
  frameworks
FROM `iluminara-health.iluminara_audit.ai_explanations`
WHERE confidence >= 0.7
ORDER BY timestamp DESC
LIMIT 100
```

**Audit record structure:**

| Field | Type | Description |
|-------|------|-------------|
| `timestamp` | TIMESTAMP | When inference was made |
| `patient_id` | STRING | Patient identifier |
| `jurisdiction` | STRING | Legal jurisdiction |
| `confidence` | FLOAT | Model confidence score |
| `risk_level` | STRING | CRITICAL, HIGH, MEDIUM, LOW |
| `requires_explanation` | BOOLEAN | Whether explanation was generated |
| `explanation_method` | STRING | SHAP, LIME, etc. |
| `explainability_score` | FLOAT | Quality of explanation (0-1) |
| `frameworks` | STRING | Applicable legal frameworks |
| `prediction` | JSON | Full prediction object |
| `explanation` | JSON | Full SHAP explanation |

## Compliance enforcement

The explainer integrates with SovereignGuardrail:

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# Validate high-risk inference
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'explanation': result['explanation'],
        'confidence_score': result['confidence'],
        'evidence_chain': result['explanation']['evidence_chain'],
        'consent_token': 'VALID_TOKEN',
        'consent_scope': 'diagnosis'
    },
    jurisdiction='EU_AI_ACT'
)
```

## Deployment

### Local development

```bash
# Install dependencies
pip install shap google-cloud-aiplatform google-cloud-bigquery

# Set environment variables
export GOOGLE_CLOUD_PROJECT=iluminara-health
export GCP_REGION=us-central1

# Run example
python integrations/vertex_ai_shap.py
```

### Production (Cloud Run)

```bash
# Deploy to Cloud Run
gcloud run deploy vertex-ai-explainer \
  --source integrations/ \
  --region us-central1 \
  --allow-unauthenticated \
  --set-env-vars GOOGLE_CLOUD_PROJECT=iluminara-health
```

## Performance considerations

- **SHAP computation time**: ~2-5 seconds per instance
- **Batch processing**: Recommended for >10 instances
- **Caching**: Cache SHAP explainer for repeated predictions
- **Async processing**: Use Cloud Tasks for non-blocking explanations

## Next steps

<CardGroup cols={2}>
  <Card
    title="Bio-Interface API"
    icon="mobile"
    href="/integrations/bio-interface"
  >
    Mobile health app integration
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Compliance enforcement
  </Card>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Autonomous surveillance
  </Card>
  <Card
    title="Audit trail"
    icon="file-contract"
    href="/governance/audit"
  >
    Tamper-proof logging
  </Card>
</CardGroup>
