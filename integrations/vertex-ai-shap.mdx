---
title: Vertex AI + SHAP integration
description: Right to Explanation with explainable AI for high-risk clinical inferences
---

## Overview

iLuminara-Core integrates Vertex AI with SHAP (SHapley Additive exPlanations) to provide transparent, auditable AI decisions. Every high-risk clinical inference automatically includes explainability to comply with the EU AI Act §6 and GDPR Art. 22.

<Card
  title="Philosophy"
  icon="brain-circuit"
>
  "Every high-risk clinical inference requires explainability."
</Card>

## Compliance requirements

<CardGroup cols={2}>
  <Card title="EU AI Act §6" icon="scale-balanced">
    High-risk AI systems must provide explanations
  </Card>
  <Card title="GDPR Art. 22" icon="shield-check">
    Right to explanation for automated decisions
  </Card>
  <Card title="HIPAA §164.524" icon="file-medical">
    Right of access to health information
  </Card>
  <Card title="ISO 27001 A.18.1.4" icon="lock">
    Privacy and protection of PII
  </Card>
</CardGroup>

## Risk levels

iLuminara classifies AI inferences by risk level per EU AI Act:

| Risk Level | Description | Examples | Explanation Required |
|------------|-------------|----------|---------------------|
| **Low** | Minimal risk | Spam filter, symptom search | No |
| **Medium** | Limited risk | Symptom checker, triage | Optional |
| **High** | High risk | Diagnosis, treatment recommendation | **Yes** |
| **Unacceptable** | Prohibited | Social scoring, manipulation | **Blocked** |

## Basic usage

```python
from integrations.vertex_ai_shap import VertexAIExplainer, RiskLevel

# Initialize explainer
explainer = VertexAIExplainer(
    project_id="iluminara-health",
    location="us-central1",
    high_risk_threshold=0.7,
    enable_compliance_check=True
)

# Example: Cholera outbreak prediction
instances = [
    {
        "temperature": 38.5,
        "diarrhea_severity": 8,
        "vomiting": 1,
        "dehydration_level": 7,
        "days_since_onset": 2,
        "population_density": 5000,
        "water_quality_index": 3
    }
]

feature_names = [
    "temperature",
    "diarrhea_severity",
    "vomiting",
    "dehydration_level",
    "days_since_onset",
    "population_density",
    "water_quality_index"
]

# Make prediction with explanation
result = explainer.predict_with_explanation(
    endpoint_id="projects/123/locations/us-central1/endpoints/456",
    instances=instances,
    feature_names=feature_names,
    risk_level=RiskLevel.HIGH,
    jurisdiction="EU_AI_ACT"
)
```

## Response structure

```json
{
  "predictions": [{"score": 0.92, "class": "cholera"}],
  "confidence_scores": [0.92],
  "risk_level": "high",
  "requires_explanation": true,
  "explanation": {
    "method": "SHAP",
    "feature_contributions": [
      {
        "feature": "diarrhea_severity",
        "value": 8.0,
        "shap_value": 0.35,
        "contribution_pct": 42.5
      },
      {
        "feature": "dehydration_level",
        "value": 7.0,
        "shap_value": 0.28,
        "contribution_pct": 34.1
      }
    ],
    "evidence_chain": [
      "diarrhea_severity=8.00 increases risk by 0.350",
      "dehydration_level=7.00 increases risk by 0.280",
      "water_quality_index=3.00 increases risk by 0.120"
    ],
    "decision_rationale": "The model's decision is primarily driven by diarrhea_severity, which has a positive contribution of 0.350. This feature accounts for 42.5% of the total prediction."
  },
  "compliance_status": "APPROVED",
  "timestamp": "2025-12-24T08:00:00.000Z"
}
```

## Explanation components

### SHAP values

SHAP (SHapley Additive exPlanations) values show how much each feature contributes to the prediction:

- **Positive SHAP value** - Feature increases the predicted risk
- **Negative SHAP value** - Feature decreases the predicted risk
- **Magnitude** - Strength of the contribution

### Feature contributions

Ranked list of features by their impact on the prediction:

```python
# Top contributing features
for contrib in result['explanation']['feature_contributions'][:3]:
    print(f"{contrib['feature']}: {contrib['contribution_pct']:.1f}%")

# Output:
# diarrhea_severity: 42.5%
# dehydration_level: 34.1%
# water_quality_index: 14.6%
```

### Evidence chain

Human-readable explanation of the decision:

```python
for evidence in result['explanation']['evidence_chain']:
    print(f"  - {evidence}")

# Output:
#   - diarrhea_severity=8.00 increases risk by 0.350
#   - dehydration_level=7.00 increases risk by 0.280
#   - water_quality_index=3.00 increases risk by 0.120
```

### Decision rationale

Natural language summary of the model's reasoning:

```
The model's decision is primarily driven by diarrhea_severity, 
which has a positive contribution of 0.350. This feature accounts 
for 42.5% of the total prediction.
```

## Compliance validation

Every high-risk inference is automatically validated against the SovereignGuardrail:

```python
# Automatic compliance check
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'actor': 'vertex_ai_model',
        'resource': endpoint_id,
        'explanation': json.dumps(explanation),
        'confidence_score': 0.92,
        'evidence_chain': explanation['evidence_chain'],
        'consent_token': 'VALID_TOKEN',
        'consent_scope': 'diagnosis'
    },
    jurisdiction='EU_AI_ACT'
)
```

**Validation checks:**
- Explanation provided for high-risk inference
- Confidence score within acceptable range
- Evidence chain complete
- Consent token valid
- Jurisdiction compliance

## Batch predictions

Process multiple instances with explanations:

```python
# Batch prediction
results = explainer.batch_predict_with_explanation(
    endpoint_id="projects/123/locations/us-central1/endpoints/456",
    instances=[instance1, instance2, instance3],
    feature_names=feature_names,
    risk_level=RiskLevel.HIGH
)

# Process results
for i, result in enumerate(results):
    print(f"Instance {i+1}:")
    print(f"  Prediction: {result['predictions']}")
    print(f"  Confidence: {result['confidence_scores'][0]:.2%}")
    print(f"  Top feature: {result['explanation']['feature_contributions'][0]['feature']}")
```

## Audit trail

Export explanations to BigQuery for compliance audit:

```python
# Export to BigQuery
explainer.export_explanation_to_bigquery(
    explanation=result['explanation'],
    dataset_id="iluminara_audit",
    table_id="ai_explanations"
)
```

**BigQuery schema:**

| Column | Type | Description |
|--------|------|-------------|
| `timestamp` | TIMESTAMP | When the inference was made |
| `method` | STRING | Explanation method (SHAP, LIME) |
| `feature_contributions` | JSON | Feature importance scores |
| `evidence_chain` | JSON | Human-readable evidence |
| `decision_rationale` | STRING | Natural language explanation |
| `base_value` | FLOAT | Model baseline prediction |

## Integration with Golden Thread

AI predictions automatically become verified signals in the Golden Thread:

```python
from edge_node.sync_protocol.golden_thread import GoldenThread

gt = GoldenThread()

# AI prediction becomes CBS signal
cbs_signal = {
    'location': patient_location,
    'symptom': result['predictions'][0]['class'],
    'timestamp': result['timestamp'],
    'source': 'AI_VERTEX',
    'confidence': result['confidence_scores'][0],
    'explanation': result['explanation']
}

# Fuse with EMR
fused = gt.fuse_data_streams(
    cbs_signal=cbs_signal,
    emr_record=emr_record,
    patient_id=patient_id
)
```

## Model deployment

### Deploy model to Vertex AI

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(project="iluminara-health", location="us-central1")

# Upload model
model = aiplatform.Model.upload(
    display_name="cholera-predictor",
    artifact_uri="gs://iluminara-models/cholera-v1",
    serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-8:latest"
)

# Deploy to endpoint
endpoint = model.deploy(
    machine_type="n1-standard-4",
    min_replica_count=1,
    max_replica_count=10
)

print(f"✅ Model deployed - Endpoint: {endpoint.resource_name}")
```

### Enable explainability

```python
# Deploy with explainability
endpoint = model.deploy(
    machine_type="n1-standard-4",
    explanation_metadata=aiplatform.explain.ExplanationMetadata(
        inputs={
            feature: aiplatform.explain.ExplanationMetadata.InputMetadata(
                input_tensor_name=feature
            )
            for feature in feature_names
        },
        outputs={
            "score": aiplatform.explain.ExplanationMetadata.OutputMetadata(
                output_tensor_name="score"
            )
        }
    ),
    explanation_parameters=aiplatform.explain.ExplanationParameters(
        sampled_shapley_attribution=aiplatform.explain.SampledShapleyAttribution(
            path_count=10
        )
    )
)
```

## Performance considerations

- **SHAP calculation time**: ~2-5 seconds per instance
- **Batch processing**: Recommended for >10 instances
- **Caching**: Cache SHAP explainer for repeated predictions
- **Async processing**: Use async for real-time applications

## Next steps

<CardGroup cols={2}>
  <Card
    title="Bio-Interface API"
    icon="mobile"
    href="/integrations/bio-interface"
  >
    Integrate with mobile health apps
  </Card>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="Golden Thread"
    icon="link"
    href="/architecture/golden-thread"
  >
    Fuse AI predictions with EMR data
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/gcp"
  >
    Deploy to Google Cloud Platform
  </Card>
</CardGroup>
