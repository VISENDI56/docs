---
title: Vertex AI + SHAP integration
description: Right to Explanation for high-risk clinical AI with explainable predictions
---

## Overview

iLuminara-Core integrates Google Cloud Vertex AI with SHAP (SHapley Additive exPlanations) to provide **Right to Explanation** for every high-risk clinical inference, ensuring compliance with EU AI Act §6 and GDPR Art. 22.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk inference automatically triggers SHAP analysis for explainability.
</Card>

## Architecture

```
┌─────────────────────────────────────┐
│         EDGE NODE                   │
│  - Collect patient data             │
│  - Local preprocessing              │
└─────────────────────────────────────┘
              │
              │ Encrypted transfer
              ▼
┌─────────────────────────────────────┐
│         VERTEX AI                   │
│  - AutoML time-series forecasting   │
│  - Custom model training            │
│  - Batch/online prediction          │
└─────────────────────────────────────┘
              │
              │ Prediction + confidence
              ▼
┌─────────────────────────────────────┐
│         SHAP EXPLAINER              │
│  - Feature importance               │
│  - SHAP values calculation          │
│  - Decision rationale               │
└─────────────────────────────────────┘
              │
              │ Explanation + evidence
              ▼
┌─────────────────────────────────────┐
│    SOVEREIGN GUARDRAIL              │
│  - Validate explainability          │
│  - Enforce EU AI Act §6             │
│  - Audit trail logging              │
└─────────────────────────────────────┘
```

## High-risk inference detection

The SovereignGuardrail automatically detects high-risk inferences:

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# High-risk threshold: confidence > 0.7
if prediction_confidence > 0.7:
    # Requires SHAP explanation
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': 'cholera_diagnosis',
            'confidence_score': 0.95,
            'explanation': shap_values,
            'evidence_chain': ['fever', 'diarrhea', 'dehydration'],
            'consent_token': 'VALID_TOKEN',
            'consent_scope': 'diagnosis'
        },
        jurisdiction='EU_AI_ACT'
    )
```

## Vertex AI model training

### Setup Vertex AI

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project='your-project-id',
    location='us-central1',
    staging_bucket='gs://your-bucket'
)

# Create dataset
dataset = aiplatform.TabularDataset.create(
    display_name='cholera_outbreak_prediction',
    gcs_source='gs://your-bucket/training_data.csv'
)

# Train AutoML model
job = aiplatform.AutoMLTabularTrainingJob(
    display_name='cholera_forecasting',
    optimization_prediction_type='regression',
    optimization_objective='minimize-rmse'
)

model = job.run(
    dataset=dataset,
    target_column='case_count',
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
    budget_milli_node_hours=1000,
    model_display_name='cholera_forecaster_v1'
)
```

### Deploy model endpoint

```python
# Deploy to endpoint
endpoint = model.deploy(
    machine_type='n1-standard-4',
    min_replica_count=1,
    max_replica_count=10,
    accelerator_type='NVIDIA_TESLA_T4',
    accelerator_count=1
)

print(f"✅ Model deployed: {endpoint.resource_name}")
```

## SHAP explainability

### Install SHAP

```bash
pip install shap
```

### Generate SHAP explanations

```python
import shap
import numpy as np
from google.cloud import aiplatform

# Load model
endpoint = aiplatform.Endpoint('projects/.../endpoints/...')

# Create SHAP explainer
def predict_fn(X):
    """Wrapper for Vertex AI predictions"""
    instances = [{'features': x.tolist()} for x in X]
    predictions = endpoint.predict(instances=instances)
    return np.array([p['value'] for p in predictions.predictions])

# Initialize SHAP explainer
explainer = shap.KernelExplainer(predict_fn, background_data)

# Generate SHAP values for new prediction
shap_values = explainer.shap_values(patient_features)

# Visualize
shap.force_plot(
    explainer.expected_value,
    shap_values,
    patient_features,
    feature_names=['fever', 'diarrhea', 'vomiting', 'location', 'age']
)
```

### SHAP integration with SovereignGuardrail

```python
from governance_kernel.vector_ledger import SovereignGuardrail
import shap
import json

guardrail = SovereignGuardrail()

# Make prediction
prediction = endpoint.predict(instances=[patient_data])
confidence = prediction.predictions[0]['confidence']

# Generate SHAP explanation
shap_values = explainer.shap_values(patient_features)

# Format explanation
explanation = {
    'method': 'SHAP',
    'base_value': float(explainer.expected_value),
    'shap_values': shap_values.tolist(),
    'feature_names': feature_names,
    'feature_values': patient_features.tolist(),
    'top_features': [
        {'name': feature_names[i], 'contribution': float(shap_values[i])}
        for i in np.argsort(np.abs(shap_values))[-5:]
    ]
}

# Validate with SovereignGuardrail
try:
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'actor': 'vertex_ai_model',
            'resource': 'patient_diagnosis',
            'inference': 'cholera_positive',
            'confidence_score': confidence,
            'explanation': json.dumps(explanation),
            'evidence_chain': ['fever', 'diarrhea', 'dehydration'],
            'consent_token': patient_consent_token,
            'consent_scope': 'diagnosis'
        },
        jurisdiction='EU_AI_ACT'
    )
    print("✅ High-risk inference validated with explanation")
except SovereigntyViolationError as e:
    print(f"❌ Validation failed: {e}")
```

## Feature importance visualization

```python
import matplotlib.pyplot as plt
import shap

# Summary plot (all features)
shap.summary_plot(shap_values, patient_features, feature_names=feature_names)

# Waterfall plot (single prediction)
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values,
        base_values=explainer.expected_value,
        data=patient_features,
        feature_names=feature_names
    )
)

# Force plot (interactive)
shap.force_plot(
    explainer.expected_value,
    shap_values,
    patient_features,
    feature_names=feature_names
)
```

## Batch prediction with explainability

```python
from google.cloud import aiplatform
import shap
import pandas as pd

# Batch prediction job
batch_prediction_job = aiplatform.BatchPredictionJob.create(
    job_display_name='cholera_batch_forecast',
    model_name=model.resource_name,
    instances_format='csv',
    gcs_source='gs://your-bucket/batch_input.csv',
    gcs_destination_prefix='gs://your-bucket/batch_output/',
    machine_type='n1-standard-4'
)

# Wait for completion
batch_prediction_job.wait()

# Load results
results = pd.read_csv('gs://your-bucket/batch_output/predictions.csv')

# Generate SHAP explanations for high-risk predictions
high_risk = results[results['confidence'] > 0.7]

for idx, row in high_risk.iterrows():
    patient_features = row[feature_columns].values
    shap_values = explainer.shap_values(patient_features)
    
    # Validate with SovereignGuardrail
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'inference': row['prediction'],
            'confidence_score': row['confidence'],
            'explanation': json.dumps({
                'shap_values': shap_values.tolist(),
                'feature_names': feature_columns
            }),
            'evidence_chain': extract_evidence(row),
            'consent_token': row['consent_token'],
            'consent_scope': 'diagnosis'
        },
        jurisdiction='EU_AI_ACT'
    )
```

## Model monitoring

### Track explainability metrics

```python
from google.cloud import aiplatform

# Create model monitoring job
monitoring_job = aiplatform.ModelDeploymentMonitoringJob.create(
    display_name='cholera_model_monitoring',
    endpoint=endpoint.resource_name,
    logging_sampling_strategy=aiplatform.gapic.SamplingStrategy(
        random_sample_config=aiplatform.gapic.SamplingStrategy.RandomSampleConfig(
            sample_rate=0.5
        )
    ),
    model_monitoring_alert_config=aiplatform.gapic.ModelMonitoringAlertConfig(
        email_alert_config=aiplatform.gapic.ModelMonitoringAlertConfig.EmailAlertConfig(
            user_emails=['alerts@iluminara.health']
        )
    )
)

# Monitor prediction drift
monitoring_job.update(
    skew_thresholds={'fever': 0.1, 'diarrhea': 0.1},
    drift_thresholds={'fever': 0.2, 'diarrhea': 0.2}
)
```

## Compliance requirements

### EU AI Act §6 (High-Risk AI)

<AccordionGroup>
  <Accordion title="Article 6.1 - High-risk classification">
    AI systems used for medical diagnosis are classified as high-risk and require explainability.
  </Accordion>
  <Accordion title="Article 8 - Transparency">
    High-risk AI systems must provide clear information about their functioning and decision-making.
  </Accordion>
  <Accordion title="Article 12 - Record keeping">
    All high-risk inferences must be logged with explanations for audit purposes.
  </Accordion>
</AccordionGroup>

### GDPR Art. 22 (Right to Explanation)

```python
# Every automated decision requires explanation
if is_automated_decision(prediction):
    explanation = generate_shap_explanation(prediction)
    
    # Store explanation with audit trail
    guardrail.validate_action(
        action_type='High_Risk_Inference',
        payload={
            'explanation': explanation,
            'right_to_explanation': True,
            'gdpr_article': 'Art. 22'
        },
        jurisdiction='GDPR_EU'
    )
```

## Example: Cholera outbreak prediction

```python
import shap
from google.cloud import aiplatform
from governance_kernel.vector_ledger import SovereignGuardrail

# Initialize
endpoint = aiplatform.Endpoint('projects/.../endpoints/cholera_forecaster')
guardrail = SovereignGuardrail()

# Patient data
patient_data = {
    'fever': 1,
    'diarrhea': 1,
    'vomiting': 1,
    'dehydration': 1,
    'location_lat': 0.0512,
    'location_lng': 40.3129,
    'age': 35,
    'recent_travel': 0
}

# Predict
prediction = endpoint.predict(instances=[patient_data])
diagnosis = prediction.predictions[0]['diagnosis']
confidence = prediction.predictions[0]['confidence']

# Generate SHAP explanation
patient_features = np.array(list(patient_data.values()))
shap_values = explainer.shap_values(patient_features)

# Validate with SovereignGuardrail
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': diagnosis,
        'confidence_score': confidence,
        'explanation': json.dumps({
            'method': 'SHAP',
            'shap_values': shap_values.tolist(),
            'feature_names': list(patient_data.keys()),
            'top_contributors': [
                {'feature': 'diarrhea', 'contribution': 0.45},
                {'feature': 'vomiting', 'contribution': 0.32},
                {'feature': 'dehydration', 'contribution': 0.18}
            ]
        }),
        'evidence_chain': ['fever', 'diarrhea', 'vomiting', 'dehydration'],
        'consent_token': 'VALID_TOKEN',
        'consent_scope': 'diagnosis'
    },
    jurisdiction='EU_AI_ACT'
)

print(f"✅ Diagnosis: {diagnosis} (confidence: {confidence:.2%})")
print(f"✅ Explanation validated and logged")
```

## Performance considerations

- **SHAP computation**: ~100-500ms per prediction (depends on model complexity)
- **Vertex AI latency**: ~50-200ms per prediction
- **Total latency**: ~150-700ms for explained prediction
- **Batch processing**: Use batch predictions for non-urgent forecasting

## Next steps

<CardGroup cols={2}>
  <Card
    title="Governance kernel"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure SovereignGuardrail validation
  </Card>
  <Card
    title="AI agents"
    icon="brain-circuit"
    href="/ai-agents/overview"
  >
    Integrate with autonomous agents
  </Card>
  <Card
    title="Deployment"
    icon="rocket"
    href="/deployment/gcp"
  >
    Deploy Vertex AI models to production
  </Card>
  <Card
    title="Audit trail"
    icon="file-contract"
    href="/governance/audit"
  >
    Configure tamper-proof logging
  </Card>
</CardGroup>
