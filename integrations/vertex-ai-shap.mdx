---
title: Vertex AI + SHAP integration
description: Right to Explanation for high-risk clinical AI with explainable predictions
---

## Overview

iLuminara-Core integrates **Vertex AI** for model training and inference with **SHAP (SHapley Additive exPlanations)** to provide the "Right to Explanation" required by EU AI Act §6 and GDPR Art. 22.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk clinical inference automatically triggers SHAP analysis to comply with EU AI Act §6 and GDPR Art. 22.
</Card>

## Architecture

```
┌─────────────────────────────────────┐
│         VERTEX AI                   │
│  ┌──────────────────────────────┐  │
│  │  AutoML Time-Series          │  │
│  │  - Outbreak forecasting      │  │
│  │  - 72-hour predictions       │  │
│  └──────────────────────────────┘  │
│  ┌──────────────────────────────┐  │
│  │  Custom Models               │  │
│  │  - Disease classification    │  │
│  │  - Risk scoring              │  │
│  └──────────────────────────────┘  │
└─────────────────────────────────────┘
              │
              │ Inference
              ▼
┌─────────────────────────────────────┐
│         SHAP EXPLAINER              │
│  ┌──────────────────────────────┐  │
│  │  TreeExplainer               │  │
│  │  - Feature importance        │  │
│  │  - SHAP values               │  │
│  └──────────────────────────────┘  │
│  ┌──────────────────────────────┐  │
│  │  Explanation Validator       │  │
│  │  - Confidence threshold      │  │
│  │  - Evidence chain            │  │
│  └──────────────────────────────┘  │
└─────────────────────────────────────┘
              │
              │ Validation
              ▼
┌─────────────────────────────────────┐
│    SOVEREIGN GUARDRAIL              │
│  - EU AI Act §6 compliance          │
│  - GDPR Art. 22 compliance          │
│  - Audit trail                      │
└─────────────────────────────────────┘
```

## High-risk AI systems

According to EU AI Act §6, these systems require explainability:

<CardGroup cols={2}>
  <Card title="Clinical diagnosis" icon="stethoscope">
    Disease classification, risk scoring, treatment recommendations
  </Card>
  <Card title="Outbreak prediction" icon="chart-line">
    Epidemic forecasting, hotspot detection, resource allocation
  </Card>
  <Card title="Patient triage" icon="hospital">
    Severity assessment, priority scoring, bed allocation
  </Card>
  <Card title="Resource allocation" icon="boxes-stacked">
    Supply distribution, staff deployment, facility planning
  </Card>
</CardGroup>

## Implementation

### Basic usage

```python
from integrations.vertex_ai_shap import VertexAIExplainer
from governance_kernel.vector_ledger import SovereignGuardrail

# Initialize explainer
explainer = VertexAIExplainer(
    project_id="iluminara-health",
    region="africa-south1",
    model_name="cholera-risk-classifier"
)

# Make prediction with explanation
result = explainer.predict_with_explanation(
    features={
        "fever": 1,
        "diarrhea": 1,
        "vomiting": 1,
        "dehydration": 1,
        "location_risk": 0.8,
        "recent_cases": 15
    },
    patient_id="PAT_12345"
)

# Result includes:
# - prediction: 0.92 (92% cholera risk)
# - shap_values: [0.3, 0.25, 0.2, 0.15, 0.05, 0.02]
# - feature_importance: {"fever": 0.3, "diarrhea": 0.25, ...}
# - explanation: "High risk due to fever (30%), diarrhea (25%), vomiting (20%)"
# - confidence_score: 0.92
# - evidence_chain: ["fever", "diarrhea", "vomiting"]

# Validate with SovereignGuardrail
guardrail = SovereignGuardrail()
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'explanation': result['explanation'],
        'confidence_score': result['confidence_score'],
        'evidence_chain': result['evidence_chain'],
        'shap_values': result['shap_values']
    },
    jurisdiction='EU_AI_ACT'
)
```

### Training with Vertex AI

```python
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(
    project="iluminara-health",
    location="africa-south1"
)

# Create AutoML time-series dataset
dataset = aiplatform.TimeSeriesDataset.create(
    display_name="cholera-outbreak-forecast",
    gcs_source="gs://iluminara-data/training/cholera_cases.csv",
    time_column="timestamp",
    time_series_identifier_column="location",
    target_column="case_count"
)

# Train AutoML model
job = aiplatform.AutoMLForecastingTrainingJob(
    display_name="cholera-forecast-model",
    optimization_objective="minimize-rmse",
    column_specs={
        "case_count": "numeric",
        "temperature": "numeric",
        "rainfall": "numeric",
        "population_density": "numeric"
    }
)

model = job.run(
    dataset=dataset,
    target_column="case_count",
    time_column="timestamp",
    time_series_identifier_column="location",
    forecast_horizon=72,  # 72-hour forecast
    data_granularity_unit="hour",
    data_granularity_count=1,
    budget_milli_node_hours=1000
)

print(f"✅ Model trained: {model.resource_name}")
```

### SHAP explanation

```python
import shap
import numpy as np

# Load trained model
model = aiplatform.Model("projects/.../models/...")

# Create SHAP explainer
explainer = shap.TreeExplainer(model)

# Generate SHAP values
shap_values = explainer.shap_values(features)

# Visualize
shap.summary_plot(shap_values, features)
shap.force_plot(explainer.expected_value, shap_values[0], features.iloc[0])
```

## Explanation validator

The `ExplanationValidator` ensures all high-risk inferences meet compliance requirements:

```python
from integrations.vertex_ai_shap import ExplanationValidator

validator = ExplanationValidator(
    confidence_threshold=0.7,  # High-risk threshold
    min_evidence_chain_length=3,
    require_shap_values=True
)

# Validate explanation
is_valid, violations = validator.validate(
    prediction=0.92,
    explanation="High risk due to fever (30%), diarrhea (25%), vomiting (20%)",
    confidence_score=0.92,
    evidence_chain=["fever", "diarrhea", "vomiting"],
    shap_values=[0.3, 0.25, 0.2, 0.15, 0.05, 0.02]
)

if not is_valid:
    print(f"❌ Explanation invalid: {violations}")
    # Block inference - EU AI Act §6 violation
```

## Compliance enforcement

### EU AI Act §6

High-risk AI systems must provide:

<Steps>
  <Step title="Transparency">
    Users must be informed when interacting with AI
  </Step>
  <Step title="Explainability">
    Decisions must be explainable in human-understandable terms
  </Step>
  <Step title="Human oversight">
    High-risk decisions require human review
  </Step>
  <Step title="Accuracy">
    Systems must meet minimum accuracy thresholds
  </Step>
</Steps>

### GDPR Art. 22

Automated decision-making requires:

- Right to explanation
- Right to human intervention
- Right to contest the decision
- Right to obtain human review

## Integration with API

The API automatically provides explanations for high-risk inferences:

```bash
curl -X POST http://localhost:8080/predict \
  -H "Content-Type: application/json" \
  -d '{
    "location": {"lat": 0.4221, "lng": 40.2255},
    "symptoms": ["fever", "diarrhea", "vomiting"],
    "patient_id": "PAT_12345"
  }'
```

Response includes explanation:

```json
{
  "status": "success",
  "prediction": {
    "disease": "cholera",
    "confidence": 0.92,
    "risk_level": "HIGH"
  },
  "explanation": {
    "summary": "High risk due to fever (30%), diarrhea (25%), vomiting (20%)",
    "shap_values": [0.3, 0.25, 0.2, 0.15, 0.05, 0.02],
    "feature_importance": {
      "fever": 0.3,
      "diarrhea": 0.25,
      "vomiting": 0.2,
      "dehydration": 0.15,
      "location_risk": 0.05,
      "recent_cases": 0.02
    },
    "evidence_chain": ["fever", "diarrhea", "vomiting"],
    "confidence_score": 0.92
  },
  "compliance": {
    "framework": "EU_AI_ACT",
    "article": "§6 (High-Risk AI)",
    "explanation_provided": true,
    "human_review_required": true
  }
}
```

## Model registry

Track all models with metadata:

```python
from google.cloud import aiplatform

# Register model
model = aiplatform.Model.upload(
    display_name="cholera-risk-classifier-v2",
    artifact_uri="gs://iluminara-models/cholera-v2",
    serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-8:latest",
    labels={
        "framework": "tensorflow",
        "version": "2.0",
        "compliance": "eu-ai-act",
        "explainability": "shap",
        "jurisdiction": "KDPA_KE"
    }
)

# Add metadata
model.update(
    description="Cholera risk classifier with SHAP explainability",
    metadata={
        "training_date": "2025-01-15",
        "accuracy": 0.94,
        "precision": 0.92,
        "recall": 0.91,
        "f1_score": 0.915,
        "explainability_method": "SHAP TreeExplainer",
        "compliance_frameworks": ["EU_AI_ACT", "GDPR", "KDPA"]
    }
)
```

## Monitoring

Monitor model performance and explanation quality:

```python
from google.cloud import aiplatform

# Create model monitoring job
monitoring_job = aiplatform.ModelDeploymentMonitoringJob.create(
    display_name="cholera-model-monitoring",
    endpoint=endpoint,
    logging_sampling_strategy=aiplatform.gapic.SamplingStrategy(
        random_sample_config=aiplatform.gapic.SamplingStrategy.RandomSampleConfig(
            sample_rate=0.1
        )
    ),
    model_monitoring_alert_config=aiplatform.gapic.ModelMonitoringAlertConfig(
        email_alert_config=aiplatform.gapic.ModelMonitoringAlertConfig.EmailAlertConfig(
            user_emails=["compliance@iluminara.health"]
        )
    )
)
```

## Testing

Test explanation quality:

```python
import pytest
from integrations.vertex_ai_shap import VertexAIExplainer

def test_explanation_quality():
    explainer = VertexAIExplainer(
        project_id="iluminara-test",
        region="africa-south1",
        model_name="test-model"
    )
    
    result = explainer.predict_with_explanation(
        features={"fever": 1, "diarrhea": 1},
        patient_id="TEST_001"
    )
    
    # Verify explanation exists
    assert "explanation" in result
    assert "shap_values" in result
    assert "confidence_score" in result
    
    # Verify SHAP values sum to prediction
    assert abs(sum(result["shap_values"]) - result["prediction"]) < 0.01
    
    # Verify compliance
    assert result["confidence_score"] >= 0.7  # High-risk threshold
```

## Next steps

<CardGroup cols={2}>
  <Card
    title="Deploy model"
    icon="rocket"
    href="/deployment/gcp"
  >
    Deploy Vertex AI models to production
  </Card>
  <Card
    title="Governance"
    icon="shield-check"
    href="/governance/overview"
  >
    Configure compliance enforcement
  </Card>
  <Card
    title="API integration"
    icon="plug"
    href="/api-reference/overview"
  >
    Integrate with REST API
  </Card>
  <Card
    title="Monitoring"
    icon="chart-line"
    href="/deployment/monitoring"
  >
    Set up model monitoring
  </Card>
</CardGroup>
