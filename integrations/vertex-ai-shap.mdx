---
title: Vertex AI + SHAP integration
description: Explainable AI with SHAP analysis for regulatory compliance
---

## Overview

The Vertex AI + SHAP integration provides **Right to Explanation** capabilities required by EU AI Act §6, GDPR Art. 22, and other global AI regulations. Every high-risk inference generates SHAP (SHapley Additive exPlanations) values for complete transparency.

<Card
  title="Philosophy"
  icon="lightbulb"
>
  "Every AI decision affecting human welfare must be explainable, auditable, and aligned with humanitarian principles."
</Card>

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    VERTEX AI + SHAP LAYER                    │
└─────────────────────────────────────────────────────────────┘
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
   ┌────▼────┐      ┌──────▼──────┐    ┌──────▼──────┐
   │ VERTEX  │      │    SHAP     │    │  GOVERNANCE │
   │   AI    │      │  EXPLAINER  │    │   KERNEL    │
   │ (Model) │      │ (Analysis)  │    │ (Validator) │
   └────┬────┘      └──────┬──────┘    └──────┬──────┘
        │                  │                  │
        └──────────────────┼──────────────────┘
                     ▼
          ┌────────────────────────┐
          │   AUDIT TRAIL          │
          │  (Tamper-proof Log)    │
          └────────────────────────┘
```

## Key features

<CardGroup cols={2}>\n  <Card title=\"SHAP analysis\" icon=\"chart-line\">\n    Feature importance attribution for every prediction\n  </Card>\n  <Card title=\"Regulatory compliance\" icon=\"scale-balanced\">\n    EU AI Act, GDPR Art. 22, NIST AI RMF compliance\n  </Card>\n  <Card title=\"Audit trail\" icon=\"file-contract\">\n    Tamper-proof logging of all explanations\n  </Card>\n  <Card title=\"Real-time\" icon=\"bolt\">\n    <5 second latency for explanation generation\n  </Card>\n</CardGroup>

## Setup

### Prerequisites

```bash\n# Install dependencies\npip install google-cloud-aiplatform shap numpy pandas\n\n# Set environment variables\nexport GOOGLE_CLOUD_PROJECT=\"your-project-id\"\nexport GCP_REGION=\"us-central1\"\n```

### Initialize Vertex AI

```python\nfrom google.cloud import aiplatform\n\naiplatform.init(\n    project=\"your-project-id\",\n    location=\"us-central1\"\n)\n```

## Basic usage

### Train a model with explainability

```python\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform import explain\n\n# Define explanation metadata\nexplanation_metadata = explain.ExplanationMetadata(\n    inputs={\n        \"cbs_signals\": explain.ExplanationMetadata.InputMetadata(\n            input_tensor_name=\"cbs_signals\"\n        ),\n        \"z_score\": explain.ExplanationMetadata.InputMetadata(\n            input_tensor_name=\"z_score\"\n        ),\n        \"location\": explain.ExplanationMetadata.InputMetadata(\n            input_tensor_name=\"location\"\n        )\n    },\n    outputs={\n        \"outbreak_probability\": explain.ExplanationMetadata.OutputMetadata(\n            output_tensor_name=\"outbreak_probability\"\n        )\n    }\n)\n\n# Define explanation parameters\nexplanation_parameters = explain.ExplanationParameters(\n    sampled_shapley_attribution=explain.SampledShapleyAttribution(\n        path_count=10\n    )\n)\n\n# Upload model with explainability\nmodel = aiplatform.Model.upload(\n    display_name=\"outbreak-predictor-v1\",\n    artifact_uri=\"gs://your-bucket/model\",\n    serving_container_image_uri=\"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-8:latest\",\n    explanation_metadata=explanation_metadata,\n    explanation_parameters=explanation_parameters\n)\n```

### Generate predictions with explanations

```python\nfrom governance_kernel.humanitarian_constraints import VertexAIExplainableAI\n\nexplainer = VertexAIExplainableAI(project_id=\"your-project-id\")\n\n# Make prediction with explanation\nexplanation = explainer.explain_prediction(\n    model_id=\"outbreak-predictor-v1\",\n    input_data={\n        \"cbs_signals\": 45,\n        \"z_score\": 3.8,\n        \"location\": \"Dadaab\"\n    },\n    prediction=\"OUTBREAK_LIKELY\",\n    feature_names=[\"cbs_signals\", \"z_score\", \"location\"]\n)\n\n# Get SHAP values\nprint(f\"SHAP Values: {explanation.shap_values}\")\nprint(f\"Base Value: {explanation.base_value}\")\nprint(f\"Prediction: {explanation.prediction}\")\n\n# Get top contributors\ntop_factors = explanation.get_top_contributors(n=3)\nfor factor in top_factors:\n    print(f\"{factor['feature']}: {factor['contribution']:.4f}\")\n```

## Integration with SovereignGuardrail\n\nAll high-risk inferences are automatically validated:\n\n```python\nfrom governance_kernel.vector_ledger import SovereignGuardrail\nfrom governance_kernel.humanitarian_constraints import VertexAIExplainableAI\n\nguardrail = SovereignGuardrail()\nexplainer = VertexAIExplainableAI()\n\n# Generate explanation\nexplanation = explainer.explain_prediction(\n    model_id=\"risk-model\",\n    input_data={\"feature1\": 10, \"feature2\": 20},\n    prediction=\"HIGH_RISK\",\n    feature_names=[\"feature1\", \"feature2\"]\n)\n\n# Validate with SovereignGuardrail\nguardrail.validate_action(\n    action_type=\"High_Risk_Inference\",\n    payload={\n        \"explanation\": explanation.to_dict(),\n        \"confidence_score\": 0.87,\n        \"evidence_chain\": explanation.get_top_contributors(n=5),\n        \"consent_token\": \"TOKEN-123\",\n        \"consent_scope\": \"diagnosis\"\n    },\n    jurisdiction=\"EU_AI_ACT\"\n)\n```

## SHAP analysis types

### Sampled Shapley Attribution

Most accurate but computationally expensive. Uses Monte Carlo sampling.

```python\nexplanation_parameters = explain.ExplanationParameters(\n    sampled_shapley_attribution=explain.SampledShapleyAttribution(\n        path_count=50  # Higher = more accurate, slower\n    )\n)\n```

### Integrated Gradients

Fast approximation for neural networks.

```python\nexplanation_parameters = explain.ExplanationParameters(\n    integrated_gradients_attribution=explain.IntegratedGradientsAttribution(\n        step_count=50\n    )\n)\n```

### XRAI (eXplanation with Ranked Area Integrals)

For image models.

```python\nexplanation_parameters = explain.ExplanationParameters(\n    xrai_attribution=explain.XraiAttribution(\n        step_count=50\n    )\n)\n```

## Compliance validation

### EU AI Act §6 (High-Risk AI)

```python\n# Check if model is high-risk\nif explanation.confidence_score > 0.7:\n    # High-risk inference - requires explanation\n    assert explanation.shap_values is not None, \"SHAP values required\"\n    assert len(explanation.get_top_contributors()) >= 3, \"Minimum 3 contributors\"\n    \n    # Log to audit trail\n    guardrail.log_audit(\n        event=\"HIGH_RISK_INFERENCE\",\n        explanation=explanation.to_dict(),\n        framework=\"EU_AI_ACT_6\"\n    )\n```

### GDPR Art. 22 (Right to Explanation)

```python\n# User requests explanation\nuser_explanation = {\n    \"decision\": explanation.prediction,\n    \"reasoning\": explanation.get_human_readable_explanation(),\n    \"top_factors\": explanation.get_top_contributors(n=5),\n    \"confidence\": explanation.confidence_score,\n    \"timestamp\": explanation.timestamp\n}\n\n# Provide to user\nreturn user_explanation\n```

### NIST AI RMF (Measure Function)

```python\n# Measure model performance\nmetrics = {\n    \"accuracy\": 0.92,\n    \"precision\": 0.89,\n    \"recall\": 0.94,\n    \"f1_score\": 0.91,\n    \"explainability_coverage\": 1.0,  # 100% of predictions explained\n    \"average_shap_computation_time\": 0.45  # seconds\n}\n\n# Validate against thresholds\nassert metrics[\"explainability_coverage\"] == 1.0, \"All predictions must be explainable\"\n```

## Visualization

### Feature importance plot

```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Get SHAP values\nshap_values = explanation.shap_values\nfeature_names = explanation.feature_names\n\n# Create bar plot\nplt.figure(figsize=(10, 6))\nplt.barh(feature_names, shap_values)\nplt.xlabel(\"SHAP Value (Impact on Prediction)\")\nplt.title(\"Feature Importance\")\nplt.tight_layout()\nplt.savefig(\"shap_importance.png\")\n```

### Waterfall plot

```python\nimport shap\n\n# Create waterfall plot\nshap.plots.waterfall(\n    shap.Explanation(\n        values=explanation.shap_values,\n        base_values=explanation.base_value,\n        data=explanation.input_data,\n        feature_names=explanation.feature_names\n    )\n)\n```

## Performance optimization

### Caching explanations

```python\nfrom functools import lru_cache\nimport hashlib\n\n@lru_cache(maxsize=1000)\ndef get_cached_explanation(input_hash: str):\n    # Retrieve from cache\n    return explainer.get_explanation(input_hash)\n\n# Generate hash of input\ninput_hash = hashlib.sha256(\n    str(input_data).encode()\n).hexdigest()\n\n# Try cache first\nexplanation = get_cached_explanation(input_hash)\n```

### Batch explanations

```python\n# Explain multiple predictions at once\nbatch_explanations = explainer.explain_batch(\n    model_id=\"outbreak-predictor-v1\",\n    inputs=[\n        {\"cbs_signals\": 45, \"z_score\": 3.8},\n        {\"cbs_signals\": 12, \"z_score\": 1.2},\n        {\"cbs_signals\": 78, \"z_score\": 5.1}\n    ],\n    feature_names=[\"cbs_signals\", \"z_score\"]\n)\n```

## Audit trail

All explanations are logged to the tamper-proof audit trail:

```python\n# Retrieve explanation history\nhistory = explainer.get_explanation_history(\n    model_id=\"outbreak-predictor-v1\",\n    start_date=\"2025-01-01\",\n    end_date=\"2025-01-31\"\n)\n\nfor entry in history:\n    print(f\"Timestamp: {entry.timestamp}\")\n    print(f\"Prediction: {entry.prediction}\")\n    print(f\"Top Factor: {entry.get_top_contributors(n=1)[0]}\")\n    print(f\"Confidence: {entry.confidence_score}\")\n    print(\"---\")\n```

## Testing

```python\nimport pytest\nfrom governance_kernel.humanitarian_constraints import VertexAIExplainableAI\n\ndef test_shap_explanation():\n    explainer = VertexAIExplainableAI()\n    \n    explanation = explainer.explain_prediction(\n        model_id=\"test-model\",\n        input_data={\"feature1\": 10},\n        prediction=\"POSITIVE\",\n        feature_names=[\"feature1\"]\n    )\n    \n    assert explanation.shap_values is not None\n    assert len(explanation.shap_values) > 0\n    assert explanation.confidence_score >= 0 and explanation.confidence_score <= 1\n\ndef test_compliance_validation():\n    explainer = VertexAIExplainableAI()\n    guardrail = SovereignGuardrail()\n    \n    explanation = explainer.explain_prediction(\n        model_id=\"test-model\",\n        input_data={\"feature1\": 10},\n        prediction=\"HIGH_RISK\",\n        feature_names=[\"feature1\"]\n    )\n    \n    # Should not raise exception\n    guardrail.validate_action(\n        action_type=\"High_Risk_Inference\",\n        payload={\n            \"explanation\": explanation.to_dict(),\n            \"confidence_score\": 0.9,\n            \"evidence_chain\": explanation.get_top_contributors(),\n            \"consent_token\": \"VALID\"\n        },\n        jurisdiction=\"EU_AI_ACT\"\n    )\n```

## Next steps

<CardGroup cols={2}>\n  <Card\n    title=\"Governance kernel\"\n    icon=\"shield-check\"\n    href=\"/governance/overview\"\n  >\n    Understand compliance enforcement\n  </Card>\n  <Card\n    title=\"Humanitarian constraints\"\n    icon=\"heart\"\n    href=\"/governance/humanitarian\"\n  >\n    Apply humanitarian protocols\n  </Card>\n  <Card\n    title=\"AI agents\"\n    icon=\"brain-circuit\"\n    href=\"/ai-agents/overview\"\n  >\n    Deploy autonomous surveillance\n  </Card>\n  <Card\n    title=\"Bio-Interface API\"\n    icon=\"dna\"\n    href=\"/integrations/bio-interface\"\n  >\n    Mobile health app integration\n  </Card>\n</CardGroup>\n