---
title: Vertex AI + SHAP integration
description: Right to Explanation for high-risk clinical AI decisions
---

## Overview

iLuminara integrates Google Cloud Vertex AI with SHAP (SHapley Additive exPlanations) to provide explainability for every high-risk clinical inference, ensuring compliance with EU AI Act Â§6 and GDPR Art. 22.

<Card
  title="Compliance"
  icon="scale-balanced"
>
  Every high-risk AI decision requires explainability with SHAP values, feature importance, and evidence chains.
</Card>

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VERTEX AI MODEL                       â”‚
â”‚  - AutoML Time-Series Forecasting                       â”‚
â”‚  - Custom Training (TensorFlow, PyTorch)                â”‚
â”‚  - Batch Prediction                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚ Prediction
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SHAP EXPLAINABILITY ENGINE                  â”‚
â”‚  - TreeExplainer (for tree-based models)                â”‚
â”‚  - DeepExplainer (for neural networks)                  â”‚
â”‚  - KernelExplainer (model-agnostic)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚ Explanation
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SOVEREIGNGUARDRAIL VALIDATION               â”‚
â”‚  - Confidence threshold check (>0.7)                    â”‚
â”‚  - Evidence chain validation                            â”‚
â”‚  - Audit trail logging                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## High-risk inference validation

### What qualifies as high-risk?

According to EU AI Act Â§6, high-risk AI systems include:
- Clinical diagnosis
- Treatment recommendations
- Resource allocation decisions
- Outbreak predictions with public health impact

### Validation requirements

```python
from governance_kernel.vector_ledger import SovereignGuardrail

guardrail = SovereignGuardrail()

# Validate high-risk inference
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'cholera_diagnosis',
        'explanation': shap_values,
        'confidence_score': 0.92,
        'evidence_chain': ['fever', 'diarrhea', 'dehydration', 'positive_test'],
        'consent_token': 'VALID_CONSENT_TOKEN',
        'consent_scope': 'diagnosis'
    },
    jurisdiction='EU_AI_ACT'
)
```

## SHAP integration

### Install dependencies

```bash
pip install shap google-cloud-aiplatform
```

### Basic usage

```python
import shap
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(project='your-project-id', location='us-central1')

# Load model
model = aiplatform.Model('projects/123/locations/us-central1/models/456')

# Get predictions
predictions = model.predict(instances=[patient_data])

# Generate SHAP explanations
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(patient_data)

# Visualize
shap.summary_plot(shap_values, patient_data)
```

### Integration with iLuminara

```python
from edge_node.ai_agents import EpidemiologicalForecastingAgent
import shap

# Initialize agent
agent = EpidemiologicalForecastingAgent(
    location=\"Nairobi\",
    population_size=100000
)

# Make forecast
forecast = agent.forecast_outbreak(
    disease=\"cholera\",
    historical_data=historical_cases,
    forecast_horizon_days=14
)

# Generate SHAP explanation
explainer = shap.TreeExplainer(agent.model)
shap_values = explainer.shap_values(forecast.input_features)

# Attach explanation to forecast
forecast.explanation = {
    'shap_values': shap_values.tolist(),
    'feature_importance': dict(zip(
        forecast.feature_names,
        abs(shap_values).mean(axis=0)
    )),
    'confidence_score': forecast.confidence,
    'evidence_chain': forecast.evidence
}

# Validate with SovereignGuardrail
guardrail.validate_action(
    action_type='High_Risk_Inference',
    payload={
        'inference': 'outbreak_forecast',
        'explanation': forecast.explanation,
        'confidence_score': forecast.confidence,
        'evidence_chain': forecast.evidence
    },
    jurisdiction='EU_AI_ACT'
)
```

## Vertex AI model deployment

### Train custom model

```python
from google.cloud import aiplatform

# Initialize
aiplatform.init(project='your-project-id', location='us-central1')

# Create training job
job = aiplatform.CustomTrainingJob(
    display_name='cholera-forecaster',
    script_path='train.py',
    container_uri='gcr.io/cloud-aiplatform/training/tf-cpu.2-11:latest',
    requirements=['shap', 'pandas', 'scikit-learn'],
    model_serving_container_image_uri='gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-11:latest'
)

# Run training
model = job.run(
    dataset=dataset,
    model_display_name='cholera-forecaster-v1',
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1
)
```

### Deploy model

```python
# Deploy to endpoint
endpoint = model.deploy(
    machine_type='n1-standard-4',
    min_replica_count=1,
    max_replica_count=10,
    accelerator_type='NVIDIA_TESLA_T4',
    accelerator_count=1
)

# Get predictions with explanations
predictions = endpoint.predict(
    instances=[patient_data],
    parameters={'explain': True}
)
```

## Explainability dashboard

### Streamlit integration

```python
import streamlit as st
import shap

st.title(\"ğŸ” AI Explainability Dashboard\")

# Load model and data
model = load_model()
patient_data = load_patient_data()

# Generate SHAP values
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(patient_data)

# Display waterfall plot
st.subheader(\"Feature Contributions\")
shap.waterfall_plot(shap.Explanation(
    values=shap_values[0],
    base_values=explainer.expected_value,
    data=patient_data.iloc[0],
    feature_names=patient_data.columns.tolist()
))

# Display feature importance
st.subheader(\"Feature Importance\")
shap.summary_plot(shap_values, patient_data, plot_type=\"bar\")

# Display decision path
st.subheader(\"Decision Path\")
shap.decision_plot(
    explainer.expected_value,
    shap_values[0],
    patient_data.iloc[0]
)
```

## Compliance validation

### EU AI Act Â§6 requirements

<Steps>
  <Step title=\"Risk assessment\">
    Classify AI system as high-risk based on use case
  </Step>
  <Step title=\"Explainability\">
    Generate SHAP values for every prediction
  </Step>
  <Step title=\"Documentation\">
    Log explanation in tamper-proof audit trail
  </Step>
  <Step title=\"Human oversight\">
    Require human review for confidence < 0.7
  </Step>
</Steps>

### GDPR Art. 22 requirements

```python
# Right to explanation
def provide_explanation(prediction, shap_values, patient_id):
    explanation = {
        'patient_id': patient_id,
        'prediction': prediction,
        'confidence': prediction.confidence,
        'top_features': get_top_features(shap_values, n=5),
        'evidence_chain': prediction.evidence,
        'human_review_required': prediction.confidence < 0.7,
        'timestamp': datetime.utcnow().isoformat()
    }
    
    # Log to audit trail
    guardrail.log_explanation(explanation)
    
    return explanation
```

## Performance optimization

### Batch explanations

```python
# Generate explanations in batch
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(patient_data_batch)

# Parallel processing
from joblib import Parallel, delayed

def explain_instance(instance):
    return explainer.shap_values(instance)

shap_values = Parallel(n_jobs=-1)(
    delayed(explain_instance)(instance)
    for instance in patient_data_batch
)
```

### Caching

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_shap_explanation(model_id, patient_id):
    model = load_model(model_id)
    patient_data = load_patient_data(patient_id)
    
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(patient_data)
    
    return shap_values
```

## Next steps

<CardGroup cols={2}>
  <Card
    title=\"Bio-Interface API\"
    icon=\"mobile\"
    href=\"/integrations/bio-interface\"
  >
    Mobile health app integration
  </Card>
  <Card
    title=\"Governance kernel\"
    icon=\"shield-check\"
    href=\"/governance/overview\"
  >
    Compliance enforcement
  </Card>
  <Card
    title=\"AI agents\"
    icon=\"brain-circuit\"
    href=\"/ai-agents/overview\"
  >
    Autonomous surveillance
  </Card>
  <Card
    title=\"Deploy to GCP\"
    icon=\"cloud\"
    href=\"/deployment/overview\"
  >
    Production deployment
  </Card>
</CardGroup>
